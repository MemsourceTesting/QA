<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>Configuring and managing high availability clusters</title><subtitle>Configuring and managing the Red Hat High Availability Add-On</subtitle>

<date>2020-12-09</date>
<productname>Red Hat Enterprise Linux</productname>
<productnumber>8</productnumber>
<release>{ProductRelease}</release>
<abstract>
  <para>
    This guide provides information about installing, configuring, and managing the Red Hat High Availability Add-On for Red Hat Enterprise Linux 8.
  </para>
</abstract>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Author_Group.xml"/>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Common_Content/Legal_Notice.xml"/>
<orgname>Red Hat</orgname>
</info>
<preface xml:id="making-open-source-more-inclusive">
<title>Making open source more inclusive</title>
<simpara>Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see <link xlink:href="https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language">our CTO Chris Wright’s message</link>.</simpara>
</preface>
<preface xml:id="proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters">
<title>Providing feedback on Red Hat documentation</title>
<simpara role="_abstract">We appreciate your input on our documentation. Please let us know how we could make it better. To do so:</simpara>
<itemizedlist>
<listitem>
<simpara>For simple comments on specific passages:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Make sure you are viewing the documentation in the <emphasis>Multi-page HTML</emphasis> format. In addition, ensure you see the <emphasis role="strong">Feedback</emphasis> button in the upper right corner of the document.</simpara>
</listitem>
<listitem>
<simpara>Use your mouse cursor to highlight the part of text that you want to comment on.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Add Feedback</emphasis> pop-up that appears below the highlighted text.</simpara>
</listitem>
<listitem>
<simpara>Follow the displayed instructions.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For submitting more complex feedback, create a Bugzilla ticket:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to the <link xlink:href="https://bugzilla.redhat.com/enter_bug.cgi?product=Red%20Hat%20Enterprise%20Linux%208">Bugzilla</link> website.</simpara>
</listitem>
<listitem>
<simpara>As the Component, use <emphasis role="strong">Documentation</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Fill in the <emphasis role="strong">Description</emphasis> field with your suggestion for improvement. Include a link to the relevant part(s) of documentation.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Submit Bug</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</preface>
<chapter xml:id="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters">
<title>High Availability Add-On overview</title>
<simpara>The High Availability Add-On is a clustered system that provides
reliability, scalability, and availability to critical production
services.</simpara>
<simpara>A cluster is two or more computers (called
<emphasis>nodes</emphasis> or <emphasis>members</emphasis>)
that work together to perform a task.
Clusters can be used to provide highly available services or resources.
The redundancy of multiple machines is used to guard against failures of many types.</simpara>
<simpara>High availability clusters provide highly available services by
eliminating single points of failure and by failing over
services from one cluster node to another in case a node becomes
inoperative. Typically, services in a high availability cluster
read and write data (by means of read-write mounted file
systems). Therefore, a high availability cluster must maintain
data integrity as one cluster node takes over control of a
service from another cluster node. Node failures in a
high availability cluster are not visible from clients outside
the cluster. (High availability clusters are sometimes referred
to as failover clusters.) The High Availability Add-On provides
high availability clustering through its high availability
service management component, <literal role="command">Pacemaker</literal>.</simpara>
<section xml:id="con_high-availability-add-on-concepts-overview-of-high-availability">
<title>High Availability Add-On components</title>
<simpara>The High Availability Add-On consists of the following major
components:</simpara>
<itemizedlist>
<listitem>
<simpara>Cluster infrastructure — Provides fundamental functions
for nodes to work together as a cluster: configuration file
management, membership management, lock management, and
fencing.</simpara>
</listitem>
<listitem>
<simpara>High availability service management — Provides failover
of services from one cluster node to another in case a node
becomes inoperative.</simpara>
</listitem>
<listitem>
<simpara>Cluster administration tools — Configuration and
management tools for setting up, configuring, and managing
the High Availability Add-On. The tools are for use with the
cluster infrastructure components, the high availability and
service management components, and storage.</simpara>
</listitem>
</itemizedlist>
<simpara>You can supplement the High Availability Add-On with the following
components:</simpara>
<itemizedlist>
<listitem>
<simpara>Red Hat GFS2 (Global File System 2) — Part of the Resilient
Storage Add-On, this provides a cluster file system for use
with the High Availability Add-On. GFS2 allows multiple nodes
to share storage at a block level as if the storage were
connected locally to each cluster node. GFS2 cluster file
system requires a cluster infrastructure.</simpara>
</listitem>
<listitem>
<simpara>LVM Locking Daemon (<literal>lvmlockd</literal>) — Part of the
Resilient Storage Add-On, this provides volume management of
cluster storage. <literal>lvmlockd</literal> support also requires cluster
infrastructure.</simpara>
</listitem>
<listitem>
<simpara>Load Balancer Add-On — Routing software that provides
high availability load balancing and failover in layer 4 (TCP)
and layer 7 (HTTP, HTTPS) services. The Load Balancer Add-On
runs in a cluster of redundant virtual routers that uses load
algorithms to distribute client requests to real servers,
collectively acting as a virtual server. It is not necessary
to use the Load Balancer Add-On in conjunction with Pacemaker.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="s1-Pacemakeroverview-HAAO">
<title>Pacemaker overview</title>
<simpara>Pacemaker is a cluster resource manager.
It achieves maximum availability for your cluster services and resources by making use of the
cluster infrastructure’s messaging and membership capabilities to deter and recover
from node and resource-level failure.</simpara>
<section xml:id="s2-Pacemakerarchitecture-HAAO">
<title>Pacemaker architecture components</title>
<simpara>A cluster configured with Pacemaker comprises separate component
daemons that monitor cluster membership, scripts that manage the
services, and resource management subsystems that monitor the
disparate resources.</simpara>
<simpara>The following components form the Pacemaker
architecture:</simpara>
<variablelist>
<varlistentry>
<term>Cluster Information Base (CIB)</term>
<listitem>
<simpara>The Pacemaker information daemon, which uses XML
internally to distribute and synchronize current
configuration and status information from the Designated
Coordinator (DC) — a node assigned by Pacemaker to
store and distribute cluster state and actions by means of the CIB
— to all other cluster nodes.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster Resource Management Daemon (CRMd)</term>
<listitem>
<simpara>Pacemaker cluster resource actions are routed through this
daemon. Resources managed by CRMd can be queried by client
systems, moved, instantiated, and changed when needed.</simpara>
<simpara>Each cluster node also includes a local resource manager
daemon (LRMd) that acts as an interface between CRMd and
resources. LRMd passes commands from CRMd to agents, such as
starting and stopping and relaying status information.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Shoot the Other Node in the Head (STONITH)</term>
<listitem>
<simpara>STONITH is the Pacemaker fencing implementation.
It acts as a cluster resource in Pacemaker that processes fence
requests, forcefully shutting down nodes and removing them
from the cluster to ensure data integrity. STONITH is
configured in the CIB and can be monitored as a normal cluster
resource. For a general overview of fencing, see
<xref linkend="s1-fencing-HAAO"/>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>corosync</term>
<listitem>
<simpara><literal>corosync</literal> is the component - and a daemon of the same name - that serves the
core membership and member-communication needs for high availability clusters.
It is required for the High Availability Add-On to function.</simpara>
<simpara>In addition to those membership and messaging functions, <literal>corosync</literal> also:</simpara>
<itemizedlist>
<listitem>
<simpara>Manages quorum rules and determination.</simpara>
</listitem>
<listitem>
<simpara>Provides messaging capabilities for applications that coordinate or operate across multiple members
of the cluster and thus must communicate stateful or other information between instances.</simpara>
</listitem>
<listitem>
<simpara>Uses the <literal>kronosnet</literal> library as its network
transport to provide multiple redundant links and automatic failover.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="s2-Pacemakertools-HAAO">
<title>Configuration and management tools</title>
<simpara>The High Availability Add-On features two configuration tools for cluster
deployment, monitoring, and management.</simpara>
<variablelist>
<varlistentry>
<term><literal role="command">pcs</literal></term>
<listitem>
<simpara>The <literal role="command">pcs</literal> command line interface controls
and configures Pacemaker and the <literal>corosync</literal> heartbeat
daemon. A command-line based program, <literal role="command">pcs</literal> can
perform the following cluster management tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Create and configure a Pacemaker/Corosync cluster</simpara>
</listitem>
<listitem>
<simpara>Modify configuration of the cluster while it is running</simpara>
</listitem>
<listitem>
<simpara>Remotely configure both Pacemaker and Corosync
as well as start, stop, and display status information
of the cluster</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="command">pcsd</literal> Web UI</term>
<listitem>
<simpara>A graphical user interface to create and configure
Pacemaker/Corosync clusters.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="s1-configfileoverview-HAAR">
<title>The cluster and pacemaker configuration files</title>
<simpara>The configuration files for the Red Hat High Availability Add-On
are <literal>corosync.conf</literal> and <literal>cib.xml</literal>.</simpara>
<simpara>The <literal>corosync.conf</literal> file
provides the cluster parameters used
by <literal>corosync</literal>, the cluster manager that Pacemaker is
built on.
In general, you should not edit the <literal>corosync.conf</literal>
directly but, instead, use the
<literal role="command">pcs</literal> or <literal role="command">pcsd</literal> interface.</simpara>
<simpara>The <literal>cib.xml</literal> file is an XML file that
represents both the cluster’s configuration and
the current state of all resources in the cluster.
This file is used by Pacemaker’s Cluster Information Base (CIB).
The contents of the CIB
are automatically kept in sync across the entire cluster.
Do not edit the <literal>cib.xml</literal> file directly; use the
<literal role="command">pcs</literal> or <literal role="command">pcsd</literal> interface instead.</simpara>
</section>
</section>
<section xml:id="s1-fencing-HAAO">
<title>Fencing overview</title>
<simpara>If communication with a single node in the cluster fails, then other
nodes in the cluster must be able to restrict or release access
to resources that the failed cluster node may have access to.
This cannot be accomplished by contacting the cluster node itself as the
cluster node may not be responsive. Instead, you must provide an
external method, which is called fencing with a fence agent.
A fence device is an external device that can be used by the
cluster to restrict access to shared resources by an errant node,
or to issue a hard reboot on the cluster node.</simpara>
<simpara>Without a fence device configured you do not have a way to know that
the resources previously used by the disconnected cluster node have been released,
and this could prevent the services from running on any of the other cluster nodes.
Conversely, the system may assume erroneously that the cluster node has released its
resources and this can lead to data corruption and data loss.
Without a fence device configured data integrity cannot be
guaranteed and the cluster configuration will be unsupported.</simpara>
<simpara>When the fencing is in progress no other cluster operation is allowed to run.
Normal operation of the cluster cannot resume until fencing has completed or
the cluster node rejoins the cluster after the cluster node has been rebooted.</simpara>
<simpara>For more information about fencing, see
<link xlink:href="https://access.redhat.com/solutions/15575">
Fencing in a Red Hat High Availability Cluster</link>.</simpara>
</section>
<section xml:id="s1-quorumoverview-HAAO">
<title>Quorum overview</title>
<simpara>In order to maintain cluster integrity and availability, cluster
systems use a concept known as <emphasis>quorum</emphasis> to
prevent data corruption and loss. A cluster has quorum when
more than half of the cluster nodes are online. To mitigate the
chance of data corruption due to failure, Pacemaker by
default stops all resources if the cluster does not have quorum.</simpara>
<simpara>Quorum is established using a voting system. When a cluster node
does not function as it should or loses communication with the
rest of the cluster, the majority working nodes can vote to
isolate and, if needed, fence the node for servicing.</simpara>
<simpara>For example, in a 6-node cluster, quorum is established when at
least 4 cluster nodes are functioning. If the majority of nodes
go offline or become unavailable, the cluster no longer has
quorum and Pacemaker stops clustered services.</simpara>
<simpara>The quorum features in Pacemaker prevent what is also known as
<emphasis>split-brain</emphasis>, a phenomenon where the
cluster is separated from communication but each part continues
working as separate clusters, potentially writing to the same
data and possibly causing corruption or loss.
For more information on what it means to be in a split-brain
state, and on quorum concepts in general, see
<link xlink:href="https://access.redhat.com/articles/2824071">Exploring Concepts of RHEL High Availability Clusters - Quorum</link>.</simpara>
<simpara>A Red Hat Enterprise Linux High Availability Add-On cluster uses
the <literal>votequorum</literal> service, in conjunction
with fencing, to avoid split brain
situations. A number of votes is assigned to each system in
the cluster, and cluster operations are allowed to proceed only
when a majority of votes is present.</simpara>
</section>
<section xml:id="s1-resourceoverview-HAAO">
<title>Resource overview</title>
<simpara>A <emphasis>cluster resource</emphasis> is an instance of
program, data, or application to be managed by the cluster
service. These resources are abstracted by
<emphasis>agents</emphasis> that provide a standard interface
for managing the resource in a cluster environment.</simpara>
<simpara>To ensure that resources remain healthy, you can add a
monitoring operation to a resource’s definition. If you do not
specify a monitoring operation for a resource, one is
added by default.</simpara>
<simpara>You can determine the behavior of a resource in a cluster by
configuring <emphasis>constraints</emphasis>. You can
configure the following categories of constraints:</simpara>
<itemizedlist>
<listitem>
<simpara>location constraints — A location constraint
determines which nodes a resource can run on.</simpara>
</listitem>
<listitem>
<simpara>ordering constraints — An ordering constraint determines the
order in which the resources run.</simpara>
</listitem>
<listitem>
<simpara>colocation constraints — A colocation constraint
determines where resources will be placed relative to other
resources.</simpara>
</listitem>
</itemizedlist>
<simpara>One of the most common elements of a cluster is a set of
resources that need to be located together, start sequentially,
and stop in the reverse order. To simplify this configuration,
Pacemaker supports the concept of <emphasis>groups</emphasis>.</simpara>
</section>
<section xml:id="con_HA-lvm-shared-volumes-overview-of-high-availability">
<title>LVM logical volumes in a Red Hat high availability cluster</title>
<simpara>The Red Hat High Availability Add-On provides support
for LVM volumes in two distinct cluster configurations:</simpara>
<itemizedlist>
<listitem>
<simpara>High availability LVM volumes (HA-LVM) in
active/passive
failover configurations in which only a single node of the
cluster accesses the storage at any one time.</simpara>
</listitem>
<listitem>
<simpara>LVM volumes that use the <literal>lvmlockd</literal> daemon to manage storage devices
in active/active configurations in which more than one node
of the cluster requires access to the storage at the same
time. The <literal>lvmlockd</literal> daemon is part of the Resilient Storage Add-On.</simpara>
</listitem>
</itemizedlist>
<section xml:id="choosing_ha_lvm_or_shared_volumes" remap="_choosing_ha_lvm_or_shared_volumes">
<title>Choosing HA-LVM or shared volumes</title>
<simpara>When to use HA-LVM or shared logical volumes managed by the <literal>lvmlockd</literal> daemon
should be based on the needs of the applications or services being deployed.</simpara>
<itemizedlist>
<listitem>
<simpara>If multiple nodes of the cluster
require simultaneous read/write access to LVM volumes in an active/active
system, then you must use the <literal>lvmlockd</literal> daemon and configure your volumes
as shared volumes. The <literal>lvmlockd</literal> daemon provides a system for coordinating
activation of and changes to LVM volumes across nodes of a cluster
concurrently. The <literal>lvmlockd</literal> daemon’s locking service provides protection to LVM
metadata as various nodes of the cluster interact with volumes and make
changes to their layout. This protection is contingent upon
configuring any volume group that will be
activated simultaneously across multiple cluster nodes as a shared volume.</simpara>
</listitem>
<listitem>
<simpara>If the high availability cluster is configured to manage shared
resources in an active/passive manner with only one single member needing
access to a given LVM volume at a time, then you can use HA-LVM without
the <literal>lvmlockd</literal> locking service.</simpara>
</listitem>
</itemizedlist>
<simpara>Most applications will run better in an
active/passive configuration, as they are not designed or optimized
to run concurrently with other instances. Choosing to run an application
that is not cluster-aware on shared logical volumes may result in degraded
performance.
This is because there is cluster communication overhead for the
logical volumes themselves in these instances. A cluster-aware application
must be able to achieve performance gains above the performance losses introduced
by cluster file systems and cluster-aware logical volumes. This is
achievable for some applications and workloads more easily than others.
Determining what the requirements of the cluster are and whether the
extra effort toward optimizing for an active/active cluster will pay dividends
is the way to choose between the two LVM variants. Most users will achieve the
best HA results from using HA-LVM.</simpara>
<simpara>HA-LVM and shared logical volumes using <literal>lvmlockd</literal> are similar in the fact
that they prevent corruption of LVM metadata and its logical
volumes, which could otherwise occur if multiple machines
are allowed to make overlapping changes. HA-LVM imposes the restriction
that a logical volume can only be activated exclusively; that is, active
on only one machine at a time. This means that only local (non-clustered)
implementations of the storage drivers are used. Avoiding the cluster coordination
overhead in this way increases performance. A shared volume using <literal>lvmlockd</literal> does not impose these
restrictions and a user is free to activate a logical volume on all machines
in a cluster; this forces the use of cluster-aware storage drivers,
which allow for cluster-aware file systems and applications to be put on top.</simpara>
</section>
<section xml:id="configuring_lvm_volumes_in_a_cluster" remap="_configuring_lvm_volumes_in_a_cluster">
<title>Configuring LVM volumes in a cluster</title>
<simpara>In Red Hat Enterprise Linux 8, clusters are managed through Pacemaker.
Both HA-LVM and shared logical volumes are supported only in conjunction with Pacemaker clusters,
and must be configured as cluster resources.</simpara>
<itemizedlist>
<listitem>
<simpara>For examples of procedures for configuring an HA-LVM volume as part of a Pacemaker cluster,
see
<link linkend="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</link>.
and
<link linkend="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/passive NFS server in a Red Hat High Availability cluster</link>.</simpara>
<simpara>Note that these procedures include the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Ensuring that only the cluster is capable of activating the volume group</simpara>
</listitem>
<listitem>
<simpara>Configuring an LVM logical volume</simpara>
</listitem>
<listitem>
<simpara>Configuring the LVM volume as a cluster resource</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>For a procedure for configuring shared LVM volumes that use the <literal>lvmlockd</literal> daemon
to manage storage devices in active/active configurations, see
<link linkend="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster">Configuring a GFS2 file system in a cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters">
<title>Getting started with Pacemaker</title>
<simpara>The following procedures provide an introduction to
the tools and processes you use to create a Pacemaker cluster.
They are intended for users who are interested in seeing what
the cluster software looks like and how it is administered,
without needing to configure a working cluster.</simpara>
<note>
<simpara>These procedures do not create a supported Red Hat cluster, which requires
at least two nodes and the configuration of a fencing device.
For full information on Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see
<link xlink:href="https://access.redhat.com/articles/2912891/">Support Policies for RHEL High Availability Clusters</link>.</simpara>
</note>
<section xml:id="proc_learning-to-use-pacemaker-getting-started-with-pacemaker">
<title>Learning to use Pacemaker</title>
<simpara>This example requires a single node running RHEL 8 and it requires a floating IP address
that resides on the same network as
one of the node’s statically assigned IP addresses.</simpara>
<itemizedlist>
<listitem>
<simpara>The node used in this example is <literal>z1.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>The floating IP address used in this example is 192.168.122.120.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Ensure that the name of the node on which you are running is in your <literal>/etc/hosts</literal> file.</simpara>
</note>
<simpara>By working through this procedure, you will learn how to use Pacemaker
to set up a cluster, how to display cluster status, and how to configure a cluster
service. This example creates an Apache HTTP server as a cluster resource and
shows how the cluster responds when the resource fails.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Red Hat High Availability Add-On software packages
from the High Availability channel, and
start and enable the <literal>pcsd</literal> service.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-all</literal>
...
# <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
<simpara>If you are running the <literal>firewalld</literal> daemon, enable the ports that are
required  by the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd --reload</literal></literallayout>
</listitem>
<listitem>
<simpara>Set a password for user <literal>hacluster</literal> on each node in the cluster and authenticate user <literal>hacluster</literal> for
each node in the cluster on the node from which you will be running the <literal>pcs</literal> commands.
This example is using only a single node, the node from which you are running the commands, but this step is included here since it is a
necessary step in configuring a supported Red Hat High Availability multi-node cluster.</simpara>
<literallayout class="monospaced"># <literal>passwd hacluster</literal>
...
# <literal>pcs host auth z1.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>Create a cluster named <literal>my_cluster</literal> with one member and check the status of the cluster.
This command creates and starts the cluster in one step.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster setup my_cluster --start z1.example.com</literal>
...
# <literal>pcs cluster status</literal>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 1 node configured
 0 resources configured

PCSD Status:
  z1.example.com: Online</literallayout>
</listitem>
<listitem>
<simpara>A Red Hat High Availability cluster requires that you configure fencing for the cluster.  The
reasons for this requirement are described in
<link xlink:href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</link>.
For this introduction, however, which is intended to show only how to use the basic Pacemaker commands,
disable fencing by setting the <literal>stonith-enabled</literal> cluster option to <literal>false</literal>.</simpara>
<warning>
<simpara>The use of <literal>stonith-enabled=false</literal> is completely inappropriate for a production cluster.
It tells the cluster to simply pretend that failed nodes are safely fenced.</simpara>
</warning>
<literallayout class="monospaced"># <literal>pcs property set stonith-enabled=false</literal></literallayout>
</listitem>
<listitem>
<simpara>Configure a web browser on your system and create a web page to display a simple
text message.  If you are running the <literal>firewalld</literal> daemon, enable the ports that are required  by <literal>httpd</literal>.</simpara>
<note>
<simpara>Do not use <literal role="command">systemctl enable</literal> to enable any services that will be managed by the cluster
to start at system boot.</simpara>
</note>
<literallayout class="monospaced"># <literal>yum install -y httpd wget</literal>
...
# <literal>firewall-cmd --permanent --add-service=http</literal>
# <literal>firewall-cmd --reload</literal>

# <literal>cat &lt;&lt;-END &gt;/var/www/html/index.html</literal>
<literal>&lt;html&gt;</literal>
<literal>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</literal>
<literal>&lt;/html&gt;</literal>
<literal>END</literal></literallayout>
<simpara>In order for the Apache resource agent to get the status of Apache,
create the following addition to the existing configuration to enable the status server URL.</simpara>
<literallayout class="monospaced"># <literal>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</literal>
<literal>&lt;Location /server-status&gt;</literal>
<literal>SetHandler server-status</literal>
<literal>Order deny,allow</literal>
<literal>Deny from all</literal>
<literal>Allow from 127.0.0.1</literal>
<literal>Allow from ::1</literal>
<literal>&lt;/Location&gt;</literal>
<literal>END</literal></literallayout>
</listitem>
<listitem>
<simpara>Create <literal>IPaddr2</literal> and <literal>apache</literal> resources for the cluster to manage.  The 'IPaddr2'
resource is a floating IP address that must not be one already associated with a physical node.
If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network
as the statically assigned IP address used by the node.</simpara>
<simpara>You can display a list of all available resource types with the <literal role="command">pcs resource list</literal> command.
You can use the
<literal role="command">pcs resource describe <emphasis>resourcetype</emphasis></literal> command
to display the parameters you can set for the specified resource type.
For example, the following command displays the parameters you can set for a resource of type <literal>apache</literal>:</simpara>
<literallayout class="monospaced"># <literal>pcs resource describe apache</literal>
...</literallayout>
<simpara>In this example, the IP address resource and the apache resource are both configured as part of a group
named <literal>apachegroup</literal>, which ensures that the resources are kept together to run on the same node
when you are configuring a working multi-node cluster.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</literal>

# <literal>pcs resource create WebSite ocf:heartbeat:apache  configfile=/etc/httpd/conf/httpd.conf  statusurl="http://localhost/server-status" --group apachegroup</literal>

# <literal>pcs status</literal>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

1 node configured
2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
...</literallayout>
<simpara>After you have configured a cluster resource, you can use the <literal role="command">pcs resource config</literal> command
to display the options that are configured for that resource.</simpara>
<literallayout class="monospaced"># <literal>pcs resource config WebSite</literal>
Resource: WebSite (class=ocf provider=heartbeat type=apache)
 Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status
 Operations: start interval=0s timeout=40s (WebSite-start-interval-0s)
             stop interval=0s timeout=60s (WebSite-stop-interval-0s)
             monitor interval=1min (WebSite-monitor-interval-1min)</literallayout>
</listitem>
<listitem>
<simpara>Point your browser to the website you created using the floating IP address you configured.
This should display the text message you defined.</simpara>
</listitem>
<listitem>
<simpara>Stop the apache web service and check the cluster status. Using
<literal role="command">killall -9</literal> simulates an application-level crash.</simpara>
<literallayout class="monospaced"># <literal>killall -9 httpd</literal></literallayout>
<simpara>Check the cluster status. You should see that stopping the web service caused a failed action,
but that the cluster software restarted the service and you should still be able to access the website.</simpara>
<literallayout class="monospaced"># <literal>pcs status</literal>
Cluster name: my_cluster
...
Current DC: z1.example.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum
1 node and 2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=13, status=complete, exitreason='none',
    last-rc-change='Thu Oct 11 23:45:50 2016', queued=0ms, exec=0ms

PCSD Status:
    z1.example.com: Online</literallayout>
<simpara>You can clear the failure status on the resource that failed once the service is up and
running again and the failed action notice will no longer appear when you view the cluster status.</simpara>
<literallayout class="monospaced"># <literal>pcs resource cleanup WebSite</literal></literallayout>
</listitem>
<listitem>
<simpara>When you are finished looking at the cluster and the cluster status, stop the cluster services on
the node. Even though you have only started services on one node for this introduction,
the <literal>--all</literal> parameter is included since it would stop cluster services on all nodes on an actual
multi-node cluster.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster stop --all</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_learning-to-configure-failover-getting-started-with-pacemaker">
<title>Learning to configure failover</title>
<simpara>This procedure provides an introduction to creating a Pacemaker cluster
running a service that will fail over from one node to another
when the node on which the service is running becomes unavailable.  By working
through this procedure, you can learn how to create a service in a
two-node cluster and you can then observe what happens to that service when it fails on the node on which it running.</simpara>
<simpara>This example procedure configures a two-node Pacemaker cluster running an Apache
HTTP server. You can then stop the Apache service on one node to see how the service remains available.</simpara>
<simpara>This procedure requires as a prerequisite that you have two
nodes running Red Hat Enterprise Linux 8 that can communicate
with each other, and it requires a floating IP address that resides
on the same network as
one of the node’s statically assigned IP addresses.</simpara>
<itemizedlist>
<listitem>
<simpara>The nodes used in this example are <literal>z1.example.com</literal> and <literal>z2.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>The floating IP address used in this example is 192.168.122.120.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Ensure that the names of the nodes you are using are in the <literal>/etc/hosts</literal> file on each node.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>On both nodes, install the Red Hat High Availability Add-On software packages
from the High Availability channel,
and start and enable the <literal>pcsd</literal> service.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-all</literal>
...
# <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
<simpara>If you are running the <literal>firewalld</literal> daemon, on both nodes enable the ports that are
required  by the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd --reload</literal></literallayout>
</listitem>
<listitem>
<simpara>On both nodes in the cluster, set a password for user <literal>hacluster</literal> .</simpara>
<literallayout class="monospaced"># <literal>passwd hacluster</literal></literallayout>
</listitem>
<listitem>
<simpara>Authenticate user <literal>hacluster</literal> for
each node in the cluster on the node from which you will be running the <literal>pcs</literal> commands.</simpara>
<literallayout class="monospaced"># <literal>pcs host auth z1.example.com z2.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>Create a cluster named <literal>my_cluster</literal> with both nodes as cluster members. This command
creates and starts the cluster in one step. You only need to run
this from one node in the cluster because <literal>pcs</literal> configuration commands take effect for the entire cluster.</simpara>
<simpara>On one node in cluster, run the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster setup my_cluster --start  z1.example.com z2.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>A Red Hat High Availability cluster requires that you configure fencing for the cluster.  The
reasons for this requirement are described in
<link xlink:href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</link>.
For this introduction, however, to show only how failover works in this configuration,
disable fencing by setting the <literal>stonith-enabled</literal> cluster option to <literal>false</literal></simpara>
<warning>
<simpara>The use of <literal>stonith-enabled=false</literal> is completely inappropriate for a production cluster.
It tells the cluster to simply pretend that failed nodes are safely fenced.</simpara>
</warning>
<literallayout class="monospaced"># <literal>pcs property set stonith-enabled=false</literal></literallayout>
</listitem>
<listitem>
<simpara>After creating a cluster and disabling fencing, check the status of the cluster.</simpara>
<note>
<simpara>When you run the <literal role="command">pcs cluster status</literal> command, it may show output that temporarily
differs slightly from the examples as the system components start up.</simpara>
</note>
<literallayout class="monospaced"># <literal>pcs cluster status</literal>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 2 nodes configured
 0 resources configured

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online</literallayout>
</listitem>
<listitem>
<simpara>On both nodes, configure a web browser and create a web page to display a simple text message.
If you are running the <literal>firewalld</literal> daemon, enable the ports that are required  by <literal>httpd</literal>.</simpara>
<note>
<simpara>Do not use <literal role="command">systemctl enable</literal> to enable any services that will be managed by the cluster
to start at system boot.</simpara>
</note>
<literallayout class="monospaced"># <literal>yum install -y httpd wget</literal>
...
# <literal>firewall-cmd --permanent --add-service=http</literal>
# <literal>firewall-cmd --reload</literal>

# <literal>cat &lt;&lt;-END &gt;/var/www/html/index.html</literal>
<literal>&lt;html&gt;</literal>
<literal>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</literal>
<literal>&lt;/html&gt;</literal>
<literal>END</literal></literallayout>
<simpara>In order for the Apache resource agent to get the status of Apache, on each node in the cluster
create the following addition to the existing configuration to enable the status server URL.</simpara>
<literallayout class="monospaced"># <literal>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</literal>
<literal>&lt;Location /server-status&gt;</literal>
<literal>SetHandler server-status</literal>
<literal>Order deny,allow</literal>
<literal>Deny from all</literal>
<literal>Allow from 127.0.0.1</literal>
<literal>Allow from ::1</literal>
<literal>&lt;/Location&gt;</literal>
<literal>END</literal></literallayout>
</listitem>
<listitem>
<simpara>Create <literal>IPaddr2</literal> and <literal>apache</literal> resources for the cluster to manage.  The 'IPaddr2'
resource is a floating IP address that must not be one already associated with a physical node.
If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network
as the statically assigned IP address used by the node.</simpara>
<simpara>You can display a list of all available resource types with the <literal role="command">pcs resource list</literal> command.
You can use the
<literal role="command">pcs resource describe <emphasis>resourcetype</emphasis></literal> command
to display the parameters you can set for the specified resource type.
For example, the following command displays the parameters you can set for a resource of type <literal>apache</literal>:</simpara>
<literallayout class="monospaced"># <literal>pcs resource describe apache</literal>
...</literallayout>
<simpara>In this example, the IP address resource and the apache resource are both configured as part of a group
named <literal>apachegroup</literal>, which ensures that the resources are kept together to run on the same node.</simpara>
<simpara>Run the following commands from one node in the cluster:</simpara>
<literallayout class="monospaced"># <literal>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</literal>

# <literal>pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group apachegroup</literal>

# <literal>pcs status</literal>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online
...</literallayout>
<simpara>Note that in this instance, the <literal>apachegroup</literal> service is running on node z1.example.com.</simpara>
</listitem>
<listitem>
<simpara>Access the website you created, stop the service on the node on which it is running,
and note how the service fails over to the second node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Point a browser to the website you created using the floating IP address you configured.
This should display the text message you defined, displaying the name of the node on which the website is running.</simpara>
</listitem>
<listitem>
<simpara>Stop the apache web service. Using <literal role="command">killall -9</literal> simulates an application-level crash.</simpara>
<literallayout class="monospaced"># <literal>killall -9 httpd</literal></literallayout>
<simpara>Check the cluster status. You should see that stopping the web service caused a failed action, but
that the cluster software restarted the service on the node on which it had been running and you
should still be able to access the web browser.</simpara>
<literallayout class="monospaced"># <literal>pcs status</literal>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=31, status=complete, exitreason='none',
    last-rc-change='Fri Feb  5 21:01:41 2016', queued=0ms, exec=0ms</literallayout>
<simpara>Clear the failure status once the service is up and running again.</simpara>
<literallayout class="monospaced"># <literal>pcs resource cleanup WebSite</literal></literallayout>
</listitem>
<listitem>
<simpara>Put the node on which the service is running into standby mode. Note that since
we have disabled fencing we can not effectively simulate a node-level failure
(such as pulling a power cable) because fencing is required for the cluster to recover from such situations.</simpara>
<literallayout class="monospaced"># <literal>pcs node standby z1.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster and note where the service is now running.</simpara>
<literallayout class="monospaced"># <literal>pcs status</literal>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Node z1.example.com: standby
Online: [ z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z2.example.com
    WebSite    (ocf::heartbeat:apache):        Started z2.example.com</literallayout>
</listitem>
<listitem>
<simpara>Access the website. There should be no loss of service, although the display message should
indicate the node on which the service is now running.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To restore cluster services to the first node, take the node out of standby mode.
This will not necessarily move the service back to that node.</simpara>
<literallayout class="monospaced"># <literal>pcs node unstandby z1.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>For final cleanup, stop the cluster services on both nodes.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster stop --all</literal></literallayout>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_pcs-operation-configuring-and-managing-high-availability-clusters">
<title>The pcs command line interface</title>
<simpara>The <literal role="command">pcs</literal> command line interface controls and configures
cluster services such as <literal>corosync</literal>, <literal>pacemaker</literal>,<literal>booth</literal>,
and <literal>sbd</literal> by providing an easier interface to their configuration files.</simpara>
<simpara>Note that you should not edit the <literal>cib.xml</literal> configuration file directly.
In most cases, Pacemaker will reject a directly modified <literal>cib.xml</literal> file.</simpara>
<section xml:id="proc_pcs-help-pcs-operation">
<title>pcs help display</title>
<simpara>You can use the <literal>-h</literal> option of <literal role="command">pcs</literal>
to display the parameters of a <literal role="command">pcs</literal> command
and a description of those parameters. For example, the
following command displays the parameters of the
<literal role="command">pcs resource</literal> command. Only a portion
of the output is shown.</simpara>
<literallayout class="monospaced"># <literal>pcs resource -h</literal></literallayout>
</section>
<section xml:id="proc_raw-config-pcs-operation">
<title>Viewing the raw cluster configuration</title>
<simpara>Although you should not edit the cluster configuration file
directly, you can view the raw cluster configuration with
the <literal role="command">pcs cluster cib</literal> command.</simpara>
<simpara>You can save the raw cluster configuration to a specified
file with the <literal role="command">pcs cluster cib <emphasis>filename</emphasis></literal> command.
If you have previously configured a cluster and
there is already an active CIB, you use the following
command to save the raw xml file.</simpara>
<literallayout class="monospaced">pcs cluster cib <emphasis>filename</emphasis></literallayout>
<simpara>For example, the following command saves the raw
xml from the CIB into a file named <literal>testfile</literal>.</simpara>
<literallayout class="monospaced">pcs cluster cib testfile</literallayout>
</section>
<section xml:id="proc_configure-testfile-pcs-operation">
<title>Saving a configuration change to a working file</title>
<simpara>When configuring a cluster, you can save configuration changes to
a specified file without affecting the active CIB.  This allows you
to specify configuration updates without immediately updating the
currently running cluster configuration with each individual update.</simpara>
<simpara>For information on saving the CIB to a file, see
<link linkend="proc_raw-config-pcs-operation">Viewing the raw cluster configuration</link>.
Once you have created that file,
you can save configuration changes to that file rather than to the active
CIB by using the <literal>-f</literal> option of the <literal>pcs</literal> command.
When you have completed the changes and are ready to
update the active CIB file, you can push those file updates
with the <literal role="command">pcs cluster cib-push</literal> command.</simpara>
<simpara>The following is the recommended procedure for pushing changes to
the CIB file. This procedure creates a copy of the original
saved CIB file and makes changes to that copy.  When pushing those
changes to the active CIB, this procedure specifies the <literal>diff-against</literal> option
of the <literal role="command">pcs cluster cib-push</literal> command so that only the changes
between the original file and the updated file are pushed to the CIB.
This allows users to make changes in parallel that do not overwrite
each other, and it reduces the load on Pacemaker which does not need
to parse the entire configuration file.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Save the active CIB to a file. This example saves the CIB to a file named <literal>original.xml</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster cib original.xml</literal></literallayout>
</listitem>
<listitem>
<simpara>Copy the saved file to the working file you will be using for the
configuration updates.</simpara>
<literallayout class="monospaced"># <literal>cp original.xml updated.xml</literal></literallayout>
</listitem>
<listitem>
<simpara>Update your configuration as needed. The following command creates a resource in the file <literal>updated.xml</literal>
but does not add that resource to the currently running
cluster configuration.</simpara>
<literallayout class="monospaced"># <literal>pcs -f updated.xml resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 op monitor interval=30s</literal></literallayout>
</listitem>
<listitem>
<simpara>Push the updated file to the active CIB, specifying that you are pushing only the changes
you have made to the original file.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster cib-push updated.xml diff-against=original.xml</literal></literallayout>
</listitem>
</orderedlist>
<simpara>Alternately, you can push the entire current content of a CIB file with
the following command.</simpara>
<literallayout class="monospaced">pcs cluster cib-push <emphasis>filename</emphasis></literallayout>
<simpara>When pushing the entire CIB file, Pacemaker checks the version and does not allow you
to push a CIB file which is older than the one already in a cluster. If you need to
update the entire CIB file with a version that is older than the one currently in the cluster,
you can use the <literal>--config</literal> option of the <literal role="command">pcs cluster cib-push</literal> command.</simpara>
<literallayout class="monospaced">pcs cluster cib-push --config <emphasis>filename</emphasis></literallayout>
</section>
<section xml:id="proc_cluster-status-pcs-operation">
<title>Displaying cluster status</title>
<simpara>You can display the status of the cluster and the
cluster resources with the following command.</simpara>
<literallayout class="monospaced">pcs status</literallayout>
<simpara>You can display the status of a particular cluster
component with the <emphasis>commands</emphasis> parameter of the <literal role="command">pcs status</literal> command,
specifying <literal>resources</literal>,
<literal>cluster</literal>, <literal>nodes</literal>, or <literal>pcsd</literal>.</simpara>
<literallayout class="monospaced">pcs status <emphasis>commands</emphasis></literallayout>
<simpara>For example, the following command displays the status of the cluster
resources.</simpara>
<literallayout class="monospaced">pcs status resources</literallayout>
<simpara>The following command displays the status of the cluster,
but not the cluster resources.</simpara>
<literallayout class="monospaced">pcs cluster status</literallayout>
</section>
<section xml:id="proc_cluster-config-display-pcs-operation">
<title>Displaying the full cluster configuration</title>
<simpara>Use the following command to display the full current
cluster configuration.</simpara>
<literallayout class="monospaced">pcs config</literallayout>
</section>
<section xml:id="proc_pcs-corosync-manage-pcs-operation">
<title>Modifying the corosync.conf file with the pcs command (RHEL 8.4 and later)</title>
<simpara role="_abstract">As of Red Hat Enterprise Linux 8.4, you can use the <literal>pcs</literal> command to modify the parameters
in the <literal>corosync.conf</literal> file.</simpara>
<simpara>The following command modifies the parameters in the <literal>corosync.conf</literal> file.</simpara>
<literallayout class="monospaced">pcs cluster config update [transport <emphasis>transport options</emphasis>] [compression <emphasis>compression options</emphasis>] [crypto <emphasis>crypto options</emphasis>] [totem <emphasis>totem options</emphasis>] [--corosync_conf <emphasis>path</emphasis>]</literallayout>
<simpara>The following example command udates the <literal>knet_pmtud_interval</literal> transport value and the <literal>token</literal> and <literal>join</literal> totem values.</simpara>
<literallayout class="monospaced">pcs cluster config update transport knet_pmtud_interval=35 totem token=10000 join=100</literallayout>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For information on adding and removing nodes from an existing cluster, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#assembly_clusternode-management-configuring-and-managing-high-availability-clusters">Managing cluster nodes</link>.</simpara>
</listitem>
<listitem>
<simpara>For information on adding and modifying links in an existing cluster, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#assembly_clusternode-management-configuring-and-managing-high-availability-clusters">Adding and modifying links in an existing cluster</link>.</simpara>
</listitem>
<listitem>
<simpara>For information on modifyng quorum options and managing the quorum device settings in a cluster, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters">Cluster quorum</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_pcs-corosync-display-pcs-operation">
<title>Displaying the corosync.conf file with the pcs command</title>
<simpara>The following command displays the contents of the <literal>corosync.conf</literal> cluster configuration file.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster corosync</literal></literallayout>
<simpara>As of Red Hat Enterprise Linux 8.4, you can print the contents of the <literal>corosync.conf</literal> file in a human-readable format
with the <literal>pcs cluster config</literal> command, as in the following example.</simpara>
<literallayout class="monospaced">[root@r8-node-01 ~]# <literal>pcs cluster config</literal>
Cluster Name: HACluster
Transport: knet
Nodes:
  r8-node-01:
    Link 0 address: r8-node-01
    Link 1 address: 192.168.122.121
    nodeid: 1
  r8-node-02:
    Link 0 address: r8-node-02
    Link 1 address: 192.168.122.122
    nodeid: 2
Links:
  Link 1:
    linknumber: 1
    ping_interval: 1000
    ping_timeout: 2000
    pong_count: 5
Compression Options:
  level: 9
  model: zlib
  threshold: 150
Crypto Options:
  cipher: aes256
  hash: sha256
Totem Options:
  downcheck: 2000
  join: 50
  token: 10000
Quorum Device: net
  Options:
    sync_timeout: 2000
    timeout: 3000
  Model Options:
    algorithm: lms
    host: r8-node-03
  Heuristics:
    exec_ping: ping -c 1 127.0.0.1</literallayout>
<simpara>Additionally, as of RHEL 8.4, you can run the <literal>pcs cluster config show</literal> command with the
<literal>--output-format=cmd</literal> option to display the <literal>pcs</literal> configuration commands that can
be used to recreate the existing <literal>corosync.conf</literal> file, as in the following example.</simpara>
<literallayout class="monospaced">[root@r8-node-01 ~]# <literal>pcs cluster config show --output-format=cmd</literal>
pcs cluster setup HACluster \
  r8-node-01 addr=r8-node-01 addr=192.168.122.121 \
  r8-node-02 addr=r8-node-02 addr=192.168.122.122 \
  transport \
  knet \
    link \
      linknumber=1 \
      ping_interval=1000 \
      ping_timeout=2000 \
      pong_count=5 \
    compression \
      level=9 \
      model=zlib \
      threshold=150 \
    crypto \
      cipher=aes256 \
      hash=sha256 \
  totem \
    downcheck=2000 \
    join=50 \
    token=10000</literallayout>
</section>
</chapter>
<chapter xml:id="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">
<title>Creating a Red Hat High-Availability cluster with Pacemaker</title>
<simpara>The following procedure creates
a Red Hat High Availability
two-node cluster using <literal role="command">pcs</literal>.</simpara>
<simpara>Configuring the cluster in this example
requires that your system include the following
components:</simpara>
<itemizedlist>
<listitem>
<simpara>2 nodes, which will be used to create the cluster. In this
example, the nodes used are
<literal>z1.example.com</literal> and
<literal>z2.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>Network switches for the private network. We recommend but do not require a private
network for communication among the cluster nodes and other cluster hardware such as
network power switches and Fibre Channel switches.</simpara>
</listitem>
<listitem>
<simpara>A fencing device for each node of the cluster. This example
uses two ports of the APC power switch with
a host name of <literal>zapc.example.com</literal>.</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_installing-cluster-software-creating-high-availability-cluster">
<title>Installing cluster software</title>
<simpara>The following procedure installs the cluster software and configures your system for cluster creation.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On each node in the cluster,
install the
Red Hat High Availability Add-On software packages along with all available
fence agents from the High Availability channel.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-all</literal></literallayout>
<simpara>Alternatively, you can install the
Red Hat High Availability Add-On software packages along with only the
fence agent that you require with the following command.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-<emphasis>model</emphasis></literal></literallayout>
<simpara>The following command displays a list of the available fence agents.</simpara>
<literallayout class="monospaced"># <literal>rpm -q -a | grep fence</literal>
fence-agents-rhevm-4.0.2-3.el7.x86_64
fence-agents-ilo-mp-4.0.2-3.el7.x86_64
fence-agents-ipmilan-4.0.2-3.el7.x86_64
...</literallayout>
<warning>
<simpara>After you install the Red Hat High Availability Add-On packages,
you should ensure that your software update preferences are set
so that nothing is installed automatically. Installation on a running
cluster can cause unexpected behaviors.
For more information, see
<link xlink:href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</link>.</simpara>
</warning>
</listitem>
<listitem>
<simpara>If you are running the <literal role="command">firewalld</literal> daemon,
execute the following commands to enable the ports that are
required by
the Red Hat High Availability Add-On.</simpara>
<note>
<simpara>You can determine whether the <literal role="command">firewalld</literal> daemon is
installed on your system with the <literal role="command">rpm -q firewalld</literal> command. If
it is installed, you can determine
whether it is running with the <literal role="command">firewall-cmd --state</literal> command.</simpara>
</note>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd  --add-service=high-availability</literal></literallayout>
<note>
<simpara>The ideal firewall configuration for cluster components depends on the local environment,
where you may need to take into account such considerations as whether the nodes have
multiple network interfaces or whether off-host firewalling is present. The example
here, which opens the ports that are generally required by a Pacemaker cluster,
should be modified to suit local conditions.
<link linkend="proc_enabling-ports-for-high-availability-creating-high-availability-cluster">Enabling ports for the High Availability Add-On</link>
shows the ports to enable for
the Red Hat High Availability Add-On and provides an explanation
for what each port is used for.</simpara>
</note>
</listitem>
<listitem>
<simpara>In order to use <literal>pcs</literal> to configure
the cluster and communicate among the nodes,
you must set a password on each node for
the user ID <literal>hacluster</literal>,
which is the <literal>pcs</literal> administration
account.
It is recommended that the
password for user <literal>hacluster</literal> be
the same on each node.</simpara>
<literallayout class="monospaced"># <literal>passwd hacluster</literal>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</literallayout>
</listitem>
<listitem>
<simpara>Before the cluster can be configured, the <literal role="command">pcsd</literal> daemon
must be started and enabled to start up on boot on each node.
This daemon works with the <literal role="command">pcs</literal> command
to manage configuration across the nodes in the cluster.</simpara>
<simpara>On each node in the cluster, execute the following
commands to start the <literal>pcsd</literal>
service and to enable <literal>pcsd</literal>
at system start.</simpara>
<literallayout class="monospaced"># <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_installing-pcp-zeroconf-creating-high-availability-cluster">
<title>Installing the pcp-zeroconf package (recommended)</title>
<simpara>When you set up your cluster, it is recommended that you install
the <literal>pcp-zeroconf</literal> package for the Performance Co-Pilot (PCP) tool.
PCP is Red Hat’s recommended resource-monitoring tool for RHEL
systems. Installing the <literal>pcp-zeroconf</literal> package allows you to have
PCP running and collecting performance-monitoring data for the benefit of investigations into fencing,
resource failures, and other events that disrupt the cluster.</simpara>
<note>
<simpara>Cluster deployments where PCP is enabled will need sufficient space available for PCP’s captured
data on the file system that contains <literal>/var/log/pcp/</literal>. Typical space usage by PCP varies across deployments,
but 10Gb is usually sufficient when using the <literal>pcp-zeroconf</literal> default settings, and some environments may require less.
Monitoring usage in this directory over a 14-day period of typical activity can provide a more
accurate usage expectation.</simpara>
</note>
<simpara>To install the <literal>pcp-zeroconf</literal> package, run the following command.</simpara>
<literallayout class="monospaced"># <literal>yum install pcp-zeroconf</literal></literallayout>
<simpara>This package enables <literal>pmcd</literal> and sets up data capture at a 10-second interval.</simpara>
<simpara>For information on reviewing PCP data, see
<link xlink:href="https://access.redhat.com/solutions/4545111">Why did a RHEL High Availability cluster node reboot - and how can I prevent it from happening again?</link> on the Red Hat Customer Portal.</simpara>
</section>
<section xml:id="proc_creating-high-availability-cluster-creating-high-availability-cluster">
<title>Creating a high availability cluster</title>
<simpara>This procedure creates a Red Hat High Availability Add-On cluster that
consists of the nodes
<literal>z1.example.com</literal> and
<literal>z2.example.com</literal>.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Authenticate the <literal role="command">pcs</literal> user <literal>hacluster</literal> for each node
in the cluster on the node from which you will be running
<literal role="command">pcs</literal>.</simpara>
<simpara>The following command authenticates user
<literal>hacluster</literal> on <literal>z1.example.com</literal> for both of the
nodes in a two-node cluster that will consist of <literal>z1.example.com</literal> and
<literal>z2.example.com</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs host auth z1.example.com z2.example.com</literal>
Username: <literal>hacluster</literal>
Password:
z1.example.com: Authorized
z2.example.com: Authorized</literallayout>
</listitem>
<listitem>
<simpara>Execute the following
command from
<literal>z1.example.com</literal>
to create the two-node cluster <literal>my_cluster</literal>
that consists of nodes
<literal>z1.example.com</literal> and
<literal>z2.example.com</literal>.
This will propagate the cluster configuration files to both
nodes in the cluster.
This command includes the <literal>--start</literal> option, which
will start the cluster services on both nodes in the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs cluster setup my_cluster --start</literal> <literal>z1.example.com z2.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>Enable the cluster services
to run on each node in the cluster when the node
is booted.</simpara>
<note>
<simpara>For your particular environment, you may choose to leave the
cluster services disabled by skipping this step.
This allows you to ensure that if a node goes down,
any issues with your cluster or your resources are resolved before
the node rejoins the cluster. If you leave the cluster services
disabled, you will need to manually start the services when you
reboot a node by executing the <literal role="command">pcs cluster start</literal> command
on that node.</simpara>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs cluster enable --all</literal></literallayout>
</listitem>
</orderedlist>
<simpara>You can display the current status of the cluster
with the <literal role="command">pcs cluster status</literal> command.
Because there may be a slight delay before the cluster
is up and running when you start the cluster services
with the <literal role="option">--start</literal> option of the
<literal role="command">pcs cluster setup</literal> command, you should ensure
that the cluster is up and running before performing any
subsequent actions on the cluster and its configuration.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs cluster status</literal>
Cluster Status:
 Stack: corosync
 Current DC: z2.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z2.example.com
 2 Nodes configured
 0 Resources configured

...</literallayout>
</section>
<section xml:id="proc_configure-multiple-ip-cluster-creating-high-availability-cluster">
<title>Creating a high availability cluster with multiple links</title>
<simpara>You can use the <literal role="command">pcs cluster setup</literal> command to create a Red Hat High Availability cluster with multiple
links by specifying all of the links for each node.</simpara>
<simpara>The format for the command to create a two-node cluster with two links is as follows.</simpara>
<literallayout class="monospaced">pcs cluster setup <emphasis>cluster_name</emphasis> <emphasis>node1_name</emphasis> addr=<emphasis>node1_link0_address</emphasis> addr=<emphasis>node1_link1_address</emphasis> <emphasis>node2_name</emphasis> addr=<emphasis>node2_link0_address</emphasis> addr=<emphasis>node2_link1_address</emphasis></literallayout>
<simpara>When creating a cluster with multiple links, you should take the following into account.</simpara>
<itemizedlist>
<listitem>
<simpara>The order of the <literal>addr=<emphasis>address</emphasis></literal> parameters is important. The first address specified after
a node name is for <literal>link0</literal>, the second one for <literal>link1</literal>, and so forth.</simpara>
</listitem>
<listitem>
<simpara>It is possible to specify up to eight links using the knet transport protocol, which is the default transport protocol.</simpara>
</listitem>
<listitem>
<simpara>All nodes must have the same number of <literal>addr=</literal> parameters.</simpara>
</listitem>
<listitem>
<simpara>As of RHEL 8.1, it is possible to add, remove, and change links in an existing cluster
using the <literal>pcs cluster link add</literal>, the <literal>pcs cluster link remove</literal>, the <literal>pcs cluster link delete</literal>, and
the <literal>pcs cluster link update</literal> commands.</simpara>
</listitem>
<listitem>
<simpara>As with single-link clusters, do not mix IPv4 and IPv6 addresses in one link, although you can have one link running IPv4 and the other running IPv6.</simpara>
</listitem>
<listitem>
<simpara>As with single-link clusters, you can specify addresses as IP addresses or as names as long as the names resolve to IPv4 or IPv6 addresses for which IPv4 and IPv6 addresses are not mixed in one link.</simpara>
</listitem>
</itemizedlist>
<simpara>The following example creates a two-node cluster named <literal>my_twolink_cluster</literal> with two nodes, <literal>rh80-node1</literal> and <literal>rh80-node2</literal>.
<literal>rh80-node1</literal> has two interfaces, IP address 192.168.122.201 as <literal>link0</literal> and 192.168.123.201 as <literal>link1</literal>.
<literal>rh80-node2</literal> has two interfaces, IP address 192.168.122.202 as <literal>link0</literal> and 192.168.123.202 as <literal>link1</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202</literal></literallayout>
<simpara>For information on adding nodes to an existing cluster with multiple links, see <link linkend="proc_add-nodes-to-multiple-ip-cluster-clusternode-management">Adding a node to a cluster with multiple links</link>.</simpara>
<simpara>For information on changing the links in an existing cluster with multiple links, see <link linkend="proc_changing-links-in-multiple-ip-cluster-clusternode-management">Adding and modifying links in an existing cluster</link>.</simpara>
</section>
<section xml:id="proc_configuring-fencing-creating-high-availability-cluster">
<title>Configuring fencing</title>
<simpara>You must configure a fencing device for each node in the cluster.
For information about the fence configuration commands and
options, see
<link linkend="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters">Configuring fencing in a Red Hat High Availability cluster</link>.</simpara>
<simpara>For general information on fencing and its importance in
a Red Hat High Availability cluster, see
<link xlink:href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</link>.</simpara>
<note>
<simpara>When configuring a fencing device, attention should be given to whether
that device shares power with any nodes or devices in the cluster. If a
node and its fence device do share power, then the cluster may be at risk
of being unable to fence that node if the power to it and its fence device
should be lost. Such a cluster should either have redundant power supplies
for fence devices and nodes, or redundant fence devices that do not share
power. Alternative methods of fencing such as SBD or storage fencing may
also bring redundancy in the event of isolated power losses.</simpara>
</note>
<simpara>This example uses the APC power switch with
a host name of <literal>zapc.example.com</literal>
to fence the nodes, and it uses the <literal>fence_apc_snmp</literal> fencing
agent. Because both nodes will be fenced by the same fencing agent, you
can configure both fencing devices as a single resource,
using the <literal>pcmk_host_map</literal> option.</simpara>
<simpara>You create a fencing device by configuring the device
as a <literal>stonith</literal>
resource with the <literal role="command">pcs stonith create</literal> command.
The following command configures a <literal>stonith</literal> resource
named <literal>myapc</literal> that uses the <literal>fence_apc_snmp</literal>
fencing agent for nodes <literal>z1.example.com</literal> and
<literal>z2.example.com</literal>. The <literal>pcmk_host_map</literal>
option maps
<literal>z1.example.com</literal> to port 1, and
<literal>z2.example.com</literal> to port 2.
The login value and password for the APC device are both
<literal>apc</literal>.
By default, this device will use a monitor interval of sixty seconds
for each node.</simpara>
<simpara>Note that you can use an IP address when specifying the host name
for the nodes.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs stonith create myapc fence_apc_snmp</literal> \
<literal>ipaddr="zapc.example.com"</literal> \
<literal>pcmk_host_map="z1.example.com:1;z2.example.com:2"</literal> \
<literal>login="apc" passwd="apc"</literal></literallayout>
<simpara>The following command displays the parameters of an
existing STONITH device.</simpara>
<literallayout class="monospaced">[root@rh7-1 ~]# <literal>pcs stonith config myapc</literal>
 Resource: myapc (class=stonith type=fence_apc_snmp)
  Attributes: ipaddr=zapc.example.com pcmk_host_map=z1.example.com:1;z2.example.com:2 login=apc passwd=apc
  Operations: monitor interval=60s (myapc-monitor-interval-60s)</literallayout>
<simpara>After configuring your fence device, you should test the device.
For information on testing a fence device, see
<link linkend="proc_testing-fence-devices-configuring-fencing">Testing a fence device</link>.</simpara>
<note>
<simpara>Do not test your fence device by disabling the network interface, as this
will not properly test fencing.</simpara>
</note>
<note>
<simpara>Once fencing is configured and a cluster has been started, a network restart will trigger fencing
for the node which restarts the network even when the timeout is not exceeded. For this reason,
do not restart the network service while the cluster service is running because it will trigger
unintentional fencing on the node.</simpara>
</note>
</section>
<section xml:id="proc_cluster-backup-creating-high-availability-cluster">
<title>Backing up and restoring a cluster configuration</title>
<simpara>You can back up
the cluster configuration in a tarball
with the following command. If you do not specify a file name, the standard output will
be used.</simpara>
<literallayout class="monospaced">pcs config backup <emphasis>filename</emphasis></literallayout>
<note>
<simpara>The <literal role="command">pcs config backup</literal> command backs up only the cluster configuration
itself as configured in the CIB; the configuration of resource daemons is out of the scope of this
command. For example if you have configured an Apache resource
in the cluster, the resource settings (which are in the CIB)
will be backed up, while the Apache daemon settings (as set in`/etc/httpd`) and the
files it serves will not be backed up. Similarly, if there is a
database resource configured in the cluster, the database itself
will not be backed up, while the database resource configuration (CIB)
will be.</simpara>
</note>
<simpara>Use the following command to restore the cluster configuration files on
all nodes from the backup. If you do not specify a file name, the standard
input will be used. Specifying the <literal role="option">--local</literal> option restores
only the files on the current node.</simpara>
<literallayout class="monospaced">pcs config restore [--local] [<emphasis>filename</emphasis>]</literallayout>
</section>
<section xml:id="proc_enabling-ports-for-high-availability-creating-high-availability-cluster">
<title>Enabling ports for the High Availability Add-On</title>
<simpara>The ideal firewall configuration for cluster components depends on the local environment,
where you may need to take into account such considerations as whether the nodes have
multiple network interfaces or whether off-host firewalling is present.</simpara>
<simpara>If you are running the <literal role="command">firewalld</literal> daemon,
execute the following commands to enable the ports that are
required by the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd  --add-service=high-availability</literal></literallayout>
<simpara>You may need to modify which ports are open to suit local conditions.</simpara>
<note>
<simpara>You can determine whether the <literal role="command">firewalld</literal> daemon is
installed on your system with the <literal role="command">rpm -q firewalld</literal> command. If
the <literal role="command">firewalld</literal> daemon is installed, you can determine
whether it is running with the <literal role="command">firewall-cmd --state</literal> command.</simpara>
</note>
<simpara><xref linkend="tb-portenable-HAAR"/> shows the ports to enable for
the Red Hat High Availability Add-On and provides an explanation
for what the port is used for.</simpara>
<table xml:id="tb-portenable-HAAR" frame="all" rowsep="1" colsep="1">
<title>Ports to Enable for High Availability Add-On</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">When Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>TCP 2224</simpara></entry>
<entry align="left" valign="top"><simpara>Default <literal>pcsd</literal> port required on all nodes
(needed by the pcsd Web UI and required for node-to-node communication).
You can configure the <literal>pcsd</literal> port
by means of the <literal>PCSD_PORT</literal> parameter in the <literal>/etc/sysconfig/pcsd</literal> file.</simpara><simpara>It is crucial to open port 2224 in such a way that <literal role="command">pcs</literal>
from any node can talk to all nodes in the cluster, including itself.
When using the Booth cluster ticket manager or a quorum device
you must open port 2224 on all related hosts, such as Booth arbiters
or the quorum device host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TCP 3121</simpara></entry>
<entry align="left" valign="top"><simpara>Required on all nodes if the cluster has any Pacemaker Remote nodes</simpara><simpara>Pacemaker’s <literal>pacemaker-based</literal> daemon on the full cluster nodes
will contact the <literal>pacemaker_remoted</literal> daemon on Pacemaker
Remote nodes at port 3121. If a separate interface is used
for cluster communication, the port only needs to be open on that interface.
At a minimum, the port should open on Pacemaker Remote nodes
to full cluster nodes.
Because users may convert a host between a full node and a remote node,
or run a remote node inside a container using the host’s network,
it can be useful to open the port to all nodes. It is not
necessary to open the port to any hosts other than nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TCP 5403</simpara></entry>
<entry align="left" valign="top"><simpara>Required on the quorum device host when using a quorum device
with <literal>corosync-qnetd</literal>.
The default value can be changed with the <literal role="option">-p</literal> option
of the <literal role="command">corosync-qnetd</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>UDP 5404-5412</simpara></entry>
<entry align="left" valign="top"><simpara>Required on corosync nodes to facilitate communication between nodes. It is crucial
to open ports 5404-5412 in such a way that <literal>corosync</literal> from any node can talk
to all nodes in the cluster, including itself.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TCP 21064</simpara></entry>
<entry align="left" valign="top"><simpara>Required on all nodes if the cluster contains any resources
requiring DLM (such as <literal>GFS2</literal>).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TCP 9929, UDP 9929</simpara></entry>
<entry align="left" valign="top"><simpara>Required to be open on all cluster nodes and booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</chapter>
<chapter xml:id="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters">
<title>Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</title>
<simpara>The following procedure configures
an active/passive Apache HTTP server in a
two-node Red Hat Enterprise Linux High Availability Add-On cluster
using <literal role="command">pcs</literal>
to configure cluster resources.
In this use case, clients access the Apache HTTP server through
a floating IP address.
The web server runs on one of two nodes in the cluster.
If the node on which the web server is running
becomes inoperative, the web server starts up again on the
second node of the cluster with minimal service interruption.</simpara>
<simpara><xref linkend="configuring-ha-http-291627-haserver_cluster4"/>
shows a high-level overview of the cluster in which
The cluster is a two-node Red Hat High Availability cluster which is
configured with a network power switch and with
shared storage.
The cluster nodes are connected to a public
network, for client access to the Apache HTTP server through
a virtual IP.
The Apache server runs on either Node 1 or Node 2,
each of which has access to the storage on which the
Apache data is kept.
In this illustration, the web server is running on Node 1
while Node 2 is available to run the server
if Node 1 becomes inoperative.</simpara>
<figure xml:id="configuring-ha-http-291627-haserver_cluster4">
<title>Apache in a Red Hat High Availability Two-Node Cluster</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/291627-haserver_cluster4.png"/>
</imageobject>
<textobject><phrase>Apache in a Red Hat High Availability Two-Node Cluster</phrase></textobject>
</mediaobject>
</figure>
<simpara>This use case
requires that your system include the following
components:</simpara>
<itemizedlist>
<listitem>
<simpara>A two-node Red Hat High Availability cluster with power fencing configured
for each node.
We recommend but do not require a private network. This procedure uses
the cluster example provided in
<link linkend="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</link>.</simpara>
</listitem>
<listitem>
<simpara>A public virtual IP address, required for Apache.</simpara>
</listitem>
<listitem>
<simpara>Shared storage for the nodes in the cluster, using
iSCSI, Fibre Channel, or other shared network block device.</simpara>
</listitem>
</itemizedlist>
<simpara>The cluster is configured with an Apache resource
group, which contains the cluster components that the
web server requires:
an LVM resource,
a file system resource, an IP address resource, and a web server resource.
This resource group can fail over from one
node of the cluster to the other, allowing either node to run the
web server.
Before creating the resource group for this cluster,
you will be performing the following
procedures:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure an <literal>ext4</literal> file system on the
logical volume <literal>my_lv</literal>.</simpara>
</listitem>
<listitem>
<simpara>Configure a web server.</simpara>
</listitem>
</orderedlist>
<simpara>After performing these steps, you create the
resource group and the resources it contains.</simpara>
<section xml:id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">
<title>Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</title>
<simpara>This use case requires that you create
an LVM logical volume on storage that is
shared between the nodes of the cluster.</simpara>
<note>
<simpara>LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected
to the cluster nodes only.</simpara>
</note>
<simpara>The following procedure creates an LVM logical
volume and then creates an ext4 file
system on that volume for use in a Pacemaker cluster. In this example,
the shared partition <literal>/dev/sdb1</literal>
is used to store the LVM physical volume from which
the LVM logical volume will be created.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On both nodes of the cluster, perform the following steps to set the value for the LVM system ID
to the value of the <literal>uname</literal> identifier for the system.
The LVM system ID
will be used to ensure that only the cluster is capable of
activating the volume group.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Set the <literal>system_id_source</literal> configuration option in the <literal>/etc/lvm/lvm.conf</literal> configuration file to <literal>uname</literal>.</simpara>
<literallayout class="monospaced"># Configuration option global/system_id_source.
system_id_source = "uname"</literallayout>
</listitem>
<listitem>
<simpara>Verify that the LVM system ID on the node matches the <literal>uname</literal> for the node.</simpara>
<literallayout class="monospaced"># <literal>lvm systemid</literal>
  system ID: z1.example.com
# <literal>uname -n</literal>
  z1.example.com</literallayout>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the LVM volume and create an ext4 file system on that volume. Since the <literal>/dev/sdb1</literal> partition
is storage that is shared,
you perform this part of the procedure on one node only.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an LVM physical volume on partition
<literal>/dev/sdb1</literal>.</simpara>
<literallayout class="monospaced"># <literal>pvcreate /dev/sdb1</literal>
  Physical volume "/dev/sdb1" successfully created</literallayout>
</listitem>
<listitem>
<simpara>Create the volume group <literal>my_vg</literal> that
consists of the physical volume <literal>/dev/sdb1</literal>.</simpara>
<literallayout class="monospaced"># <literal>vgcreate my_vg /dev/sdb1</literal>
  Volume group "my_vg" successfully created</literallayout>
</listitem>
<listitem>
<simpara>Verify that the new volume group has the system ID of the
node on which you are running and from which you created the volume group.</simpara>
<literallayout class="monospaced"># <literal>vgs -o+systemid</literal>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</literallayout>
</listitem>
<listitem>
<simpara>Create a logical volume using the volume group
<literal>my_vg</literal>.</simpara>
<literallayout class="monospaced"># <literal>lvcreate -L450 -n my_lv my_vg</literal>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</literallayout>
<simpara>You can use the <literal role="command">lvs</literal> command to display the logical
volume.</simpara>
<literallayout class="monospaced"># <literal>lvs</literal>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</literallayout>
</listitem>
<listitem>
<simpara>Create an ext4 file system on the
logical volume <literal>my_lv</literal>.</simpara>
<literallayout class="monospaced"># <literal>mkfs.ext4 /dev/my_vg/my_lv</literal>
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 462848 1k blocks and 115824 inodes
...</literallayout>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_ensuring-cluster-volume-not-multiply-activated-configuring-ha-http">
<title>Ensuring a volume group is not activated on multiple cluster nodes</title>
<simpara role="_abstract">This procedure ensures that volume groups
that are managed by Pacemaker in a cluster
will not be automatically activated on startup.
If a volume group is automatically activated on startup
rather than by Pacemaker,
there is a risk that the volume group will be active on
multiple nodes at the same time, which
could corrupt the volume group’s metadata.</simpara>
<simpara>This procedure modifies the <literal>auto_activation_volume_list</literal> entry
in the <literal>/etc/lvm/lvm.conf</literal> configuration file.
The <literal>auto_activation_volume_list</literal>
entry is used to limit autoactivation to
specific logical volumes.
Setting <literal>auto_activation_volume_list</literal>
to an empty list disables autoactivation entirely.</simpara>
<simpara>Any local volumes that are not shared and are not managed
by Pacemaker should be included in the <literal>auto_activation_volume_list</literal> entry,
including volume groups related to the node’s local
root and home directories.
All volume groups managed by the cluster manager must
be excluded from the <literal>auto_activation_volume_list</literal> entry.</simpara>
<formalpara>
<title>Procedure</title>
<para>Perform the following procedure on each node in the cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Determine which volume groups are currently configured on your
local storage with the following command.
This will output a list of the currently-configured volume groups.
If you have space allocated in separate volume
groups for root and for your home directory on this
node, you will see
those volumes in the output, as in this example.</simpara>
<literallayout class="monospaced"># <literal>vgs --noheadings -o vg_name</literal>
  my_vg
  rhel_home
  rhel_root</literallayout>
</listitem>
<listitem>
<simpara>Add the volume groups other than <literal>my_vg</literal> (the volume
group you have just defined for the cluster) as entries
to <literal>auto_activation_volume_list</literal> in the <literal>/etc/lvm/lvm.conf</literal> configuration file.</simpara>
<simpara>For example, if you have space allocated in separate volume
groups for root and for your home directory, you would uncomment
the <literal>auto_activation_volume_list</literal> line of the <literal>lvm.conf</literal> file and add
these volume groups as entries to <literal>auto_activation_volume_list</literal> as follows. Note that
the volume group you have just defined for the cluster (<literal>my_vg</literal> in
this example) is not in this list.</simpara>
<literallayout class="monospaced">auto_activation_volume_list = [ "rhel_root", "rhel_home" ]</literallayout>
<note>
<simpara>If no local volume groups are present on a node to be
activated outside of the cluster manager, you must still
initialize the <literal>auto_activation_volume_list</literal> entry
as <literal>auto_activation_volume_list = []</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Rebuild the <literal>initramfs</literal> boot image to
guarantee that the boot image will not try to activate
a volume group controlled by the cluster. Update the
<literal>initramfs</literal>
device with the following command.
This command may take up to a minute to complete.</simpara>
<literallayout class="monospaced"># <literal>dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)</literal></literallayout>
</listitem>
<listitem>
<simpara>Reboot the node.</simpara>
<note>
<simpara>If you have installed a new Linux kernel since booting the node on which you created the
boot image, the new <literal>initrd</literal> image will be for the kernel that was running when you created it
and not for the new kernel that is running when you reboot the
node. You can ensure that the correct <literal>initrd</literal> device is in use by running
the <literal>uname -r</literal> command before and after the reboot to determine the kernel release that is running.
If the releases are not the same, update the <literal>initrd</literal> file after rebooting
with the new kernel and then reboot the node.</simpara>
</note>
</listitem>
<listitem>
<simpara>When the node has rebooted, check whether the cluster services have started up again
on that node by executing the <literal>pcs cluster status</literal> command on that
node. If this yields the message <literal>Error: cluster is not currently running on this node</literal>
then enter the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster start</literal></literallayout>
<simpara>Alternately, you can wait until you have rebooted each node
in the cluster and start cluster services on all of the nodes in the cluster with
the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster start --all</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring-apache-http-web-server-configuring-ha-http">
<title>Configuring an Apache HTTP Server</title>
<simpara>The following procedure configures an Apache HTTP Server.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Ensure that the
Apache HTTP Server is installed on each node in the cluster. You also need
the <literal>wget</literal> tool installed on the cluster
to be able to check the status of the Apache HTTP Server.</simpara>
<simpara>On each node, execute the following command.</simpara>
<literallayout class="monospaced"># <literal>yum install -y httpd wget</literal></literallayout>
<simpara>If you are running the <literal>firewalld</literal> daemon, on each node in the cluster enable the ports that are
required  by the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd --reload</literal></literallayout>
</listitem>
<listitem>
<simpara>In order for the Apache resource agent to
get the status of the Apache HTTP Server,
ensure that the following text is
present in
the <literal>/etc/httpd/conf/httpd.conf</literal> file
on each node in the cluster, and ensure
that it has not been commented out.
If this text is not already present, add the text
to the end of the file.</simpara>
<literallayout class="monospaced">&lt;Location /server-status&gt;
    SetHandler server-status
    Require local
&lt;/Location&gt;</literallayout>
</listitem>
<listitem>
<simpara>When you use the <literal>apache</literal> resource agent
to manage Apache, it does not use <literal>systemd</literal>.
Because of this, you must edit the <literal>logrotate</literal> script supplied
with Apache so that it does not use <literal>systemctl</literal>
to reload Apache.</simpara>
<simpara>Remove the following line in the <literal>/etc/logrotate.d/httpd</literal> file on
each node in the cluster.</simpara>
<literallayout class="monospaced">/bin/systemctl reload httpd.service &gt; /dev/null 2&gt;/dev/null || true</literallayout>
<simpara>Replace the line you removed with the following three lines.</simpara>
<literallayout class="monospaced">/usr/bin/test -f /run/httpd.pid &gt;/dev/null 2&gt;/dev/null &amp;&amp;
/usr/bin/ps -q $(/usr/bin/cat /run/httpd.pid) &gt;/dev/null 2&gt;/dev/null &amp;&amp;
/usr/sbin/httpd -f /etc/httpd/conf/httpd.conf \
-c "PidFile /run/httpd.pid" -k graceful &gt; /dev/null 2&gt;/dev/null || true</literallayout>
</listitem>
<listitem>
<simpara>Create a web page for Apache to serve up.
On one node in the cluster, mount the file system you created in
<link linkend="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">Configuring an LVM volume with an ext4 file system</link>,
create the file <literal>index.html</literal>
on that file system, and then unmount the file system.</simpara>
<literallayout class="monospaced"># <literal>mount /dev/my_vg/my_lv /var/www/</literal>
# <literal>mkdir /var/www/html</literal>
# <literal>mkdir /var/www/cgi-bin</literal>
# <literal>mkdir /var/www/error</literal>
# <literal>restorecon -R /var/www</literal>
# <literal>cat &lt;&lt;-END &gt;/var/www/html/index.html</literal>
<literal>&lt;html&gt;</literal>
<literal>&lt;body&gt;Hello&lt;/body&gt;</literal>
<literal>&lt;/html&gt;</literal>
<literal>END</literal>
# <literal>umount /var/www</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http">
<title>Creating the resources and resource groups</title>
<simpara>This use case requires that you create four cluster resources.
To ensure these resources all run on the same node, they
are configured as part of the
resource group <literal>apachegroup</literal>.
The resources to create are as follows,
listed in the order in which they will start.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>An <literal>LVM</literal> resource named <literal>my_lvm</literal> that uses the
LVM volume group you created in
<link linkend="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">Configuring an LVM volume with an ext4 file system</link>.</simpara>
</listitem>
<listitem>
<simpara>A <literal>Filesystem</literal> resource named <literal>my_fs</literal>,
that uses the file system
device <literal>/dev/my_vg/my_lv</literal> you created in
<link linkend="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">Configuring an LVM volume with an ext4 file system</link>.</simpara>
</listitem>
<listitem>
<simpara>An <literal>IPaddr2</literal> resource, which is a floating IP
address for the <literal>apachegroup</literal> resource group.
The IP address must not be one already associated with a physical node.
If the <literal>IPaddr2</literal> resource’s NIC device is not specified,
the floating IP must reside on the same network as
one of the node’s statically assigned IP addresses,
otherwise the NIC device to assign the floating IP address
cannot be properly detected.</simpara>
</listitem>
<listitem>
<simpara>An <literal>apache</literal> resource named <literal>Website</literal> that uses
the <literal>index.html</literal> file and the Apache configuration you
defined in
<link linkend="proc_configuring-apache-http-web-server-configuring-ha-http">Configuring an Apache HTTP server</link>.</simpara>
</listitem>
</orderedlist>
<simpara>The following procedure creates
the resource group <literal>apachegroup</literal>
and the resources that the group contains.
The resources will start in the order in which you add them to the group,
and they will stop in
the reverse order in which they are added to the group.
Run this procedure from one node of the cluster only.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The following command creates the <literal>LVM-activate</literal> resource <literal>my_lvm</literal>.
Because the resource group <literal>apachegroup</literal> does not yet exist, this
command creates the resource group.</simpara>
<note>
<simpara>Do not configure more than one <literal>LVM-activate</literal> resource that uses the same LVM volume group in an active/passive HA configuration,
as this could cause data corruption.  Additionally, do not configure an <literal>LVM-activate</literal> resource as a clone resource in an active/passive
HA configuration.</simpara>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create my_lvm ocf:heartbeat:LVM-activate</literal> <literal>vgname=my_vg</literal> <literal>vg_access_mode=system_id --group apachegroup</literal></literallayout>
<simpara>When you create a resource,
the resource is started automatically.
You can use the following command to confirm that the resource was
created and has started.</simpara>
<literallayout class="monospaced"># <literal>pcs resource status</literal>
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM-activate):	Started</literallayout>
<simpara>You can manually stop and start an individual resource
with the <literal role="command">pcs resource disable</literal> and
<literal role="command">pcs resource enable</literal> commands.</simpara>
</listitem>
<listitem>
<simpara>The following commands create the remaining resources
for the configuration, adding
them to the existing resource group <literal>apachegroup</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create my_fs Filesystem</literal> \
<literal>device="/dev/my_vg/my_lv" directory="/var/www" fstype="ext4"</literal> \
<literal>--group apachegroup</literal>

[root@z1 ~]# <literal>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3</literal> \
<literal>cidr_netmask=24 --group apachegroup</literal>

[root@z1 ~]# <literal>pcs resource create Website apache</literal> \
<literal>configfile="/etc/httpd/conf/httpd.conf"</literal> \
<literal>statusurl="http://127.0.0.1/server-status" --group apachegroup</literal></literallayout>
</listitem>
<listitem>
<simpara>After creating the resources and the resource group that
contains them,
you can check the status of the cluster. Note that all
four resources are running on the same node.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
Cluster name: my_cluster
Last updated: Wed Jul 31 16:38:51 2013
Last change: Wed Jul 31 16:42:14 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM):	Started z1.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z1.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z1.example.com
     Website	(ocf::heartbeat:apache):	Started z1.example.com</literallayout>
<simpara>Note that if you have not configured a fencing device for your cluster,
by default the resources do not start.</simpara>
</listitem>
<listitem>
<simpara>Once the cluster is up and running, you can point a browser to the IP
address you defined as the <literal>IPaddr2</literal> resource
to view the sample display, consisting of
the simple word "Hello".</simpara>
<literallayout class="monospaced">Hello</literallayout>
<simpara>If you find that the resources you configured are not running,
you can run the <literal role="command">pcs resource debug-start
<emphasis>resource</emphasis></literal> command
to test the resource configuration.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_testing-resource-configuration-in-a-cluster-configuring-ha-http">
<title>Testing the resource configuration</title>
<simpara>In the cluster status display shown in
<link linkend="proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http">Creating the resources and resource groups</link>,
all of the resources are running
on node <literal>z1.example.com</literal>.
You can test whether the resource group
fails over to node
<literal>z2.example.com</literal>
by using the following procedure to put the first node in <literal>standby</literal>
mode, after which the node will no longer
be able to host resources.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The following command puts node <literal>z1.example.com</literal>
in <literal>standby</literal> mode.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs node standby z1.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>After putting node <literal>z1</literal> in <literal>standby</literal> mode, check the cluster
status. Note that the resources should now all be running on <literal>z2</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
Cluster name: my_cluster
Last updated: Wed Jul 31 17:16:17 2013
Last change: Wed Jul 31 17:18:34 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Node z1.example.com (1): standby
Online: [ z2.example.com ]

Full list of resources:

 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM):	Started z2.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z2.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z2.example.com
     Website	(ocf::heartbeat:apache):	Started z2.example.com</literallayout>
<simpara>The web site at the defined IP address should still display, without
interruption.</simpara>
</listitem>
<listitem>
<simpara>To remove <literal>z1</literal> from <literal>standby</literal> mode,
enter the following command.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs node unstandby z1.example.com</literal></literallayout>
<note>
<simpara>Removing a node from <literal>standby</literal> mode does not
in itself cause the resources to fail back over to that node.
This will depend on the <literal>resource-stickiness</literal>
value for the resources. For information on the
<literal>resource-stickiness</literal> meta attribute, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</link>.</simpara>
</note>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters">
<title>Configuring an active/passive NFS server in a Red Hat High Availability cluster</title>
<simpara>The following procedure configures
a highly available active/passive NFS server on a two-node
Red Hat Enterprise Linux High Availability Add-On
cluster using shared storage. The procedure
uses <literal role="command">pcs</literal>
to configure Pacemaker cluster resources.
In this use case, clients access the NFS file system
through
a floating IP address.
The NFS server runs on one of two nodes in the cluster.
If the node on which the NFS server is running
becomes inoperative, the NFS server starts up again on the
second node of the cluster with minimal service interruption.</simpara>
<section xml:id="prerequisites" remap="_prerequisites">
<title>Prerequisites</title>
<simpara>This use case
requires that your system include the following
components:</simpara>
<itemizedlist>
<listitem>
<simpara>A two-node Red Hat High Availability cluster with power fencing configured
for each node.
We recommend but do not require a private network. This procedure uses
the cluster example provided in
<link linkend="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</link>.</simpara>
</listitem>
<listitem>
<simpara>A public virtual IP address, required for the NFS server.</simpara>
</listitem>
<listitem>
<simpara>Shared storage for the nodes in the cluster, using
iSCSI, Fibre Channel, or other shared network block device.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="procedural_overview" remap="_procedural_overview">
<title>Procedural overview</title>
<simpara>Configuring
a highly available active/passive NFS server on an existing two-node
Red Hat Enterprise Linux High Availability cluster requires that you perform
the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure an <literal>ext4</literal> file system on the
LVM logical volume <literal>my_lv</literal> on the shared
storage for the nodes in the cluster.</simpara>
</listitem>
<listitem>
<simpara>Configure an NFS share on the shared storage on the
LVM logical volume.</simpara>
</listitem>
<listitem>
<simpara>Create the cluster resources.</simpara>
</listitem>
<listitem>
<simpara>Test the NFS server you have configured.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">
<title>Configuring an LVM volume with an ext4 file system in a Pacemaker cluster</title>
<simpara>This use case requires that you create
an LVM logical volume on storage that is
shared between the nodes of the cluster.</simpara>
<note>
<simpara>LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected
to the cluster nodes only.</simpara>
</note>
<simpara>The following procedure creates an LVM logical
volume and then creates an ext4 file
system on that volume for use in a Pacemaker cluster. In this example,
the shared partition <literal>/dev/sdb1</literal>
is used to store the LVM physical volume from which
the LVM logical volume will be created.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On both nodes of the cluster, perform the following steps to set the value for the LVM system ID
to the value of the <literal>uname</literal> identifier for the system.
The LVM system ID
will be used to ensure that only the cluster is capable of
activating the volume group.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Set the <literal>system_id_source</literal> configuration option in the <literal>/etc/lvm/lvm.conf</literal> configuration file to <literal>uname</literal>.</simpara>
<literallayout class="monospaced"># Configuration option global/system_id_source.
system_id_source = "uname"</literallayout>
</listitem>
<listitem>
<simpara>Verify that the LVM system ID on the node matches the <literal>uname</literal> for the node.</simpara>
<literallayout class="monospaced"># <literal>lvm systemid</literal>
  system ID: z1.example.com
# <literal>uname -n</literal>
  z1.example.com</literallayout>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the LVM volume and create an ext4 file system on that volume. Since the <literal>/dev/sdb1</literal> partition
is storage that is shared,
you perform this part of the procedure on one node only.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an LVM physical volume on partition
<literal>/dev/sdb1</literal>.</simpara>
<literallayout class="monospaced"># <literal>pvcreate /dev/sdb1</literal>
  Physical volume "/dev/sdb1" successfully created</literallayout>
</listitem>
<listitem>
<simpara>Create the volume group <literal>my_vg</literal> that
consists of the physical volume <literal>/dev/sdb1</literal>.</simpara>
<literallayout class="monospaced"># <literal>vgcreate my_vg /dev/sdb1</literal>
  Volume group "my_vg" successfully created</literallayout>
</listitem>
<listitem>
<simpara>Verify that the new volume group has the system ID of the
node on which you are running and from which you created the volume group.</simpara>
<literallayout class="monospaced"># <literal>vgs -o+systemid</literal>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</literallayout>
</listitem>
<listitem>
<simpara>Create a logical volume using the volume group
<literal>my_vg</literal>.</simpara>
<literallayout class="monospaced"># <literal>lvcreate -L450 -n my_lv my_vg</literal>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</literallayout>
<simpara>You can use the <literal role="command">lvs</literal> command to display the logical
volume.</simpara>
<literallayout class="monospaced"># <literal>lvs</literal>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</literallayout>
</listitem>
<listitem>
<simpara>Create an ext4 file system on the
logical volume <literal>my_lv</literal>.</simpara>
<literallayout class="monospaced"># <literal>mkfs.ext4 /dev/my_vg/my_lv</literal>
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 462848 1k blocks and 115824 inodes
...</literallayout>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_ensuring-cluster-volume-not-multiply-activated-configuring-ha-nfs">
<title>Ensuring a volume group is not activated on multiple cluster nodes</title>
<simpara role="_abstract">This procedure ensures that volume groups
that are managed by Pacemaker in a cluster
will not be automatically activated on startup.
If a volume group is automatically activated on startup
rather than by Pacemaker,
there is a risk that the volume group will be active on
multiple nodes at the same time, which
could corrupt the volume group’s metadata.</simpara>
<simpara>This procedure modifies the <literal>auto_activation_volume_list</literal> entry
in the <literal>/etc/lvm/lvm.conf</literal> configuration file.
The <literal>auto_activation_volume_list</literal>
entry is used to limit autoactivation to
specific logical volumes.
Setting <literal>auto_activation_volume_list</literal>
to an empty list disables autoactivation entirely.</simpara>
<simpara>Any local volumes that are not shared and are not managed
by Pacemaker should be included in the <literal>auto_activation_volume_list</literal> entry,
including volume groups related to the node’s local
root and home directories.
All volume groups managed by the cluster manager must
be excluded from the <literal>auto_activation_volume_list</literal> entry.</simpara>
<formalpara>
<title>Procedure</title>
<para>Perform the following procedure on each node in the cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Determine which volume groups are currently configured on your
local storage with the following command.
This will output a list of the currently-configured volume groups.
If you have space allocated in separate volume
groups for root and for your home directory on this
node, you will see
those volumes in the output, as in this example.</simpara>
<literallayout class="monospaced"># <literal>vgs --noheadings -o vg_name</literal>
  my_vg
  rhel_home
  rhel_root</literallayout>
</listitem>
<listitem>
<simpara>Add the volume groups other than <literal>my_vg</literal> (the volume
group you have just defined for the cluster) as entries
to <literal>auto_activation_volume_list</literal> in the <literal>/etc/lvm/lvm.conf</literal> configuration file.</simpara>
<simpara>For example, if you have space allocated in separate volume
groups for root and for your home directory, you would uncomment
the <literal>auto_activation_volume_list</literal> line of the <literal>lvm.conf</literal> file and add
these volume groups as entries to <literal>auto_activation_volume_list</literal> as follows. Note that
the volume group you have just defined for the cluster (<literal>my_vg</literal> in
this example) is not in this list.</simpara>
<literallayout class="monospaced">auto_activation_volume_list = [ "rhel_root", "rhel_home" ]</literallayout>
<note>
<simpara>If no local volume groups are present on a node to be
activated outside of the cluster manager, you must still
initialize the <literal>auto_activation_volume_list</literal> entry
as <literal>auto_activation_volume_list = []</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Rebuild the <literal>initramfs</literal> boot image to
guarantee that the boot image will not try to activate
a volume group controlled by the cluster. Update the
<literal>initramfs</literal>
device with the following command.
This command may take up to a minute to complete.</simpara>
<literallayout class="monospaced"># <literal>dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)</literal></literallayout>
</listitem>
<listitem>
<simpara>Reboot the node.</simpara>
<note>
<simpara>If you have installed a new Linux kernel since booting the node on which you created the
boot image, the new <literal>initrd</literal> image will be for the kernel that was running when you created it
and not for the new kernel that is running when you reboot the
node. You can ensure that the correct <literal>initrd</literal> device is in use by running
the <literal>uname -r</literal> command before and after the reboot to determine the kernel release that is running.
If the releases are not the same, update the <literal>initrd</literal> file after rebooting
with the new kernel and then reboot the node.</simpara>
</note>
</listitem>
<listitem>
<simpara>When the node has rebooted, check whether the cluster services have started up again
on that node by executing the <literal>pcs cluster status</literal> command on that
node. If this yields the message <literal>Error: cluster is not currently running on this node</literal>
then enter the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster start</literal></literallayout>
<simpara>Alternately, you can wait until you have rebooted each node
in the cluster and start cluster services on all of the nodes in the cluster with
the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster start --all</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring-nfs-share-configuring-ha-nfs">
<title>Configuring an NFS share</title>
<simpara>The following procedure configures the
NFS share for
the NFS service failover.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On both nodes in the cluster, create the <literal>/nfsshare</literal> directory.</simpara>
<literallayout class="monospaced"># <literal>mkdir /nfsshare</literal></literallayout>
</listitem>
<listitem>
<simpara>On one node in the cluster, perform the following procedure.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Mount the
ext4 file system that you created in
<link linkend="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an ext4 file system</link>
on the <literal>/nfsshare</literal>
directory.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>mount /dev/my_vg/my_lv /nfsshare</literal></literallayout>
</listitem>
<listitem>
<simpara>Create an <literal>exports</literal> directory tree on the <literal>/nfsshare</literal> directory.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>mkdir -p /nfsshare/exports</literal>
[root@z1 ~]# <literal>mkdir -p /nfsshare/exports/export1</literal>
[root@z1 ~]# <literal>mkdir -p /nfsshare/exports/export2</literal></literallayout>
</listitem>
<listitem>
<simpara>Place files in the <literal>exports</literal> directory for the NFS clients
to access. For this example, we are creating test files named
<literal>clientdatafile1</literal> and
<literal>clientdatafile2</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>touch /nfsshare/exports/export1/clientdatafile1</literal>
[root@z1 ~]# <literal>touch /nfsshare/exports/export2/clientdatafile2</literal></literallayout>
</listitem>
<listitem>
<simpara>Unmount the ext4 file system and deactivate the
LVM volume group.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>umount /dev/my_vg/my_lv</literal>
[root@z1 ~]# <literal>vgchange -an my_vg</literal></literallayout>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs">
<title>Configuring the resources and resource group for an NFS server in a cluster</title>
<simpara>This section provides the
procedure for configuring the cluster resources
for this use case.</simpara>
<note>
<simpara>If you have not configured a fencing device for your cluster,
by default the resources do not start.</simpara>
<simpara>If you find that the resources you configured are not running,
you can run the
<literal role="command">pcs resource debug-start <emphasis>resource</emphasis></literal> command
to test the resource configuration.
This starts the service outside of the cluster’s control and knowledge.
At the point the configured resources are running again,
run <literal role="command">pcs resource cleanup <emphasis>resource</emphasis></literal>  to make the cluster aware
of the updates.</simpara>
</note>
<simpara>The following procedure configures the system resources.
To ensure these resources all run on the same node, they
are configured as part of the
resource group <literal>nfsgroup</literal>.
The resources will start in the order in which you add them to the group,
and they will stop in
the reverse order in which they are added to the group.
Run this procedure from one node of the cluster only.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the LVM-activate resource named <literal>my_lvm</literal>.
Because the resource group <literal>nfsgroup</literal> does not yet exist, this
command creates the resource group.</simpara>
<warning>
<simpara>Do not configure more than one <literal>LVM-activate</literal> resource that uses the same LVM volume group in an active/passive HA configuration,
as this risks data corruption.  Additionally, do not configure an <literal>LVM-activate</literal> resource as a clone resource in an active/passive
HA configuration.</simpara>
</warning>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create my_lvm ocf:heartbeat:LVM-activate</literal> <literal>vgname=my_vg</literal> <literal>vg_access_mode=system_id --group nfsgroup</literal></literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster to verify that the resource is running.</simpara>
<literallayout class="monospaced">root@z1 ~]#  <literal>pcs status</literal>
Cluster name: my_cluster
Last updated: Thu Jan  8 11:13:17 2015
Last change: Thu Jan  8 11:13:08 2015
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.12-a14efad
2 Nodes configured
3 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</literallayout>
</listitem>
<listitem>
<simpara>Configure a <literal>Filesystem</literal> resource for the cluster.</simpara>
<simpara>The following command configures an ext4 <literal>Filesystem</literal> resource
named <literal>nfsshare</literal>
as part of the <literal>nfsgroup</literal> resource group.
This file system
uses the LVM volume group and ext4 file system you created in
<link linkend="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an ext4 file system</link>
and will be mounted
on the <literal>/nfsshare</literal> directory you created in
<link linkend="proc_configuring-nfs-share-configuring-ha-nfs">Configuring an NFS share</link>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create nfsshare Filesystem</literal> \
<literal>device=/dev/my_vg/my_lv directory=/nfsshare</literal> \
<literal>fstype=ext4 --group nfsgroup</literal></literallayout>
<simpara>You can specify mount options as part of the resource configuration for
a <literal>Filesystem</literal> resource with the
<literal>options=<emphasis>options</emphasis></literal> parameter. Run the
<literal role="command">pcs resource describe Filesystem</literal> command
for full configuration options.</simpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>my_lvm</literal> and <literal>nfsshare</literal> resources are running.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
...</literallayout>
</listitem>
<listitem>
<simpara>Create the <literal>nfsserver</literal> resource named <literal>nfs-daemon</literal>
as part of the resource group <literal>nfsgroup</literal>.</simpara>
<note>
<simpara>The <literal>nfsserver</literal> resource allows
you to specify an <literal>nfs_shared_infodir</literal> parameter,
which is a directory that NFS servers use to store
NFS-related stateful information.</simpara>
<simpara>It is recommended that this attribute be set to a subdirectory of one of
the <literal>Filesystem</literal> resources you created in this collection of exports.
This ensures that the NFS servers are storing their stateful information on a device
that will become available to another node if this resource group
needs to relocate. In this example;</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/nfsshare</literal> is the
shared-storage directory managed by the <literal>Filesystem</literal> resource</simpara>
</listitem>
<listitem>
<simpara><literal>/nfsshare/exports/export1</literal> and <literal>/nfsshare/exports/export2</literal> are the export directories</simpara>
</listitem>
<listitem>
<simpara><literal>/nfsshare/nfsinfo</literal> is the shared-information directory for the
<literal>nfsserver</literal> resource</simpara>
</listitem>
</itemizedlist>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create nfs-daemon nfsserver</literal> \
<literal>nfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true</literal> \
<literal>--group nfsgroup</literal>

[root@z1 ~]# <literal>pcs status</literal>
...</literallayout>
</listitem>
<listitem>
<simpara>Add the <literal>exportfs</literal> resources to export the <literal>/nfsshare/exports</literal>
directory. These resources are part of the resource group <literal>nfsgroup</literal>.
This builds a virtual directory for NFSv4 clients. NFSv3 clients can access these exports as well.</simpara>
<note>
<simpara>The <literal>fsid=0</literal> option is required only if you want to create
a virtual directory for NFSv4 clients.  For more information, see
<link xlink:href="https://access.redhat.com/solutions/548083/">How do I configure the fsid option in an NFS server’s /etc/exports file?</link>.</simpara>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create  nfs-root exportfs</literal> \
<literal>clientspec=192.168.122.0/255.255.255.0</literal> \
<literal>options=rw,sync,no_root_squash</literal> \
<literal>directory=/nfsshare/exports</literal> \
<literal>fsid=0 --group nfsgroup</literal>

[root@z1 ~]# # <literal>pcs resource create  nfs-export1 exportfs</literal> \
<literal>clientspec=192.168.122.0/255.255.255.0</literal> \
<literal>options=rw,sync,no_root_squash directory=/nfsshare/exports/export1</literal> \
<literal>fsid=1 --group nfsgroup</literal>

[root@z1 ~]# # <literal>pcs resource create  nfs-export2 exportfs</literal> \
<literal>clientspec=192.168.122.0/255.255.255.0</literal> \
<literal>options=rw,sync,no_root_squash directory=/nfsshare/exports/export2</literal> \
<literal>fsid=2 --group nfsgroup</literal></literallayout>
</listitem>
<listitem>
<simpara>Add the floating IP address resource that NFS clients will use to access the NFS share.
This resource is part of the resource group <literal>nfsgroup</literal>.
For this example deployment, we are using 192.168.122.200 as the floating IP address.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create nfs_ip IPaddr2</literal> \
<literal>ip=192.168.122.200 cidr_netmask=24 --group nfsgroup</literal></literallayout>
</listitem>
<listitem>
<simpara>Add an <literal>nfsnotify</literal> resource for sending NFSv3
reboot notifications once the entire NFS deployment has initialized.
This resource is part of the resource group <literal>nfsgroup</literal>.</simpara>
<note>
<simpara>For the NFS notification to be processed correctly, the floating IP address
must have a host name associated with it that is consistent on both the NFS servers and the NFS client.</simpara>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create nfs-notify nfsnotify</literal> \
<literal>source_host=192.168.122.200 --group nfsgroup</literal></literallayout>
</listitem>
<listitem>
<simpara>After creating the resources and the resource constraints,
you can check the status of the cluster. Note that all
resources are running on the same node.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_testing-nfs-resource-configuration-configuring-ha-nfs">
<title>Testing the NFS resource configuration</title>
<simpara>You can validate your system configuration with the following procedures.
You should be able to mount the exported file system with either
NFSv3 or NFSv4.</simpara>
<section xml:id="testing_the_nfs_export" remap="_testing_the_nfs_export">
<title>Testing the NFS export</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>On a node outside of the cluster, residing in the same network as the
deployment, verify that the NFS share can be seen by mounting
the NFS share. For this example, we are using the 192.168.122.0/24 network.</simpara>
<literallayout class="monospaced"># <literal>showmount -e 192.168.122.200</literal>
Export list for 192.168.122.200:
/nfsshare/exports/export1 192.168.122.0/255.255.255.0
/nfsshare/exports         192.168.122.0/255.255.255.0
/nfsshare/exports/export2 192.168.122.0/255.255.255.0</literallayout>
</listitem>
<listitem>
<simpara>To verify that you can mount the NFS share with NFSv4,
mount the NFS share to a directory on the client node.
After mounting, verify that the contents of the export directories are visible.
Unmount the share after testing.</simpara>
<literallayout class="monospaced"># <literal>mkdir nfsshare</literal>
# <literal>mount -o "vers=4" 192.168.122.200:export1 nfsshare</literal>
# <literal>ls nfsshare</literal>
clientdatafile1
# <literal>umount nfsshare</literal></literallayout>
</listitem>
<listitem>
<simpara>Verify that you can mount the NFS share with NFSv3.
After mounting,
verify that the test file <literal>clientdatafile1</literal>
is visible.
Unlike NFSv4, since NFSv3 does not use the virtual file
system, you must mount a specific export.
Unmount the share after testing.</simpara>
<literallayout class="monospaced"># <literal>mkdir nfsshare</literal>
# <literal>mount -o "vers=3" 192.168.122.200:/nfsshare/exports/export2 nfsshare</literal>
# <literal>ls nfsshare</literal>
clientdatafile2
# <literal>umount nfsshare</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="testing_for_failover" remap="_testing_for_failover">
<title>Testing for failover</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>On a node outside of the cluster, mount the NFS share and verify access to
the <literal>clientdatafile1</literal> we created in
<link linkend="proc_configuring-nfs-share-configuring-ha-nfs">Configuring an NFS share</link></simpara>
<literallayout class="monospaced"># <literal>mkdir nfsshare</literal>
# <literal>mount -o "vers=4" 192.168.122.200:export1 nfsshare</literal>
# <literal>ls nfsshare</literal>
clientdatafile1</literallayout>
</listitem>
<listitem>
<simpara>From a node within the cluster, determine which node in the cluster is running
<literal>nfsgroup</literal>. In this example,
<literal>nfsgroup</literal> is running
on
<literal>z1.example.com</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</literallayout>
</listitem>
<listitem>
<simpara>From a node within the cluster, put the node that is running <literal>nfsgroup</literal>
in standby mode.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs node standby z1.example.com</literal></literallayout>
</listitem>
<listitem>
<simpara>Verify that <literal>nfsgroup</literal> successfully starts on
the other cluster node.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status</literal>
...
Full list of resources:
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM):   Started z2.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z2.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z2.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z2.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z2.example.com
...</literallayout>
</listitem>
<listitem>
<simpara>From the node outside the cluster on which you have mounted the NFS
share,
verify that this outside node still
continues to have access to the test file within the NFS mount.</simpara>
<literallayout class="monospaced"># <literal>ls nfsshare</literal>
clientdatafile1</literallayout>
<simpara>Service will be lost briefly for the client
during the failover but the client should
recover it with no user intervention. By default, clients using
NFSv4 may take up to 90 seconds to recover the mount;
this 90 seconds represents the NFSv4 file lease grace period observed by the server on
startup. NFSv3 clients should recover access to the mount in a matter of a few seconds.</simpara>
</listitem>
<listitem>
<simpara>From a node within the cluster, remove the node that was initially running
running <literal>nfsgroup</literal>
from standby mode.</simpara>
<note>
<simpara>Removing a node from <literal>standby</literal> mode does not
in itself cause the resources to fail back over to that node.
This will depend on the <literal>resource-stickiness</literal>
value for the resources. For information on the
<literal>resource-stickiness</literal> meta attribute, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</link>.</simpara>
</note>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs node unstandby z1.example.com</literal></literallayout>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters">
<title>GFS2 file systems in a cluster</title>
<simpara>This section provides:</simpara>
<itemizedlist>
<listitem>
<simpara>A procedure to set up a Pacemaker cluster that includes GFS2 file systems
file systems</simpara>
</listitem>
<listitem>
<simpara>A procedure to set up a Pacemaker cluster with an encrypted GFS2 file system</simpara>
</listitem>
<listitem>
<simpara>A procedure to migrate RHEL 7 logical volumes that contain GFS2 file systems to a RHEL 8 cluster</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster">
<title>Configuring a GFS2 file system in a cluster</title>
<simpara>This procedure is an outline of the steps
required to set up a Pacemaker cluster that includes GFS2
file systems. This example creates three GFS2 file systems on three logical volumes.</simpara>
<itemizedlist>
<title>Prerequisities</title>
<listitem>
<simpara>Install and start the cluster software on all nodes and
create a basic two-node cluster.</simpara>
</listitem>
<listitem>
<simpara>Configure fencing for the cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on creating a Pacemaker cluster and configuring
fencing for the cluster, see
<link linkend="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</link>.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On both nodes of the cluster,
install the <literal>lvm2-lockd</literal>, <literal>gfs2-utils</literal>, and <literal>dlm</literal> packages.
To support these packages, you must be subscribed to the AppStream channel
and the Resilient Storage channel.</simpara>
<literallayout class="monospaced"># <literal>yum install lvm2-lockd gfs2-utils dlm</literal></literallayout>
</listitem>
<listitem>
<simpara>Set the global Pacemaker parameter <literal>no-quorum-policy</literal> to <literal>freeze</literal>.</simpara>
<note>
<simpara>By default, the value of <literal>no-quorum-policy</literal> is set to <literal>stop</literal>, indicating that
once quorum is lost, all the resources on the remaining partition will immediately
be stopped. Typically this default is the safest and most optimal option,
but unlike most resources, GFS2 requires quorum to function. When
quorum is lost both the applications using the GFS2 mounts
and the GFS2 mount itself cannot be correctly stopped. Any attempts
to stop these resources without quorum will fail which will ultimately
result in the entire cluster being fenced every time quorum is lost.</simpara>
<simpara>To address this situation, set <literal>no-quorum-policy</literal> to <literal>freeze</literal> when
GFS2 is in use. This means that when quorum is lost, the remaining partition
will do nothing until quorum is regained.</simpara>
</note>
<literallayout class="monospaced"># <literal>pcs property set no-quorum-policy=freeze</literal></literallayout>
</listitem>
<listitem>
<simpara>Set up a <literal>dlm</literal> resource. This is
a required dependency for configuring a GFS2 file system in a cluster.
This example creates the <literal>dlm</literal> resource as part of a resource group named <literal>locking</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</literal></literallayout>
</listitem>
<listitem>
<simpara>Clone the <literal>locking</literal> resource group so that the resource group can be active on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource clone locking interleave=true</literal></literallayout>
</listitem>
<listitem>
<simpara>Set up an <literal>lvmlockd</literal> resource as part of the group <literal>locking</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</literal></literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster to ensure that the <literal>locking</literal> resource group has started on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status --full</literal>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</literallayout>
</listitem>
<listitem>
<simpara>On one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems,
and the other volume group will contain one GFS2 file system.</simpara>
<simpara>The following command creates the shared volume group <literal>shared_vg1</literal> on <literal>/dev/vdb</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>vgcreate --shared shared_vg1 /dev/vdb</literal>
  Physical volume "/dev/vdb" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
<simpara>The following command creates the shared volume group <literal>shared_vg2</literal> on <literal>/dev/vdc</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>vgcreate --shared shared_vg2 /dev/vdc</literal>
  Physical volume "/dev/vdc" successfully created.
  Volume group "shared_vg2" successfully created
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
</listitem>
<listitem>
<simpara>On the second node in the cluster, start the lock manager for each of the shared volume groups.</simpara>
<literallayout class="monospaced">[root@z2 ~]# <literal>vgchange --lock-start shared_vg1</literal>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@z2 ~]# <literal>vgchange --lock-start shared_vg2</literal>
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
</listitem>
<listitem>
<simpara>On one node in the cluster, create the shared logical volumes and format the volumes
with a GFS2 file system. One journal is required for each node that mounts the file system.
Ensure that you create enough journals for each of the nodes in your cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</literal>
  Logical volume "shared_lv1" created.
[root@z1 ~]# <literal>lvcreate --activate sy -L5G -n shared_lv2 shared_vg1</literal>
  Logical volume "shared_lv2" created.
[root@z1 ~]# <literal>lvcreate --activate sy -L5G -n shared_lv1 shared_vg2</literal>
  Logical volume "shared_lv1" created.

[root@z1 ~]# <literal>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1</literal>
[root@z1 ~]# <literal>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2</literal>
[root@z1 ~]# <literal>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1</literal></literallayout>
</listitem>
<listitem>
<simpara>Create an <literal>LVM-activate</literal> resource for each logical volume to automatically activate that logical volume on all nodes.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an <literal>LVM-activate</literal> resource named <literal>sharedlv1</literal> for the logical volume <literal>shared_lv1</literal> in volume group <literal>shared_vg1</literal>.
This command also creates the
resource group <literal>shared_vg1</literal> that includes the resource.  In this example, the resource group has the
same name as the shared volume group that includes the logical volume.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</literal></literallayout>
</listitem>
<listitem>
<simpara>Create an <literal>LVM-activate</literal> resource named <literal>sharedlv2</literal> for the logical volume <literal>shared_lv2</literal> in volume group <literal>shared_vg1</literal>. This resource will also be part of the resource group <literal>shared_vg1</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</literal></literallayout>
</listitem>
<listitem>
<simpara>Create an <literal>LVM-activate</literal> resource named <literal>sharedlv3</literal> for the logical volume <literal>shared_lv1</literal> in volume group <literal>shared_vg2</literal>.
This command also creates the
resource group <literal>shared_vg2</literal> that includes the resource.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd</literal></literallayout>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Clone the two new resource groups.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource clone shared_vg1 interleave=true</literal>
[root@z1 ~]# <literal>pcs resource clone shared_vg2 interleave=true</literal></literallayout>
</listitem>
<listitem>
<simpara>Configure ordering constraints to ensure that the <literal>locking</literal> resource group
that includes the <literal>dlm</literal> and <literal>lvmlockd</literal> resources starts first.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs constraint order start locking-clone then shared_vg1-clone</literal>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)
[root@z1 ~]# <literal>pcs constraint order start locking-clone then shared_vg2-clone</literal>
Adding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)</literallayout>
</listitem>
<listitem>
<simpara>Configure colocation constraints to ensure that the <literal>vg1</literal> and <literal>vg2</literal> resource groups
start on the same node as the <literal>locking</literal> resource group.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs constraint colocation add shared_vg1-clone with locking-clone</literal>
[root@z1 ~]# <literal>pcs constraint colocation add shared_vg2-clone with locking-clone</literal></literallayout>
</listitem>
<listitem>
<simpara>On both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>lvs</literal>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g

[root@z2 ~]# <literal>lvs</literal>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g</literallayout>
</listitem>
<listitem>
<simpara>Create a file system resource to automatically mount each GFS2 file system on all nodes.</simpara>
<simpara>You should not add the file system to the <literal>/etc/fstab</literal>
file because it will be managed as a Pacemaker cluster resource.
Mount options can be specified as part of the resource configuration with
<literal>options=<emphasis>options</emphasis></literal>. Run the
<literal role="command">pcs resource describe Filesystem</literal> command
for full configuration options.</simpara>
<simpara>The following commands create the file system resources. These commands add each resource to the resource group that
includes the logical volume resource for that file system.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</literal>
[root@z1 ~]# <literal>pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv2" directory="/mnt/gfs2" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</literal>
[root@z1 ~]# <literal>pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device="/dev/shared_vg2/shared_lv1" directory="/mnt/gfs3" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</literal></literallayout>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification steps</title>
<listitem>
<simpara>Verify that the GFS2 file systems are mounted on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>mount | grep gfs2</literal>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <literal>mount | grep gfs2</literal>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)</literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status --full</literal>
Cluster name: my_cluster
[...]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg1:1
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg2-clone [shared_vg2]
     Resource Group: shared_vg2:0
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg2:1
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]

...</literallayout>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For information on configuring shared block storage for a Red Hat High Availability cluster with Microsoft Azure Shared Disks, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_red_hat_enterprise_linux_8_on_public_cloud_platforms/configuring-rhel-high-availability-on-azure_cloud-content#azure-configuring-shared-block-storage_configuring-rhel-high-availability-on-azure">Configuring shared block storage</link>.</simpara>
</listitem>
<listitem>
<simpara>For information on configuring shared block storage for a Red Hat High Availability cluster
with Amazon EBS Multi-Attach volumes, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_red_hat_enterprise_linux_8_on_public_cloud_platforms/configuring-a-red-hat-high-availability-cluster-on-aws_deploying-a-virtual-machine-on-aws#aws-configuring-shared-block-storage_configuring-a-red-hat-high-availability-cluster-on-aws">Configuring shared block storage</link>.</simpara>
</listitem>
<listitem>
<simpara>For information on configuring shared block storage for a Red Hat High Availability cluster om Alibaba Cloud, see
<link xlink:href="https://access.redhat.com/articles/5371181">Configuring Shared Block Storage for a Red Hat High Availability Cluster on Alibaba Cloud</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster">
<title>Configuring an encrypted GFS2 file system in a cluster (RHEL 8.4 and later)</title>
<simpara role="_abstract">This procedure creates a Pacemaker cluster that includes a LUKS encrypted GFS2
file system. This example creates one GFS2 file systems on a logical volume and
encrypts the file system.
Encrypted GFS2 file systems are supported using the
<literal>crypt</literal> resource agent which provides support for LUKS encryption.</simpara>
<itemizedlist>
<title>Prerequisities</title>
<listitem>
<simpara>Install and start the cluster software on all nodes and
create a basic two-node cluster.</simpara>
</listitem>
<listitem>
<simpara>Configure fencing for the cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on creating a Pacemaker cluster and configuring
fencing for the cluster, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</link>.</simpara>
<formalpara>
<title>Procedure</title>
<para>Configure a shared logical volume in a Pacemaker cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On both nodes of the cluster,
install the <literal>lvm2-lockd</literal>, <literal>gfs2-utils</literal>, and <literal>dlm</literal> packages.
To support these packages, you must be subscribed to
the AppStream and Resilient Storage channel.</simpara>
<literallayout class="monospaced"># <literal>yum install lvm2-lockd gfs2-utils dlm</literal></literallayout>
</listitem>
<listitem>
<simpara>Set the global Pacemaker parameter <literal>no-quorum-policy</literal> to <literal>freeze</literal>.</simpara>
<note>
<simpara>By default, the value of <literal>no-quorum-policy</literal> is set to <literal>stop</literal>, indicating that
once quorum is lost, all the resources on the remaining partition will immediately
be stopped. Typically this default is the safest and most optimal option,
but unlike most resources, GFS2 requires quorum to function. When
quorum is lost both the applications using the GFS2 mounts
and the GFS2 mount itself cannot be correctly stopped. Any attempts
to stop these resources without quorum will fail which will ultimately
result in the entire cluster being fenced every time quorum is lost.</simpara>
<simpara>To address this situation, set <literal>no-quorum-policy</literal> to <literal>freeze</literal> when
GFS2 is in use. This means that when quorum is lost, the remaining partition
will do nothing until quorum is regained.</simpara>
</note>
<literallayout class="monospaced"># <literal>pcs property set no-quorum-policy=freeze</literal></literallayout>
</listitem>
<listitem>
<simpara>Set up a <literal>dlm</literal> resource. This is
a required dependency for configuring a GFS2 file system in a cluster.
This example creates the <literal>dlm</literal> resource as part of a resource group named <literal>locking</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</literal></literallayout>
</listitem>
<listitem>
<simpara>Clone the <literal>locking</literal> resource group so that the resource group can be active on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource clone locking interleave=true</literal></literallayout>
</listitem>
<listitem>
<simpara>Set up an <literal>lvmlockd</literal> resource as part of the group <literal>locking</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</literal></literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster to ensure that the <literal>locking</literal> resource group has started on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status --full</literal>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</literallayout>
</listitem>
<listitem>
<simpara>On one node of the cluster, create a shared volume group.</simpara>
<simpara>The following command creates the shared volume group <literal>shared_vg1</literal> on <literal>/dev/sda1</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>vgcreate --shared shared_vg1 /dev/sda1</literal>
  Physical volume "/dev/sda1" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
</listitem>
<listitem>
<simpara>On the second node in the cluster, start the lock manager for the shared volume group.</simpara>
<literallayout class="monospaced">[root@z2 ~]# <literal>vgchange --lock-start shared_vg1</literal>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@z2 ~]# <literal>vgchange --lock-start shared_vg2</literal>
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
</listitem>
<listitem>
<simpara>On one node in the cluster, create the shared logical volume.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</literal>
  Logical volume "shared_lv1" created.</literallayout>
</listitem>
<listitem>
<simpara>Create an <literal>LVM-activate</literal> resource for the logical volume to automatically activate the logical volume on all nodes.</simpara>
<simpara>The following command creates an <literal>LVM-activate</literal> resource named <literal>sharedlv1</literal> for the logical volume <literal>shared_lv1</literal> in
volume group <literal>shared_vg1</literal>.
This command also creates the
resource group <literal>shared_vg1</literal> that includes the resource.  In this example, the resource group has the
same name as the shared volume group that includes the logical volume.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</literal></literallayout>
</listitem>
<listitem>
<simpara>Clone the new resource group.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource clone shared_vg1 interleave=true</literal></literallayout>
</listitem>
<listitem>
<simpara>Configure an ordering constraints to ensure that the <literal>locking</literal> resource group
that includes the <literal>dlm</literal> and <literal>lvmlockd</literal> resources starts first.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs constraint order start locking-clone then shared_vg1-clone</literal>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)</literallayout>
</listitem>
<listitem>
<simpara>Configure a colocation constraints to ensure that the <literal>vg1</literal> and <literal>vg2</literal> resource groups
start on the same node as the <literal>locking</literal> resource group.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs constraint colocation add shared_vg1-clone with locking-clone</literal></literallayout>
</listitem>
<listitem>
<simpara>On both nodes in the cluster, verify that the logical volume is active. There may be a delay of a few seconds.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>lvs</literal>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g

[root@z2 ~]# <literal>lvs</literal>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g</literallayout>
</listitem>
</orderedlist>
<simpara>After configuring a shared logical volume in a Pacemaker cluster, encrypt the logical volume and create a
<literal>crypt</literal> resource.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On one node in the cluster, create the file that will contain the crypt
key and set the permissions on the file so that it is readable only by root.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>touch /etc/crypt_keyfile</literal>
[root@z1 ~]# <literal>chmod 600 /etc/crypt_keyfile</literal></literallayout>
</listitem>
<listitem>
<simpara>Create the crypt key.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>dd if=/dev/urandom bs=4K count=1 of=/etc/crypt_keyfile</literal>
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000306202 s, 13.4 MB/s
[root@z1 ~]# <literal>scp /etc/crypt_keyfile root@z2.example.com:/etc/</literal></literallayout>
</listitem>
<listitem>
<simpara>Distribute the crypt keyfile to the other nodes in the cluster, using the <literal>-p</literal> parameter to preserve the permissions you set.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>scp -p /etc/crypt_keyfile root@z2.example.com:/etc/</literal></literallayout>
</listitem>
<listitem>
<simpara>Create the encrypted device on the LVM volume where you will configure the encrypted GFS2 file system.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>cryptsetup luksFormat /dev/shared_vg1/shared_lv1 --type luks2 --key-file=/etc/crypt_keyfile</literal>
WARNING!
========
This will overwrite data on /dev/shared_vg1/shared_lv1 irrevocably.

Are you sure? (Type 'yes' in capital letters): YES</literallayout>
</listitem>
<listitem>
<simpara>Create the crypt resource as part of the <literal>shared_vg1</literal> volume group.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev="luks_lv1" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev="/dev/shared_vg1/shared_lv1"</literal></literallayout>
</listitem>
<listitem>
<simpara>Before proceeding, check to be sure that the crypt resource has created the crypt device,
which in this example is <literal>/dev/mapper/luks_lv1</literal>.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>ls -l /dev/mapper/</literal>
...
lrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -&gt; ../dm-3
...</literallayout>
</listitem>
</orderedlist>
<simpara>Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On one node in the cluster, format the volume
with a GFS2 file system. One journal is required for each node that mounts the file system.
Ensure that you create enough journals for each of the nodes in your cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1</literal>
/dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3
This will destroy any data on /dev/dm-3
Are you sure you want to proceed? [y/n] y
Discarding device contents (may take a while on large devices): Done
Adding journals: Done
Building resource groups: Done
Creating quota file: Done
Writing superblock and syncing: Done
Device:                    /dev/mapper/luks_lv1
Block size:                4096
Device size:               4.98 GB (1306624 blocks)
Filesystem size:           4.98 GB (1306622 blocks)
Journals:                  3
Journal size:              16MB
Resource groups:           23
Locking protocol:          "lock_dlm"
Lock table:                "my_cluster:gfs2-demo1"
UUID:                      de263f7b-0f12-4d02-bbb2-56642fade293</literallayout>
</listitem>
<listitem>
<simpara>Create a file system resource to automatically mount the GFS2 file system on all nodes.</simpara>
<simpara>You should not add the file system to the <literal>/etc/fstab</literal> file because it will be managed as a Pacemaker
cluster resource. Mount options can be specified as part of the resource configuration with
<literal>options=<emphasis>options</emphasis></literal>. Run the <literal>pcs resource describe Filesystem</literal> command for full configuration options.</simpara>
<simpara>The following command creates the file system resource. This command adds the resource to the resource
group that includes the logical volume resource for that file system.</simpara>
<literallayout class="monospaced">root@z1 ~]# <literal>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/mapper/luks_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</literal></literallayout>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification steps</title>
<listitem>
<simpara>Verify that the GFS2 file system is mounted on both nodes of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>mount | grep gfs2</literal>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <literal>mount | grep gfs2</literal>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)</literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs status --full</literal>
Cluster name: my_cluster
[...]

Full list of resources:

  smoke-apc      (stonith:fence_apc):    Started z1.example.com
  Clone Set: locking-clone [locking]
      Resource Group: locking:0
          dlm    (ocf::pacemaker:controld):      Started z2.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
      Resource Group: locking:1
          dlm    (ocf::pacemaker:controld):      Started z1.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
  Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
             crypt       (ocf::heartbeat:crypt) Started z2.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
    Resource Group: shared_vg1:1
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
             crypt      (ocf::heartbeat:crypt)  Started z1.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
          Started:  [z1.example.com z2.example.com ]
...</literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_migrate-gfs2-rhel7-rhel8-configuring-gfs2-cluster">
<title>Migrating a GFS2 file system from RHEL7 to RHEL8</title>
<simpara>In Red Hat Enterprise Linux 8, LVM uses the LVM lock daemon <literal>lvmlockd</literal>
instead of <literal>clvmd</literal> for managing shared storage devices in an active/active
cluster.
This requires that you configure the logical volumes that your active/active cluster
will require as shared logical volumes.
Additionally, this requires that you use the <literal>LVM-activate</literal>
resource to manage an LVM volume and that you use the <literal>lvmlockd</literal> resource
agent to manage the <literal>lvmlockd</literal> daemon.
See <link linkend="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster">Configuring a GFS2 file system in
a cluster</link> for a full procedure for configuring a Pacemaker cluster that
includes GFS2 file systems using shared logical volumes.</simpara>
<simpara>To use your existing Red Hat Enterprise Linux 7 logical volumes when configuring a RHEL8
cluster that includes GFS2 file systems, perform the following procedure from the RHEL8 cluster.
In this example, the clustered RHEL 7 logical volume is part of the volume group <literal>upgrade_gfs_vg</literal>.</simpara>
<note>
<simpara>The RHEL8 cluster must have the same name as the RHEL7 cluster that includes the GFS2 file system in
order for the existing file system to be valid.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Ensure that the logical volumes containing the GFS2 file systems are currently
inactive. This procedure is safe only if all nodes have stopped using the volume group.</simpara>
</listitem>
<listitem>
<simpara>From one node in the cluster, forcibly change the volume group to be local.</simpara>
<literallayout class="monospaced">[root@rhel8-01 ~]# <literal>vgchange --lock-type none --lock-opt force upgrade_gfs_vg</literal>
Forcibly change VG lock type to none? [y/n]: y
  Volume group "upgrade_gfs_vg" successfully changed</literallayout>
</listitem>
<listitem>
<simpara>From one node in the cluster, change the local volume group to a shared volume group</simpara>
<literallayout class="monospaced">[root@rhel8-01 ~]# <literal>vgchange --lock-type dlm upgrade_gfs_vg</literal>
   Volume group "upgrade_gfs_vg" successfully changed</literallayout>
</listitem>
<listitem>
<simpara>On each node in the cluster, start locking for the volume group.</simpara>
<literallayout class="monospaced">[root@rhel8-01 ~]# <literal>vgchange --lock-start upgrade_gfs_vg</literal>
  VG upgrade_gfs_vg starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@rhel8-02 ~]# <literal>vgchange --lock-start upgrade_gfs_vg</literal>
  VG upgrade_gfs_vg starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</literallayout>
</listitem>
</orderedlist>
<simpara>After performing this procedure, you can create an <literal>LVM-activate</literal> resource for each logical volume.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters">
<title>Getting started with the pcsd Web UI</title>
<simpara>The <literal>pcsd</literal> Web UI is a
graphical user interface to create and configure Pacemaker/Corosync clusters.</simpara>
<section xml:id="proc_installing-cluster-software-getting-started-with-the-pcsd-web-ui">
<title>Installing cluster software</title>
<simpara>The following procedure installs the cluster software and configures your system for cluster creation.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On each node in the cluster,
install the
Red Hat High Availability Add-On software packages along with all available
fence agents from the High Availability channel.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-all</literal></literallayout>
<simpara>Alternatively, you can install the
Red Hat High Availability Add-On software packages along with only the
fence agent that you require with the following command.</simpara>
<literallayout class="monospaced"># <literal>yum install pcs pacemaker fence-agents-<emphasis>model</emphasis></literal></literallayout>
<simpara>The following command displays a list of the available fence agents.</simpara>
<literallayout class="monospaced"># <literal>rpm -q -a | grep fence</literal>
fence-agents-rhevm-4.0.2-3.el7.x86_64
fence-agents-ilo-mp-4.0.2-3.el7.x86_64
fence-agents-ipmilan-4.0.2-3.el7.x86_64
...</literallayout>
<warning>
<simpara>After you install the Red Hat High Availability Add-On packages,
you should ensure that your software update preferences are set
so that nothing is installed automatically. Installation on a running
cluster can cause unexpected behaviors.
For more information, see
<link xlink:href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</link>.</simpara>
</warning>
</listitem>
<listitem>
<simpara>If you are running the <literal role="command">firewalld</literal> daemon,
execute the following commands to enable the ports that are
required by
the Red Hat High Availability Add-On.</simpara>
<note>
<simpara>You can determine whether the <literal role="command">firewalld</literal> daemon is
installed on your system with the <literal role="command">rpm -q firewalld</literal> command. If
it is installed, you can determine
whether it is running with the <literal role="command">firewall-cmd --state</literal> command.</simpara>
</note>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd  --add-service=high-availability</literal></literallayout>
<note>
<simpara>The ideal firewall configuration for cluster components depends on the local environment,
where you may need to take into account such considerations as whether the nodes have
multiple network interfaces or whether off-host firewalling is present. The example
here, which opens the ports that are generally required by a Pacemaker cluster,
should be modified to suit local conditions.
<link linkend="proc_enabling-ports-for-high-availability-creating-high-availability-cluster">Enabling ports for the High Availability Add-On</link>
shows the ports to enable for
the Red Hat High Availability Add-On and provides an explanation
for what each port is used for.</simpara>
</note>
</listitem>
<listitem>
<simpara>In order to use <literal>pcs</literal> to configure
the cluster and communicate among the nodes,
you must set a password on each node for
the user ID <literal>hacluster</literal>,
which is the <literal>pcs</literal> administration
account.
It is recommended that the
password for user <literal>hacluster</literal> be
the same on each node.</simpara>
<literallayout class="monospaced"># <literal>passwd hacluster</literal>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</literallayout>
</listitem>
<listitem>
<simpara>Before the cluster can be configured, the <literal role="command">pcsd</literal> daemon
must be started and enabled to start up on boot on each node.
This daemon works with the <literal role="command">pcs</literal> command
to manage configuration across the nodes in the cluster.</simpara>
<simpara>On each node in the cluster, execute the following
commands to start the <literal>pcsd</literal>
service and to enable <literal>pcsd</literal>
at system start.</simpara>
<literallayout class="monospaced"># <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">
<title>Setting up the pcsd Web UI</title>
<simpara>After you have installed the Pacemaker configuration tools and configured
your system for cluster configuration, use the following
procedure to
set up your system to use the <literal role="command">pcsd</literal>
Web UI to configure a cluster.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On any system, open a browser to the following URL, specifying one
of the nodes of the cluster (note that this uses
the <literal>https</literal> protocol). This brings
up the <literal role="command">pcsd</literal> Web UI login screen.</simpara>
<literallayout class="monospaced"><link xlink:href="https://">https://</link><emphasis>nodename</emphasis>:2224</literallayout>
</listitem>
<listitem>
<simpara>Log in as user <literal>hacluster</literal>. This brings up the <literal>Manage
Clusters</literal> page as shown in
<xref linkend="getting-started-with-the-pcsd-web-ui-fig-manage-cluster"/>.</simpara>
<figure xml:id="getting-started-with-the-pcsd-web-ui-fig-manage-cluster">
<title>Manage Clusters page</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/manageclusters.png"/>
</imageobject>
<textobject><phrase>The Manage Clusters page</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
</section>
<section xml:id="assembly_creating-a-cluster-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">
<title>Creating a cluster with the pcsd Web UI</title>
<simpara>From the Manage Clusters page, you can create
a new cluster, add an existing cluster to the Web UI, or remove
a cluster from the Web UI.</simpara>
<itemizedlist>
<listitem>
<simpara>To create a cluster, click on <literal>Create New</literal>. Enter the name
of the cluster to create and the nodes that constitute the cluster.
If you have not previously authenticated the user <literal>hacluster</literal> for each node
in the cluster, you will be asked to authenticate the cluster nodes.</simpara>
</listitem>
<listitem>
<simpara>When creating the cluster, you can configure advanced cluster options
by clicking <literal>Go to advanced settings</literal> on this screen.
The advanced cluster configurations you can configure are
described in
<link linkend="proc_configuring-advanced-cluster-options-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui">Configuring advanced cluster configuration options with the pcsd Web UI</link>.</simpara>
</listitem>
<listitem>
<simpara>To add an existing cluster to the Web UI, click on <literal>Add Existing</literal>
and enter the host name or IP address of a node in the cluster that you would
like to manage with the Web UI.</simpara>
</listitem>
</itemizedlist>
<simpara>Once you have
created or added a cluster, the cluster name is displayed on the Manage Cluster page.
Selecting the cluster displays information about the cluster.</simpara>
<note>
<simpara>When using the <literal role="command">pcsd</literal> Web UI to configure a cluster,
you can move your mouse over the text describing many of the options
to see longer descriptions of those options as a <literal>tooltip</literal> display.</simpara>
</note>
<section xml:id="proc_configuring-advanced-cluster-options-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui">
<title>Configuring advanced cluster configuration options with the pcsd Web UI</title>
<simpara>When creating a cluster, you can configure additional cluster options
by clicking <guibutton>Go to advanced settings</guibutton> on the Create cluster screen.
This allows you to modify the configurable settings of the following
cluster components:</simpara>
<itemizedlist>
<listitem>
<simpara>Transport settings: Values for the transport mechanism used for cluster communication</simpara>
</listitem>
<listitem>
<simpara>Quorum settings: Values for the quorum options of the <literal>votequorum</literal> service</simpara>
</listitem>
<listitem>
<simpara>Totem settings: Values for the Totem protocol used by Corosync</simpara>
</listitem>
</itemizedlist>
<simpara>Selecting those options displays the settings you
can configure.
For information on each of the settings, place the mouse pointer over the particular option.</simpara>
</section>
<section xml:id="proc_setting-permissions-with-the-pcsd-web-ui-creating-a-cluster-with-the-pcsd-web-ui">
<title>Setting cluster management permissions</title>
<simpara>There are two sets of cluster permissions that you can grant to users:</simpara>
<itemizedlist>
<listitem>
<simpara>Permissions for managing the cluster with the Web UI, which also grants permissions
to run <literal role="command">pcs</literal> commands that connect to
nodes over a network. This section describes how to configure those permissions
with the Web UI.</simpara>
</listitem>
<listitem>
<simpara>Permissions for
local users to allow read-only or read-write access to the cluster
configuration, using ACLs. Configuring ACLs with the Web UI
is described in
<link linkend="proc_configuring-cluster-components-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">Configuring cluster components with the pcsd Web UI</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>You can grant permission for specific users other than
user <literal>hacluster</literal> to manage the cluster
through the Web UI and to run <literal role="command">pcs</literal> commands that connect to nodes over a network
by adding them to the group
<literal>haclient</literal>.
You can then configure the permissions set for an individual member
of the group <literal>haclient</literal> by clicking
the Permissions tab on the Manage
Clusters page and setting the permissions on the
resulting screen. From this screen, you can also set permissions
for groups.</simpara>
<simpara>You can grant the following permissions:</simpara>
<itemizedlist>
<listitem>
<simpara>Read permissions, to view the cluster settings</simpara>
</listitem>
<listitem>
<simpara>Write permissions, to modify the cluster settings (except for permissions
and ACLs)</simpara>
</listitem>
<listitem>
<simpara>Grant permissions, to modify the cluster permissions and ACLs</simpara>
</listitem>
<listitem>
<simpara>Full permissions, for unrestricted access to a cluster, including adding and
removing nodes, with access to keys and certificates</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="proc_configuring-cluster-components-with-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">
<title>Configuring cluster components with the pcsd Web UI</title>
<simpara>To configure the components and attributes of a cluster,
click on the name of the cluster displayed on the
<literal>Manage Clusters</literal> screen.
This brings up the <literal>Nodes</literal> page,
as described in
<xref linkend="s2-guiclustnodes-HAAR"/>.
This page displays a menu along the top of the
page
with the following entries:</simpara>
<itemizedlist>
<listitem>
<simpara>Nodes, as described in
<xref linkend="s2-guiclustnodes-HAAR"/></simpara>
</listitem>
<listitem>
<simpara>Resources, as described in
<xref linkend="s2-guiclustresources-HAAR"/></simpara>
</listitem>
<listitem>
<simpara>Fence Devices, as described in
<xref linkend="s2-guifencedevices-HAAR"/></simpara>
</listitem>
<listitem>
<simpara>ACLs, as described in
<xref linkend="s2-guiaclset-HAAR"/></simpara>
</listitem>
<listitem>
<simpara>Cluster Properties, as described in
<xref linkend="s2-guiclustprops-HAAR"/></simpara>
</listitem>
</itemizedlist>
<section xml:id="s2-guiclustnodes-HAAR">
<title>Configuring cluster nodes with the pcsd Web UI</title>
<simpara>Selecting the <literal>Nodes</literal> option from the menu
along the top of the cluster management page displays the currently configured
nodes and the status of the currently selected node, including
which resources are running on the node and the resource location preferences.
This is the default page that is displayed when you select a cluster
from the
<literal>Manage Clusters</literal> screen.</simpara>
<simpara>Form this page, You can add or remove nodes.
You can also start, stop, restart, or put a node in standby or
maintenance mode.
For information on standby mode, see
<link linkend="proc_stopping-individual-node-cluster-maintenance">Putting a node into standby mode</link>.
For information on maintenance mode, see
<link linkend="proc_setting-maintenance-mode-cluster-maintenance">Putting a cluster in maintenance mode</link>.</simpara>
<simpara>You can also configure fence devices directly from this page,
as described in
by selecting
<literal>Configure Fencing</literal>.
Configuring fence devices is described in
<xref linkend="s2-guifencedevices-HAAR"/>.</simpara>
</section>
<section xml:id="s2-guiclustresources-HAAR">
<title>Configuring cluster resources with the pcsd Web UI</title>
<simpara>Selecting the <literal>Resources</literal> option from the menu
along the top of the cluster management page displays the currently configured
resources for the cluster, organized according to resource groups.
Selecting a group or a resource displays the attributes of that group or
resource.</simpara>
<simpara>From this screen, you can add or remove resources, you can edit
the configuration of existing resources, and you can create a resource group.</simpara>
<simpara>To add a new resource to the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Click <literal>Add</literal>. This brings up the
<literal>Add Resource</literal> screen.</simpara>
</listitem>
<listitem>
<simpara>When you select a resource type from
the dropdown <literal>Type</literal> menu, the arguments you must specify for
that resource appear in the menu.</simpara>
</listitem>
<listitem>
<simpara>You can click <literal>Optional
Arguments</literal> to display additional arguments you can specify for
the resource you are defining.</simpara>
</listitem>
<listitem>
<simpara>After entering the parameters for the
resource you are creating,
click <literal>Create Resource</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>When configuring the arguments for a resource, a brief description
of the argument appears in the menu. If you move the cursor to the
field, a longer help description of that argument is displayed.</simpara>
<simpara>You can define a resource as a cloned resource, or as a promotable clone resource.
For information on these resource types, see
<link linkend="assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters">Creating cluster resources that are active on multiple nodes (cloned resources)</link>.</simpara>
<simpara>Once you have created at least one resource,
you can create
a resource group.
For general information on resource groups, see
<link linkend="assembly_resource-groups-configuring-cluster-resources">Configuring resource groups</link>.</simpara>
<simpara>To create a resource group:</simpara>
<itemizedlist>
<listitem>
<simpara>Select the resources that will be part of the group from
the <literal>Resources</literal> screen,
then click <literal>Create Group</literal>. This displays
the <literal>Create Group</literal> screen.</simpara>
</listitem>
<listitem>
<simpara>From the <literal>Create Group</literal> screen, you can rearrange the order of the resources in a resource
group by using drag-and-drop to move the list of
the resources around.</simpara>
</listitem>
<listitem>
<simpara>Enter a group name and click <literal>Create Group</literal>. This returns you to the <literal>Resources</literal> screen,
which now displays the group name and the resources within that group.</simpara>
</listitem>
</itemizedlist>
<simpara>After you have created a resource group, you can indicate that
group’s name as a resource parameter when you create or modify
additional resources.</simpara>
</section>
<section xml:id="s2-guifencedevices-HAAR">
<title>Configuring fence devices with the pcsd Web UI</title>
<simpara>Selecting the <literal>Fence Devices</literal> option from the menu
along the top of the cluster management page displays
<literal>Fence Devices</literal> screen, showing
the currently configured fence devices.</simpara>
<simpara>To add a new fence device to the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Click <literal>Add</literal>. This brings up the
<literal>Add Fence Device</literal> screen.</simpara>
</listitem>
<listitem>
<simpara>When you select a fence device type from
the drop-down <literal>Type</literal> menu, the arguments you must specify for
that fence device appear in the menu.</simpara>
</listitem>
<listitem>
<simpara>You can click on <literal>Optional
Arguments</literal> to display additional arguments you can specify for
the fence device you are defining.</simpara>
</listitem>
<listitem>
<simpara>After entering the parameters for the new fence device,
click <literal>Create Fence Instance</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>To configure an SBD fencing device, click on
<literal>SBD</literal> on the <literal>Fence Devices</literal> screen.
This calls up a screen that allows you to enable or disable
SBD in the cluster.</simpara>
<simpara>For more information on fence devices, see
<link linkend="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters">Configuring fencing in a Red Hat High Availability cluster</link>.</simpara>
</section>
<section xml:id="s2-guiaclset-HAAR">
<title>Configuring ACLs with the pcsd Web UI</title>
<simpara>Selecting the <literal>ACLS</literal> option from the menu
along the top of the cluster management page displays
a screen from which you can set permissions for local users,
allowing read-only or read-write access to the cluster configuration
by using access control lists (ACLs).</simpara>
<simpara>To assign ACL permissions, you create a role and define
the access permissions for that role.
Each role can have an unlimited number of permissions
(read/write/deny) applied to either an XPath query or the ID
of a specific element. After defining the role, you
can assign it to an existing user or group.</simpara>
<simpara>For more information on assigning permission using ACLs, see
<link linkend="proc_setting-local-cluster-permissions-cluster-permissions">Setting local permissions using ACLs</link>.</simpara>
</section>
<section xml:id="s2-guiclustprops-HAAR">
<title>Configuring cluster properties with the pcsd Web UI</title>
<simpara>Selecting the <literal>Cluster Properties</literal> option from the menu
along the top of the cluster management page displays the
cluster properties and allows you to modify these properties
from their default values.
For information on the Pacemaker cluster properties, see
<link linkend="ref_cluster-properties-options-controlling-cluster-behavior">Pacemaker cluster properties</link>.</simpara>
</section>
</section>
<section xml:id="proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui">
<title>Configuring a high availability pcsd Web UI</title>
<simpara>When you use the <literal>pcsd</literal> Web UI, you connect to one of the nodes of the cluster
to display the cluster management pages. If the node to which
you are connecting goes down or becomes
unavailable, you can reconnect to the cluster by opening your browser to a URL that specifies a different
node of the cluster.</simpara>
<simpara>It is possible, however, to configure the <literal>pcsd</literal> Web UI itself
for high availability, in
which case you can continue to manage the cluster without entering a new URL.</simpara>
<simpara>To configure the <literal>pcsd</literal> Web UI for high availability, perform the following steps.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Ensure that the <literal>pcsd</literal> certificates are synced across the nodes of the cluster
by setting <literal>PCSD_SSL_CERT_SYNC_ENABLED</literal> to <literal>true</literal> in the <literal>/etc/sysconfig/pcsd</literal> configuration
file. Enabling certificate syncing causes <literal>pcsd</literal> to sync the certificates
for the cluster setup and node add commands. In RHEL 8,
<literal>PCSD_SSL_CERT_SYNC_ENABLED</literal> is set to <literal>false</literal> by default.</simpara>
</listitem>
<listitem>
<simpara>Create an <literal>IPaddr2</literal> cluster resource, which is a floating IP address that you
will use to connect to the <literal>pcsd</literal> Web UI.
The IP address must not be one already associated with a physical node. If the
<literal>IPaddr2</literal> resource’s NIC device is not specified, the floating IP must reside
on the same network as one of the node’s statically assigned IP addresses,
otherwise the NIC device to assign the floating IP address cannot be properly detected.</simpara>
</listitem>
<listitem>
<simpara>Create custom SSL certificates for use with <literal>pcsd</literal> and ensure that they are valid for
the addresses of the nodes used to
connect to the <literal>pcsd</literal> Web UI.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To create custom SSL certificates, you can use either wildcard certificates or
you can use the Subject Alternative Name certificate extension.
For information on the Red Hat Certificate System, see the
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_certificate_system/9/html/administration_guide/index">Red Hat Certificate System Administration Guide</link>.</simpara>
</listitem>
<listitem>
<simpara>Install the custom certificates for <literal>pcsd</literal> with the <literal>pcs pcsd certkey</literal> command.</simpara>
</listitem>
<listitem>
<simpara>Sync the <literal>pcsd</literal> certificates to all nodes in the cluster
with the <literal>pcs pcsd sync-certificates</literal> command.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Connect to the <literal>pcsd</literal> Web UI using the floating IP address you configured as a cluster resource.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>Even when you configure the <literal>pcsd</literal> Web UI for high availability,
you will be asked to log in again when the node to which you
are connecting goes down.</simpara>
</note>
</section>
</chapter>
<chapter xml:id="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters">
<title>Configuring fencing in a Red Hat High Availability cluster</title>
<simpara>A node that is unresponsive may still be accessing data.
The only way to be certain that your data is
safe is to fence the node using STONITH.
STONITH is an acronym for "Shoot The Other Node In The Head" and it
protects your data from being corrupted by rogue nodes or concurrent
access. Using STONITH,
you can be certain that a node is truly
offline before allowing the data to be accessed from another node.</simpara>
<simpara>STONITH also has a role to play in the event that a clustered service
cannot be stopped. In this case, the cluster uses STONITH to force the
whole node offline, thereby making it safe to start the service
elsewhere.</simpara>
<simpara>For more complete general information on fencing and its importance in
a Red Hat High Availability cluster, see
<link xlink:href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</link>.</simpara>
<simpara>You implement STONITH in a Pacemaker cluster by configuring fence devices
for the nodes of the cluster.</simpara>
<section xml:id="proc_displaying-fence-agents-configuring-fencing">
<title>Displaying available fence agents and their options</title>
<simpara>Use the following command to view of list of all
available STONITH agents. When you specify a filter,
this command displays only the STONITH agents
that match the filter.</simpara>
<literallayout class="monospaced">pcs stonith list [<emphasis>filter</emphasis>]</literallayout>
<simpara>Use the following command to view the options for the specified STONITH agent.</simpara>
<literallayout class="monospaced">pcs stonith describe <emphasis>stonith_agent</emphasis></literallayout>
<simpara>For example, the following command displays the options
for the fence agent for APC over telnet/SSH.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith describe fence_apc</literal>
Stonith options for: fence_apc
  ipaddr (required): IP Address or Hostname
  login (required): Login Name
  passwd: Login password or passphrase
  passwd_script: Script to retrieve password
  cmd_prompt: Force command prompt
  secure: SSH connection
  port (required): Physical plug number or name of virtual machine
  identity_file: Identity file for ssh
  switch: Physical switch number on device
  inet4_only: Forces agent to use IPv4 addresses only
  inet6_only: Forces agent to use IPv6 addresses only
  ipport: TCP port to use for connection with device
  action (required): Fencing Action
  verbose: Verbose mode
  debug: Write debug information to given file
  version: Display version information and exit
  help: Display help and exit
  separator: Separator for CSV created by operation list
  power_timeout: Test X seconds for status change after ON/OFF
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  login_timeout: Wait X seconds for cmd prompt after login
  power_wait: Wait X seconds after issuing ON/OFF
  delay: Wait X seconds before fencing is started
  retry_on: Count of attempts to retry power on</literallayout>
<warning>
<simpara>For fence agents that provide a <literal>method</literal> option, a value of
<literal>cycle</literal> is unsupported and should not be specified, as it may cause data
corruption.</simpara>
</warning>
</section>
<section xml:id="proc_creating-fence-devices-configuring-fencing">
<title>Creating a fence device</title>
<simpara>The format for the command to create a stonith device is as follows. For a listing of the
available stonith creation options, see the <literal role="command">pcs stonith -h</literal> display.</simpara>
<literallayout class="monospaced">pcs stonith create <emphasis>stonith_id</emphasis> <emphasis>stonith_device_type</emphasis> [<emphasis>stonith_device_options</emphasis>] [op  <emphasis>operation_action</emphasis> <emphasis>operation_options</emphasis>]</literallayout>
<simpara>The following command creates a single fencing device for a single node.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith create MyStonith fence_virt pcmk_host_list=f1 op monitor interval=30s</literal></literallayout>
<simpara>Some fence devices can fence only a single node, while other devices can
fence multiple nodes.
The parameters you specify when you create a fencing device depend on what your
fencing device supports and requires.</simpara>
<itemizedlist>
<listitem>
<simpara>Some fence devices can automatically determine what nodes they can fence.</simpara>
</listitem>
<listitem>
<simpara>You can use the <literal>pcmk_host_list</literal> parameter when creating
a fencing device to specify all of the machines that are controlled by that fencing device.</simpara>
</listitem>
<listitem>
<simpara>Some fence devices require a mapping of host names to the specifications that
the fence device understands. You can map host names
with the <literal>pcmk_host_map</literal> parameter when creating a fencing device.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on the <literal>pcmk_host_list</literal> and <literal>pcmk_host_map</literal> parameters, see
<link linkend="tb-fencedevice-props-HAAR">General Properties of Fencing Devices</link>.</simpara>
<simpara>After configuring a fence device, it is imperative that you test the device
to ensure that it is working correctly.
For information on testing a fence device, see
<link linkend="proc_testing-fence-devices-configuring-fencing">Testing a fence device</link>.</simpara>
</section>
<section xml:id="ref_general-fence-device-properties-configuring-fencing">
<title>General properties of fencing devices</title>
<simpara>Any cluster node can fence any other cluster node with any fence device,
regardless of whether the fence resource is started or stopped.
Whether the resource is started controls only the recurring monitor
for the device, not whether it can be used, with the following exceptions:</simpara>
<itemizedlist>
<listitem>
<simpara>You can disable a fencing device by running the
<literal role="command">pcs stonith disable <emphasis>stonith_id</emphasis></literal> command.
This will prevent any node from using that device.</simpara>
</listitem>
<listitem>
<simpara>To prevent a specific node from using a fencing device, you
can configure location constraints for the fencing resource with
the <literal role="command">pcs constraint location … avoids</literal> command.</simpara>
</listitem>
<listitem>
<simpara>Configuring <literal>stonith-enabled=false</literal> will disable fencing altogether.
Note, however, that Red Hat does not support clusters when fencing is disabled,
as it is not suitable for a production environment.</simpara>
</listitem>
</itemizedlist>
<simpara><xref linkend="tb-fencedevice-props-HAAR"/>
describes the general properties you can set for fencing devices.</simpara>
<table xml:id="tb-fencedevice-props-HAAR" frame="all" rowsep="1" colsep="1">
<title>General Properties of Fencing Devices</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="27*"/>
<colspec colname="col_2" colwidth="18*"/>
<colspec colname="col_3" colwidth="27*"/>
<colspec colname="col_4" colwidth="27*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_host_map</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>A mapping of host names to port numbers for devices that do
not support host names. For example: <literal>node1:1;node2:2,3</literal>
tells the cluster to use port 1 for node1 and ports 2 and 3 for node2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_host_list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>A list of machines controlled by this device
(Optional unless <literal>pcmk_host_check=static-list</literal>).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_host_check</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>* <literal>static-list</literal> if either <literal>pcmk_host_list</literal> or
<literal>pcmk_host_map</literal> is set</simpara><simpara>* Otherwise, <literal>dynamic-list</literal> if the fence device
supports the <literal>list</literal> action</simpara><simpara>* Otherwise, <literal>status</literal> if the fence device
supports the <literal>status</literal> action</simpara><simpara>*Otherwise, <literal>none</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara>How to determine which machines are controlled by the device.
Allowed values: <literal>dynamic-list</literal> (query the device),
<literal>static-list</literal> (check the <literal>pcmk_host_list</literal>
attribute), none (assume every device can fence every machine)</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="ref_advanced-fence-device-properties-configuring-fencing">
<title>Advanced fencing configuration options</title>
<simpara><xref linkend="tb-fencepropsadvanced-HAAR"/>
summarizes additional properties you
can set for fencing devices. Note that these
properties are for advanced use only.</simpara>
<table xml:id="tb-fencepropsadvanced-HAAR" frame="all" rowsep="1" colsep="1">
<title>Advanced Properties of Fencing Devices</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="14*"/>
<colspec colname="col_3" colwidth="14*"/>
<colspec colname="col_4" colwidth="43*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_host_argument</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>port</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate parameter to supply instead of port.
Some devices do not support the standard port parameter
or may provide additional ones. Use this to specify an alternate,
device-specific parameter that should indicate the machine to be fenced.
A value of <literal>none</literal> can be used to tell the cluster
not to supply any additional parameters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_reboot_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>reboot</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate command to run instead of <literal>reboot</literal>.
Some devices do not support the standard commands
or may provide additional ones. Use this to specify an alternate,
device-specific, command that implements the reboot action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_reboot_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Specify an alternate timeout to use for reboot actions instead of
<literal>stonith-timeout</literal>.
Some devices need much more/less time to complete than normal.
Use this to specify an alternate, device-specific, timeout for reboot actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_reboot_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of times to retry the <literal>reboot</literal>
command within the timeout period.
Some devices do not support multiple connections. Operations may
fail if the device is busy with another task so Pacemaker
will automatically retry the operation, if there is time
remaining. Use this option to alter the number of
times Pacemaker retries reboot actions before giving up.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_off_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>off</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate command to run instead of <literal>off</literal>.
Some devices do not support the standard commands or may
provide additional ones. Use this to specify an alternate,
device-specific, command that implements the off action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_off_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Specify an alternate timeout to use for off actions instead
of <literal>stonith-timeout</literal>. Some devices
need much more or much less time to complete than normal.
Use this to specify an alternate, device-specific, timeout for off actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_off_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of times to retry the off command within the timeout period.
Some devices do not support multiple connections. Operations
may fail if the device is busy with another task so Pacemaker
will automatically retry the operation, if there is time remaining.
Use this option to alter the number of times Pacemaker retries
off actions before giving up.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_list_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>list</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate command to run instead of <literal>list</literal>.
Some devices do not support the standard commands or
may provide additional ones. Use this to specify an
alternate, device-specific, command that implements the list action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_list_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Specify an alternate timeout to use for list actions.
Some devices need much more or much less time to complete
than normal. Use this to specify an alternate, device-specific,
timeout for list actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_list_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of times to retry the <literal>list</literal> command
within the timeout period.
Some devices do not support multiple connections.
Operations may fail if the device is busy with another task
so Pacemaker will automatically retry the operation, if there is time
remaining. Use this option to alter the number of
times Pacemaker retries list actions before giving up.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_monitor_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>monitor</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate command to run instead of <literal>monitor</literal>.
Some devices do not support the standard commands or
may provide additional ones. Use this to specify an
alternate, device-specific, command that implements the monitor action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_monitor_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Specify an alternate timeout to use for monitor actions
instead of <literal>stonith-timeout</literal>.
Some devices need much more or much less time to complete
than normal. Use this to specify an alternate,
device-specific, timeout for monitor actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_monitor_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of times to retry the <literal>monitor</literal> command
within the timeout period.
Some devices do not support multiple connections. Operations may fail
if the device is busy with another task so Pacemaker will
automatically retry the operation, if there is time remaining.
Use this option to alter the number of times
Pacemaker retries monitor actions before giving up.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_status_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>status</simpara></entry>
<entry align="left" valign="top"><simpara>An alternate command to run instead of <literal>status</literal>.
Some devices do not support the standard commands or may
provide additional ones. Use this to specify an alternate,
device-specific, command that implements the status action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_status_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Specify an alternate timeout to use for status
actions instead of <literal>stonith-timeout</literal>.
Some devices need much more or much less time to complete
than normal. Use this to specify an alternate, device-specific,
timeout for status actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_status_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of times to retry the status command within the
timeout period.
Some devices do not support multiple connections.
Operations may fail if the device is busy with another
task so Pacemaker will automatically retry the operation,
if there is time remaining. Use this option to alter
the number of times Pacemaker retries status actions before giving up.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_delay_base</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>0s</simpara></entry>
<entry align="left" valign="top"><simpara>Enable a base delay for stonith actions and specify a base delay value.
In a cluster with an even number of nodes, configuring a delay can
help avoid nodes fencing each other at the same time in an even split.
A random delay can be useful when the same fence device is used for all
nodes, and differing static delays can be useful on each fencing device
when a separate device is used for each node.
The overall delay is derived from a random delay value adding this
static delay so that the sum is kept below the maximum delay.
If you set <literal>pcmk_delay_base</literal> but do not set <literal>pcmk_delay_max</literal>, there is no
random component to the delay and it will be the value of <literal>pcmk_delay_base</literal>.</simpara><simpara>Some individual fence agents implement a "delay" parameter, which is independent of
delays configured with a <literal>pcmk_delay_*</literal> property. If both of these delays are
configured, they are added together and thus would generally not be used in conjunction.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_delay_max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>0s</simpara></entry>
<entry align="left" valign="top"><simpara>Enable a random delay for stonith actions and specify the maximum of random delay.
In a cluster with an even number of nodes, configuring a delay can
help avoid nodes fencing each other at the same time in an even split.
A random delay can be useful when the same fence device is used for all
nodes, and differing static delays can be useful on each fencing device
when a separate device is used for each node.
The overall delay is derived from this random delay value adding a static delay so that
the sum is kept below the maximum delay.
If you set <literal>pcmk_delay_max</literal> but do not set <literal>pcmk_delay_base</literal> there is no static
component to the delay.</simpara><simpara>Some individual fence agents implement a "delay" parameter, which is independent of
delays configured with a <literal>pcmk_delay_*</literal> property. If both of these delays are
configured, they are added together and thus would generally not be used in conjunction.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_action_limit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>1</simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of actions that can be performed in parallel on this device.
The cluster property <literal>concurrent-fencing=true</literal> needs to be configured first (this is the default
value for RHEL 8.1 and later).
A value of -1 is unlimited.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_on_action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>on</simpara></entry>
<entry align="left" valign="top"><simpara>For advanced use only: An alternate command to run instead of <literal>on</literal>.
Some devices do not support the standard commands or may provide additional ones. Use this to specify an
alternate, device-specific, command that implements the <literal>on</literal> action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_on_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>time</simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>For advanced use only: Specify an alternate timeout to use for <literal>on</literal> actions instead
of <literal>stonith-timeout</literal>.
Some devices need much more or much less time to complete than normal. Use this to specify an alternate,
device-specific, timeout for <literal>on</literal> actions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pcmk_on_retries</literal></simpara></entry>
<entry align="left" valign="top"><simpara>integer</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>For advanced use only: The maximum number of times to retry the <literal>on</literal> command within the timeout period.
Some devices do not support multiple connections. Operations may <literal>fail</literal> if the device is busy with another
task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to
alter the number of times Pacemaker retries <literal>on</literal> actions before giving up.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>In addition to the properties you can set for individual fence devices, there are also
cluster properties you can set that determine fencing behavior, as described in
<xref linkend="tb-clusterfenceprops-HAAR"/>.</simpara>
<table xml:id="tb-clusterfenceprops-HAAR" frame="all" rowsep="1" colsep="1">
<title>Cluster properties that determine fencing behavior</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="29*"/>
<colspec colname="col_3" colwidth="43*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>stonith-enabled</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates that failed nodes and nodes with resources that cannot be
stopped should be fenced. Protecting your data requires that
you set this <literal>true</literal>.</simpara><simpara>If <literal>true</literal>, or unset, the cluster will refuse to start resources unless
one or more STONITH resources have been configured also.</simpara><simpara>Red Hat only supports clusters with this value set to <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stonith-action</literal></simpara></entry>
<entry align="left" valign="top"><simpara>reboot</simpara></entry>
<entry align="left" valign="top"><simpara>Action to send to STONITH device. Allowed values: <literal>reboot</literal>, <literal>off</literal>.
The value <literal>poweroff</literal> is also allowed, but is only used for
legacy devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stonith-timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>How long to wait for a STONITH action to complete.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stonith-max-attempts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>10</simpara></entry>
<entry align="left" valign="top"><simpara>How many times fencing can fail for a target before the cluster
will no longer immediately re-attempt it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stonith-watchdog-timeout</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>The maximum time to wait
until a node can be assumed to have been killed by
the hardware watchdog. It is recommended that this value be
set to twice the value of the hardware watchdog timeout. This option is needed
only if watchdog-based SBD is used for fencing.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>concurrent-fencing</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true (RHEL 8.1 and later)</simpara></entry>
<entry align="left" valign="top"><simpara>Allow fencing operations to be performed in parallel.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>fence-reaction</literal></simpara></entry>
<entry align="left" valign="top"><simpara>stop</simpara></entry>
<entry align="left" valign="top"><simpara>(Red Hat Enterprise Linux 8.2 and later) Determines how a
cluster node should react if notified of its own fencing. A cluster node may receive
notification of its own fencing if fencing is misconfigured, or if fabric fencing
is in use that does not cut cluster communication.
Allowed values are <literal>stop</literal> to
attempt to immediately stop Pacemaker and stay stopped, or <literal>panic</literal> to attempt
to immediately reboot the local node, falling back to stop on failure.</simpara><simpara>Although the default value for this property is <literal>stop</literal>,
the safest choice for this value is <literal>panic</literal>, which attempts to
immediately reboot the local node. If you prefer the stop behavior,
as is most likely to be the case in conjunction with fabric fencing,
it is recommended that you set this explicitly.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For information on setting cluster properties, see
<link linkend="setting-cluster-properties-controlling-cluster-behavior">Setting and removing cluster properties</link>.</simpara>
</section>
<section xml:id="proc_testing-fence-devices-configuring-fencing">
<title>Testing a fence device</title>
<simpara>Fencing is a fundamental part of the Red Hat Cluster infrastructure and
it is therefore important to validate or test that fencing is working properly.</simpara>
<simpara>Use the following procedure to test a fence device.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use ssh, telnet, HTTP, or whatever remote protocol is used to connect
to the device to manually log in and test the fence device or see what output is given.
For example, if you will be configuring fencing for
an IPMI-enabled device,
then try to log in remotely with <literal role="command">ipmitool</literal>.
Take note of the options used when logging in manually
because those options might be needed when using the fencing agent.</simpara>
<simpara>If you are unable to log in to the fence device, verify that the device is pingable,
there is nothing such as a firewall configuration that
is preventing access
to the fence device,
remote access is enabled on the fencing device, and the credentials are correct.</simpara>
</listitem>
<listitem>
<simpara>Run the fence agent manually, using the fence agent script.
This does not require that the cluster services are running,
so you can perform this step
before the device is configured in the cluster.
This can ensure that the fence device is responding properly
before proceeding.</simpara>
<note>
<simpara>The examples in this section use the <literal role="command">fence_ipmilan</literal>
fence agent script for an iLO device.
The actual fence agent you will use and the command that calls
that agent will depend on your server hardware. You should
consult the man page for the fence agent you are using to
determine which options to specify.
You will usually need to know the
login and password for the fence device and other information related to the fence
device.</simpara>
</note>
<simpara>The following example shows the format you would use
to run the <literal role="command">fence_ipmilan</literal> fence agent script
with <literal>-o status</literal> parameter to check the status
of the fence device interface on another node without actually fencing it.
This allows you to test the device and get it working before
attempting to reboot the node.
When running this command, you specify the name and password
of an iLO user that has power on and off permissions for
the iLO device.</simpara>
<literallayout class="monospaced"># <literal>fence_ipmilan -a ipaddress -l username -p password -o status</literal></literallayout>
<simpara>The following example shows the format you would use
to run the <literal role="command">fence_ipmilan</literal> fence agent script
with the <literal>-o reboot</literal> parameter.
Running this command on one node reboots
the node managed by this iLO device.</simpara>
<literallayout class="monospaced"># <literal>fence_ipmilan -a ipaddress -l username -p password -o reboot</literal></literallayout>
<simpara>If the fence agent failed to properly do a status, off, on, or
reboot action, you should check the hardware, the configuration of the fence device,
and the syntax of your commands. In addition, you can run the
fence agent script with the debug output enabled.
The debug output is useful
for some fencing agents to see where in the sequence of events the fencing agent script
is failing when logging into the fence device.</simpara>
<literallayout class="monospaced"># <literal>fence_ipmilan -a ipaddress -l username -p password -o status -D /tmp/$(hostname)-fence_agent.debug</literal></literallayout>
<simpara>When diagnosing a failure that has occurred, you should
ensure that the options you specified when manually logging in
to the fence device are identical to what you passed on to the fence
agent with the fence agent script.</simpara>
<simpara>For fence agents that support an encrypted connection, you may see
an error due to certificate validation failing, requiring that you trust
the host or that you use the fence agent’s <literal>ssl-insecure</literal> parameter.
Similarly, if SSL/TLS is disabled on the target device, you may need
to account for this when setting the SSL parameters for the fence agent.</simpara>
<note>
<simpara>If the fence agent that is being tested is a <literal role="command">fence_drac</literal>,
<literal role="command">fence_ilo</literal>, or some other fencing agent for a systems management device
that continues to fail, then
fall back to trying <literal role="command">fence_ipmilan</literal>. Most systems management cards support IPMI remote
login and the only supported fencing agent is <literal role="command">fence_ipmilan</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Once the fence device has been configured in
the cluster with the same options that worked manually and the cluster has
been started,
test fencing with the <literal role="command">pcs stonith fence</literal> command
from any node (or even multiple times from different
nodes), as in the following example.
The <literal role="command">pcs stonith fence</literal> command reads the cluster configuration from the CIB
and calls the fence agent as configured to execute the fence action.
This verifies that the cluster configuration is correct.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith fence node_name</literal></literallayout>
<simpara>If the <literal role="command">pcs stonith fence</literal> command works properly, that means the fencing configuration for the
cluster should work when a fence event occurs.
If the command fails, it
means that cluster management cannot invoke the fence
device through the configuration it has retrieved.
Check for the following issues and update your cluster configuration as needed.</simpara>
<itemizedlist>
<listitem>
<simpara>Check your fence configuration.
For example, if you have used a host map you should ensure that the
system can find the node using the host name you have provided.</simpara>
</listitem>
<listitem>
<simpara>Check whether the password and user name for the device include any special
characters that could be misinterpreted by the bash shell.
Making sure that you enter passwords and user names surrounded by quotation marks could address this issue.</simpara>
</listitem>
<listitem>
<simpara>Check whether you can connect to the device using the exact IP address or host
name you specified in the <literal role="command">pcs stonith</literal> command. For example, if you give
the host name in the stonith command but test by using the IP address, that is not a valid test.</simpara>
</listitem>
<listitem>
<simpara>If the protocol that your fence device uses is accessible to you,
use that protocol to try to connect to the device. For example many agents
use ssh or telnet. You should try to connect to the device with the credentials
you provided when configuring the device, to see if you get a valid prompt and can log in to the device.</simpara>
<simpara>If you determine that all your parameters are appropriate but you still have trouble
connecting to your fence device, you can check the logging on the
fence device itself, if the device provides that, which will show if
the user has connected and what command the user issued.
You can also search through the <literal>/var/log/messages</literal> file for
instances of stonith and error, which could give some idea of what is transpiring,
but some agents can provide additional information.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Once the fence device tests are working and the cluster is up and
running, test an actual failure.
To do this, take an action in the
cluster that should initiate a token loss.</simpara>
<itemizedlist>
<listitem>
<simpara>Take down a network.
How you take a network depends on your specific configuration.
In many cases, you can
physically pull the network or power cables out of the host.
For information on simulating a network failure, see
<link xlink:href="https://access.redhat.com/solutions/79523/">What is the proper way to simulate a network failure on a RHEL Cluster?</link>.</simpara>
<note>
<simpara>Disabling the network interface on the local host rather than physically disconnecting
the network or power cables is not recommended as a test of fencing
because it does not accurately simulate a typical real-world
failure.</simpara>
</note>
</listitem>
<listitem>
<simpara>Block corosync traffic both inbound and outbound
using the local firewall.</simpara>
<simpara>The following example blocks corosync,
assuming the default corosync
port is used, <literal>firewalld</literal> is used as the local firewall, and the network
interface used by corosync is in the default firewall zone:</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --direct --add-rule ipv4 filter OUTPUT 2 -p udp --dport=5405 -j DROP</literal>
# <literal>firewall-cmd --add-rich-rule='rule family="ipv4" port port="5405" protocol="udp" drop'</literal></literallayout>
</listitem>
<listitem>
<simpara>Simulate a crash and panic your machine
with <literal>sysrq-trigger</literal>. Note, however,
that triggering a kernel panic can cause data loss; it is recommended that
you disable your cluster resources first.</simpara>
<literallayout class="monospaced"># <literal>echo c &gt; /proc/sysrq-trigger</literal></literallayout>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_configuring-fencing-levels-configuring-fencing">
<title>Configuring fencing levels</title>
<simpara>Pacemaker supports fencing nodes with multiple devices through
a feature called fencing topologies.
To implement topologies,
create the individual devices as you normally would
and then define one or more fencing levels in
the fencing topology section in the configuration.</simpara>
<itemizedlist>
<listitem>
<simpara>Each level is attempted in ascending numeric order, starting at 1.</simpara>
</listitem>
<listitem>
<simpara>If a device fails, processing terminates for the current level. No further devices in that level are exercised and the next level is attempted instead.</simpara>
</listitem>
<listitem>
<simpara>If all devices are successfully fenced, then that level has succeeded
and no other levels are tried.</simpara>
</listitem>
<listitem>
<simpara>The operation is finished when a level has passed (success), or all levels
have been attempted (failed).</simpara>
</listitem>
</itemizedlist>
<simpara>Use the following command to add a fencing level to a node.
The devices are given as a comma-separated list of
stonith ids, which are attempted for the node at
that level.</simpara>
<literallayout class="monospaced">pcs stonith level add <emphasis>level</emphasis> <emphasis>node</emphasis> <emphasis>devices</emphasis></literallayout>
<simpara>The following command lists all of the fencing levels that are
currently configured.</simpara>
<literallayout class="monospaced">pcs stonith level</literallayout>
<simpara>In the following example, there are two fence devices configured
for node <literal>rh7-2</literal>: an ilo fence device called
<literal>my_ilo</literal> and an apc fence device
called <literal>my_apc</literal>. These commands
set up fence levels so that if the device <literal>my_ilo</literal>
fails and is unable to fence the node, then Pacemaker
will attempt to use the device <literal>my_apc</literal>.
This example also shows the output of the <literal>pcs stonith level</literal>
command after the levels are configured.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith level add 1 rh7-2 my_ilo</literal>
# <literal>pcs stonith level add 2 rh7-2 my_apc</literal>
# <literal>pcs stonith level</literal>
 Node: rh7-2
  Level 1 - my_ilo
  Level 2 - my_apc</literallayout>
<simpara>The following command removes the fence level for the specified
node and devices. If no nodes or devices are specified then
the fence level you specify is removed from all nodes.</simpara>
<literallayout class="monospaced">pcs stonith level remove <emphasis>level</emphasis> [<emphasis>node_id</emphasis>] [<emphasis>stonith_id</emphasis>] ... [<emphasis>stonith_id</emphasis>]</literallayout>
<simpara>The following command clears the fence levels on the specified node
or stonith id. If you do not specify a node or stonith id, all
fence levels are cleared.</simpara>
<literallayout class="monospaced">pcs stonith level clear [<emphasis>node</emphasis>|<emphasis>stonith_id</emphasis>(s)]</literallayout>
<simpara>If you specify more than one stonith id,
they must be separated by a comma and no spaces,
as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith level clear dev_a,dev_b</literal></literallayout>
<simpara>The following command verifies that all fence devices and nodes
specified in fence levels exist.</simpara>
<literallayout class="monospaced">pcs stonith level verify</literallayout>
<simpara>You can specify nodes in fencing
topology by a regular expression applied on a node name and by a node attribute and its value.
For example, the following commands configure nodes
<literal>node1</literal>, <literal>node2</literal>, and <literal>`node3</literal>
to use fence devices <literal>apc1</literal> and <literal>`apc2</literal>,
and nodes <literal>`node4</literal>, <literal>node5</literal>, and <literal>`node6</literal>
to use fence devices <literal>apc3</literal> and <literal>`apc4</literal>.</simpara>
<literallayout class="monospaced">pcs stonith level add 1 "regexp%node[1-3]" apc1,apc2
pcs stonith level add 1 "regexp%node[4-6]" apc3,apc4</literallayout>
<simpara>The following commands yield the same results by using node attribute matching.</simpara>
<literallayout class="monospaced">pcs node attribute node1 rack=1
pcs node attribute node2 rack=1
pcs node attribute node3 rack=1
pcs node attribute node4 rack=2
pcs node attribute node5 rack=2
pcs node attribute node6 rack=2
pcs stonith level add 1 attrib%rack=1 apc1,apc2
pcs stonith level add 1 attrib%rack=2 apc3,apc4</literallayout>
</section>
<section xml:id="proc_configuring-fencing-for-redundant-power-configuring-fencing">
<title>Configuring fencing for redundant power supplies</title>
<simpara>When configuring fencing for redundant power supplies,
the cluster must ensure that when attempting to reboot a host,
both power supplies are turned off before either power supply is turned back on.</simpara>
<simpara>If the node never completely loses power, the node may not release its resources.
This opens up the possibility of nodes accessing these resources
simultaneously and corrupting them.</simpara>
<simpara>You need to define each device only once and to specify that both are required to fence the node,
as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith create apc1 fence_apc_snmp ipaddr=apc1.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</literal>

# <literal>pcs stonith create apc2 fence_apc_snmp ipaddr=apc2.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</literal>

# <literal>pcs stonith level add 1 node1.example.com apc1,apc2</literal>
# <literal>pcs stonith level add 1 node2.example.com apc1,apc2</literal></literallayout>
</section>
<section xml:id="proc_displaying-configuring-fence-devices-configuring-fencing">
<title>Displaying configured fence devices</title>
<simpara>The following command shows all currently configured fence devices.
If a <emphasis>stonith_id</emphasis> is specified, the command
shows the options for that configured stonith device only. If the
<literal>--full</literal> option is specified, all
configured stonith options are displayed.</simpara>
<literallayout class="monospaced">pcs stonith config [<emphasis>stonith_id</emphasis>] [--full]</literallayout>
</section>
<section xml:id="proc_modifying-fence-devices-configuring-fencing">
<title>Modifying and deleting fence devices</title>
<simpara>Use the following command to modify or add options to a
currently configured fencing device.</simpara>
<literallayout class="monospaced">pcs stonith update <emphasis>stonith_id</emphasis> [<emphasis>stonith_device_options</emphasis>]</literallayout>
<simpara>Use the following command to remove a fencing device from the current
configuration.</simpara>
<literallayout class="monospaced">pcs stonith delete <emphasis>stonith_id</emphasis></literallayout>
</section>
<section xml:id="proc_manually-fencing-a-node-configuring-fencing">
<title>Manually fencing a cluster node</title>
<simpara>You can fence a node manually with the following command.
If you specify <literal role="option">--off</literal> this will
use the <literal>off</literal> API call to stonith which will
turn the node off instead of rebooting it.</simpara>
<literallayout class="monospaced">pcs stonith fence <emphasis>node</emphasis> [--off]</literallayout>
<simpara>In a situation where no stonith device is able to fence a node even if it
is no longer active, the cluster may not be able to recover the resources on
the node. If this occurs, after manually ensuring that the node is powered
down you can enter the following command to confirm to the cluster that the
node is powered down and free its resources for recovery.</simpara>
<warning>
<simpara>If the node you specify is not actually off, but running the
cluster software or services normally controlled by
the cluster, data corruption/cluster failure will occur.</simpara>
</warning>
<literallayout class="monospaced">pcs stonith confirm <emphasis>node</emphasis></literallayout>
</section>
<section xml:id="proc_disabling-a-fence-device-configuring-fencing">
<title>Disabling a fence device</title>
<simpara>To disable a fencing device/resource, you run
the <literal>pcs stonith disable</literal> command.</simpara>
<simpara>The following command disables the fence device <literal>myapc</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs stonith disable myapc</literal></literallayout>
</section>
<section xml:id="proc_preventing-a-node-from-using-a-fence-device-configuring-fencing">
<title>Preventing a node from using a fence device</title>
<simpara>To prevent a specific node from using a fencing device, you can configure location
constraints for the fencing resource.</simpara>
<simpara>The following example prevents fence device <literal>node1-ipmi</literal> from running on <literal>node1</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location node1-ipmi avoids node1</literal></literallayout>
</section>
<section xml:id="proc_configuring-acpi-for-fence-devices-configuring-fencing">
<title>Configuring ACPI for use with integrated fence devices</title>
<simpara>If your cluster uses integrated fence devices, you must
configure ACPI (Advanced Configuration and Power Interface) to
ensure immediate and complete fencing.</simpara>
<simpara>If a cluster node is configured to be fenced by an integrated
fence device, disable ACPI Soft-Off for that node. Disabling
ACPI Soft-Off allows an integrated fence device to turn off a
node immediately and completely rather than attempting a clean
shutdown (for example, <literal role="command">shutdown -h now</literal>).
Otherwise, if ACPI Soft-Off is enabled, an integrated fence
device can take four or more seconds to turn off a node (see
the note that follows). In addition, if ACPI Soft-Off is enabled
and a node panics or freezes during shutdown, an integrated
fence device may not be able to turn off the node. Under those
circumstances, fencing is delayed or unsuccessful. Consequently,
when a node is fenced with an integrated fence device and ACPI
Soft-Off is enabled, a cluster recovers slowly or requires
administrative intervention to recover.</simpara>
<note>
<simpara>The amount of time required to fence a node depends on the
integrated fence device used. Some integrated fence devices
perform the equivalent of pressing and holding the power
button; therefore, the fence device turns off the node in four
to five seconds. Other integrated fence devices perform the
equivalent of pressing the power button momentarily, relying
on the operating system to turn off the node; therefore, the
fence device turns off the node in a time span much longer
than four to five seconds.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>The preferred way to disable ACPI Soft-Off is to
change the BIOS setting to "instant-off" or an equivalent
setting that turns off the node without delay, as described in
<xref linkend="s2-bios-setting-CA"/>.</simpara>
</listitem>
</itemizedlist>
<simpara>Disabling ACPI Soft-Off with the BIOS may not be possible with
some systems.
If disabling ACPI Soft-Off with the BIOS is
not satisfactory for your cluster, you can
disable ACPI Soft-Off with one of the following alternate methods:</simpara>
<itemizedlist>
<listitem>
<simpara>Setting <literal>HandlePowerKey=ignore</literal>
in the <literal>/etc/systemd/logind.conf</literal> file and verifying that the node
node turns off immediately when fenced, as described in
<xref linkend="s2-acpi-disable-logind-CA"/>.
This is the first alternate
method of disabling ACPI Soft-Off.</simpara>
</listitem>
<listitem>
<simpara>Appending <literal>acpi=off</literal> to the kernel boot command line,
as described in
<xref linkend="s2-acpi-disable-boot-CA"/>. This is the second alternate method
of disabling ACPI Soft-Off, if the preferred or the first alternate method is not
available.</simpara>
<important>
<simpara>This method completely disables ACPI; some computers do
not boot correctly if ACPI is completely disabled. Use
this method <emphasis>only</emphasis> if the other methods
are not effective for your cluster.</simpara>
</important>
</listitem>
</itemizedlist>
<section xml:id="s2-bios-setting-CA">
<title>Disabling ACPI Soft-Off with the BIOS</title>
<simpara>You can disable ACPI Soft-Off by configuring the BIOS of each cluster
node with the following procedure.</simpara>
<note>
<simpara>The procedure for disabling ACPI Soft-Off with the BIOS may differ among server systems.
You should verify this procedure with your hardware documentation.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Reboot the node and start the <literal role="command">BIOS CMOS Setup
Utility</literal> program.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the Power menu (or
equivalent power management menu).</simpara>
</listitem>
<listitem>
<simpara>At the Power menu, set the
<literal>Soft-Off by PWR-BTTN</literal> function
(or equivalent) to <literal>Instant-Off</literal>
(or the equivalent setting that turns off the node by means of the
power button without delay). <xref linkend="ex-bios-acpi-off-CA"/> shows a
Power menu with <literal>ACPI
Function</literal> set to
<literal>Enabled</literal> and
<literal>Soft-Off by PWR-BTTN</literal> set to
<literal>Instant-Off</literal>.</simpara>
<note>
<simpara>The equivalents to <literal>ACPI
Function</literal>, <literal>Soft-Off by
PWR-BTTN</literal>, and
<literal>Instant-Off</literal> may vary among
computers. However, the objective of this procedure is
to configure the BIOS so that the computer is turned off
by means of the power button without delay.</simpara>
</note>
</listitem>
<listitem>
<simpara>Exit the <literal role="command">BIOS CMOS Setup Utility</literal>
program, saving the BIOS configuration.</simpara>
</listitem>
<listitem>
<simpara>Verify that the node turns off
immediately when fenced.
For information on testing a fence device, see
<link linkend="proc_testing-fence-devices-configuring-fencing">Testing a fence device</link>.</simpara>
</listitem>
</orderedlist>
<formalpara xml:id="ex-bios-acpi-off-CA">
<title><literal role="command">BIOS CMOS Setup Utility</literal>:</title>
<para>
<literallayout class="monospaced">`Soft-Off by PWR-BTTN` set to
`Instant-Off`</literallayout>
</para>
</formalpara>
<informalexample>
<literallayout class="monospaced">+---------------------------------------------|-------------------+
|    ACPI Function             [Enabled]      |    Item Help      |
|    ACPI Suspend Type         [S1(POS)]      |-------------------|
|  x Run VGABIOS if S3 Resume   Auto          |   Menu Level   *  |
|    Suspend Mode              [Disabled]     |                   |
|    HDD Power Down            [Disabled]     |                   |
|    Soft-Off by PWR-BTTN      [Instant-Off   |                   |
|    CPU THRM-Throttling       [50.0%]        |                   |
|    Wake-Up by PCI card       [Enabled]      |                   |
|    Power On by Ring          [Enabled]      |                   |
|    Wake Up On LAN            [Enabled]      |                   |
|  x USB KB Wake-Up From S3     Disabled      |                   |
|    Resume by Alarm           [Disabled]     |                   |
|  x  Date(of Month) Alarm       0            |                   |
|  x  Time(hh:mm:ss) Alarm       0 :  0 :     |                   |
|    POWER ON Function         [BUTTON ONLY   |                   |
|  x KB Power ON Password       Enter         |                   |
|  x Hot Key Power ON           Ctrl-F1       |                   |
|                                             |                   |
|                                             |                   |
+---------------------------------------------|-------------------+</literallayout>
<simpara>This example shows <literal>ACPI
Function</literal> set to
<literal>Enabled</literal>, and
<literal>Soft-Off by PWR-BTTN</literal> set to
<literal>Instant-Off</literal>.</simpara>
</informalexample>
</section>
<section xml:id="s2-acpi-disable-logind-CA">
<title>Disabling ACPI Soft-Off in the logind.conf file</title>
<simpara>To disable power-key handing
in the <literal>/etc/systemd/logind.conf</literal> file,
use the following procedure.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Define the following configuration in the <literal>/etc/systemd/logind.conf</literal>
file:</simpara>
<literallayout class="monospaced">HandlePowerKey=ignore</literallayout>
</listitem>
<listitem>
<simpara>Restart the <literal>systemd-logind</literal> service:</simpara>
<literallayout class="monospaced"># <literal>systemctl restart systemd-logind.service</literal></literallayout>
</listitem>
<listitem>
<simpara>Verify that the node turns off
immediately when fenced.
For information on testing a fence device, see
<link linkend="proc_testing-fence-devices-configuring-fencing">Testing a fence device</link>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="s2-acpi-disable-boot-CA">
<title>Disabling ACPI completely in the GRUB 2 File</title>
<simpara>You can disable ACPI Soft-Off by appending <literal>acpi=off</literal>
to the GRUB menu entry for a kernel.</simpara>
<important>
<simpara>This method completely disables ACPI; some computers do not
boot correctly if ACPI is completely disabled. Use this
method <emphasis>only</emphasis> if the other methods are
not effective for your cluster.</simpara>
</important>
<simpara>Use the following procedure to disable ACPI in the GRUB 2 file:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use the
<literal role="option">--args</literal> option in combination with the
<literal role="option">--update-kernel</literal> option
of the <literal role="command">grubby</literal>
tool to change the <literal>grub.cfg</literal> file
of each cluster node as follows:</simpara>
<literallayout class="monospaced"># <literal>grubby --args=acpi=off --update-kernel=ALL</literal></literallayout>
</listitem>
<listitem>
<simpara>Reboot the node.</simpara>
</listitem>
<listitem>
<simpara>Verify that the node turns off
immediately when fenced.
For information on testing a fence device, see
<link linkend="proc_testing-fence-devices-configuring-fencing">Testing a fence device</link>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters">
<title>Configuring cluster resources</title>
<simpara>The format for the command to create a cluster resource is as follows:</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource_options</emphasis>] [op <emphasis>operation_action operation_options</emphasis> [<emphasis>operation_action</emphasis> <emphasis>operation options</emphasis>]...] [meta <emphasis>meta_options</emphasis>...] [clone [<emphasis>clone_options</emphasis>] | master [<emphasis>master_options</emphasis>] | --group <emphasis>group_name</emphasis> [--before <emphasis>resource_id</emphasis> | --after <emphasis>resource_id</emphasis>] | [bundle <emphasis>bundle_id</emphasis>] [--disabled] [--wait[=<emphasis>n</emphasis>]]</literallayout>
<simpara>Key cluster resource creation options include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>When you specify the <literal role="option">--group</literal> option, the resource is added
to the resource group named. If the group does not exist, this creates the group and adds
this resource to the group.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>The <literal role="option">--before</literal> and <literal role="option">--after</literal> options
specify the position of the added resource relative to a resource that already
exists in a resource group.</simpara>
</listitem>
<listitem>
<simpara>Specifying the <literal role="option">--disabled</literal> option indicates that the resource is
not started automatically.</simpara>
</listitem>
</itemizedlist>
<simpara>You can determine the behavior of a resource in a cluster by configuring constraints for that resource.</simpara>
<bridgehead xml:id="resource_creation_examples" renderas="sect2" remap="_resource_creation_examples">Resource creation examples</bridgehead>
<simpara>The following command creates a resource with the name <literal>VirtualIP</literal> of
standard <literal>ocf</literal>, provider <literal>heartbeat</literal>, and
type <literal>IPaddr2</literal>. The floating address of this resource is
192.168.0.120, and the system will check whether the resource is running every
30 seconds.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</literal></literallayout>
<simpara>Alternately, you can omit the <emphasis>standard</emphasis> and
<emphasis>provider</emphasis> fields and use the following command.
This will default to a standard of <literal>ocf</literal> and a provider
of <literal>heartbeat</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</literal></literallayout>
<bridgehead xml:id="deleting_a_configured_resource" renderas="sect2" remap="_deleting_a_configured_resource">Deleting a configured resource</bridgehead>
<simpara>Use the following command to delete a configured resource.</simpara>
<literallayout class="monospaced">pcs resource delete <emphasis>resource_id</emphasis></literallayout>
<simpara>For example, the following command deletes an existing
resource with a resource ID of <literal>VirtualIP</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource delete VirtualIP</literal></literallayout>
<section xml:id="ref_resource-properties.adoc-configuring-cluster-resources">
<title>Resource agent identifiers</title>
<simpara>The identifiers that you define for a resource tell the cluster
which agent to use for the resource,
where to find that agent and what standards it conforms to.
<xref linkend="tb-resource-props-summary-HAAR"/>,
describes these properties.</simpara>
<table xml:id="tb-resource-props-summary-HAAR" frame="all" rowsep="1" colsep="1">
<title>Resource Agent Identifiers</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="67*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>standard</simpara></entry>
<entry align="left" valign="top"><simpara>The standard the agent conforms to. Allowed values and their meaning:</simpara><simpara>* <literal>ocf</literal> - The specified <emphasis>type</emphasis> is the name of an executable file conforming to the Open Cluster Framework Resource Agent API and located beneath <literal>/usr/lib/ocf/resource.d/<emphasis>provider</emphasis></literal></simpara><simpara>* <literal>lsb</literal> - The specified <emphasis>type</emphasis> is the name of an executable file conforming to Linux Standard Base Init Script Actions. If the type does not specify a full path, the system will look for it in the <literal>/etc/init.d</literal> directory.</simpara><simpara>* <literal>systemd</literal> - The specified <emphasis>type</emphasis> is the name of an installed <literal>systemd</literal> unit</simpara><simpara>* <literal>service</literal> - Pacemaker will search for the specified <emphasis>type</emphasis>, first as an <literal>lsb</literal> agent, then as a <literal>systemd</literal> agent</simpara><simpara>* <literal>nagios</literal> - The specified <emphasis>type</emphasis> is the name of an executable file conforming to the Nagios Plugin API and located
in the <literal>/usr/libexec/nagios/plugins</literal> directory, with OCF-style metadata stored separately in
the <literal>/usr/share/nagios/plugins-metadata</literal> directory (available in the <literal>nagios-agents-metadata</literal> package for certain common plugins).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>type</simpara></entry>
<entry align="left" valign="top"><simpara>The name of the resource agent you wish to use,
for example <literal>IPaddr</literal> or <literal>Filesystem</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>provider</simpara></entry>
<entry align="left" valign="top"><simpara>The OCF spec allows multiple vendors to supply the same
resource agent.
Most of the agents shipped by Red Hat use <literal>heartbeat</literal>
as the provider.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><xref linkend="tb-resource-displayopts-HAAR"/>
summarizes the commands that
display the available resource properties.</simpara>
<table xml:id="tb-resource-displayopts-HAAR" frame="all" rowsep="1" colsep="1">
<title>Commands to Display Resource Properties</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">pcs Display Command</entry>
<entry align="left" valign="top">Output</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal role="command">pcs resource list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of all available resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="command">pcs resource standards</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of available resource agent standards.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="command">pcs resource providers</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of available resource agent providers.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="command">pcs resource list <emphasis>string</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of available resources filtered by the specified
                    string. You can use this command to display resources filtered by
                    the name of a standard, a provider, or a type.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="proc_displaying-resource-specific-parameters-configuring-cluster-resources">
<title>Displaying resource-specific parameters</title>
<simpara>For any individual resource, you can use the following
command to display a description of the resource,
the parameters you can set for that resource, and the default
values that are set for the resource.</simpara>
<literallayout class="monospaced">pcs resource describe [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis></literallayout>
<simpara>For example, the following command displays information
for a resource of type <literal>apache</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource describe ocf:heartbeat:apache</literal>
This is the resource agent for the Apache Web server.
This resource agent operates both version 1.x and version 2.x Apache
servers.

...</literallayout>
</section>
<section xml:id="proc_configuring-resource-meta-options-configuring-cluster-resources">
<title>Configuring resource meta options</title>
<simpara>In addition to the resource-specific parameters, you
can configure additional resource options
for any resource. These
options are used by the cluster to decide
how your resource should behave.</simpara>
<simpara><xref linkend="tb-resource-options-HAAR"/>
describes the resource meta options.</simpara>
<table xml:id="tb-resource-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Resource Meta Options</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="29*"/>
<colspec colname="col_3" colwidth="43*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>priority</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If not all resources can be active, the cluster will stop lower
priority resources in order to keep higher priority ones active.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>target-role</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>Started</literal></simpara></entry>
<entry align="left" valign="top"><simpara>What state should the cluster attempt to keep this resource in? Allowed values:</simpara><simpara>* <emphasis>Stopped</emphasis> - Force the resource to be stopped</simpara><simpara>* <emphasis>Started</emphasis> - Allow the resource to be started (and in the case of promotable clones, promoted to master role if appropriate)</simpara><simpara>* <emphasis>Master</emphasis> - Allow the resource to be started and, if appropriate, promoted</simpara><simpara>* <emphasis>Slave</emphasis> - Allow the resource to be started, but only in Slave mode if the resource is promotable</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>is-managed</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>true</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Is the cluster allowed to start and stop the resource? Allowed
values: <literal>true</literal>, <literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>resource-stickiness</literal></simpara></entry>
<entry align="left" valign="top"><simpara>0</simpara></entry>
<entry align="left" valign="top"><simpara>Value to indicate how much the resource prefers to
stay where it is.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>requires</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Calculated</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates under what conditions the resource can be started.</simpara><simpara>Defaults to <literal>fencing</literal> except under the
conditions noted below.
Possible values:</simpara><simpara>* <literal>nothing</literal> - The cluster can always start the resource.</simpara><simpara>* <literal>quorum</literal> - The cluster can only start this resource if a majority of
the configured nodes are active. This is the default value if <literal>stonith-enabled</literal>
is <literal>false</literal> or the resource’s <literal>standard</literal>
is <literal>stonith</literal>.</simpara><simpara>* <literal>fencing</literal> - The cluster can only start this resource if a majority
of the configured nodes are active <emphasis>and</emphasis> any failed or unknown nodes
have been fenced.</simpara><simpara>* <literal>unfencing</literal> - The cluster can only start this resource if a majority
of the configured nodes are active <emphasis>and</emphasis> any failed or unknown nodes
have been fenced <emphasis>and</emphasis> only on nodes that have
been <emphasis>unfenced</emphasis>.
This is the default value if the
<literal>provides=unfencing</literal> <literal>stonith</literal> meta option has been
set for a fencing device.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>migration-threshold</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>INFINITY</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How many failures may occur for this resource on a node, before this
node is marked ineligible to host this resource.
A value of 0 indicates that this feature is disabled (the node will never be marked ineligible);
by contrast, the cluster treats <literal>INFINITY</literal>
(the default) as a very large but finite number. This option has an effect
only if the failed operation has <literal>on-fail=restart</literal> (the default),
and additionally for failed start operations if the cluster property
<literal>start-failure-is-fatal</literal> is
<literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>failure-timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal> (disabled)</simpara></entry>
<entry align="left" valign="top"><simpara>Used in conjunction with the
<literal>migration-threshold</literal> option, indicates
how many seconds to wait before acting as if the failure had not
occurred, and potentially allowing the resource back to the node on
which it failed. As with any time-based actions, this is not guaranteed to be checked more
frequently than the value of the <literal>cluster-recheck-interval</literal> cluster parameter.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>multiple-active</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>stop_start</literal></simpara></entry>
<entry align="left" valign="top"><simpara>What should the cluster do if it ever finds the resource active on
more than one node. Allowed values:</simpara><simpara>* <literal>block</literal> - mark the resource as unmanaged</simpara><simpara>* <literal>stop_only</literal> - stop all active instances and leave them that way</simpara><simpara>* <literal>stop_start</literal> - stop all active instances and start the resource in
one location only</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>critical</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>true</literal></simpara></entry>
<entry align="left" valign="top"><simpara>(RHEL 8.4 and later) Sets the default value for the <literal>influence</literal> option for
all colocation constraints involving the resource as a dependent resource (<emphasis>target_resource</emphasis>), including implicit
colocation constraints created when the resource is part of a resource group.
The <literal>influence</literal> colocation constraint option determines whether the cluster will move both
the primary and dependent resources to another node when the dependent resource reaches
its migration threshold for failure, or whether the cluster will leave the
dependent resource offline without causing a service switch. The <literal>critical</literal> resource
meta option can have a value of <literal>true</literal> or <literal>false</literal>, with a default value of <literal>true</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="changing_the_default_value_of_a_resource_option" remap="_changing_the_default_value_of_a_resource_option">
<title>Changing the default value of a resource option</title>
<simpara>As of Red Hat Enterprise Linux 8.3, you can change the default value
of a resource option for all resources with the
<literal>pcs resource defaults update</literal> command.
The following command resets the default value of
<literal>resource-stickiness</literal> to 100.</simpara>
<literallayout class="monospaced"># <literal>pcs resource defaults update resource-stickiness=100</literal></literallayout>
<simpara>The original <literal>pcs resource defaults <emphasis>name</emphasis>=<emphasis>value</emphasis></literal> command, which set defaults for all resources in
previous RHEL 8 releases, remains supported unless there is more than one set of defaults configured. However,
<literal>pcs resource defaults update</literal> is now the preferred version of the command.</simpara>
</section>
<section xml:id="changing_the_default_value_of_a_resource_option_for_sets_of_resources_rhel_8_3_and_later" remap="_changing_the_default_value_of_a_resource_option_for_sets_of_resources_rhel_8_3_and_later">
<title>Changing the default value of a resource option for sets of resources (RHEL 8.3 and later)</title>
<simpara>As of Red Hat Enterprise Linux 8.3, you can
create multiple sets of resource defaults with the
<literal>pcs resource defaults set create</literal> command, which allows you
to specify a rule that contains <literal>resource</literal> expressions.
In RHEL 8.3, only <literal>resource</literal> expressions, including <literal>and</literal>, <literal>or</literal> and parentheses,
are allowed in rules that you specify with this command. In RHEL 8.4 and later,
only <literal>resource</literal> and <literal>date</literal> expressions, including <literal>and</literal>, <literal>or</literal> and parentheses,
are allowed in rules that you specify with this command.</simpara>
<simpara>With the <literal>pcs resource defaults set create</literal> command, you can configure a default resource value
for all resources of a particular type. If, for example, you are running databases which
take a long time to stop, you can increase the <literal>resource-stickiness</literal> default value for all
resources of the database type to prevent those resources from moving to other nodes more often than
you desire.</simpara>
<simpara>The following command sets the default value of <literal>resource-stickiness</literal> to 100 for all resources of type
<literal>pqsql</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>id</literal> option, which names the set of resource defaults, is not mandatory. If you do not set
this option <literal>pcs</literal> will generate an ID automatically. Setting this value allows you to provide
a more descriptive name.</simpara>
</listitem>
<listitem>
<simpara>In this example, <literal>::pgsql</literal> means a resource of any class, any provider, of type <literal>pgsql</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>Specifying <literal>ocf:heartbeat:pgsql</literal> would indicate class <literal>ocf</literal>, provider <literal>heartbeat</literal>, type <literal>pgsql</literal>,</simpara>
</listitem>
<listitem>
<simpara>Specifying <literal>ocf:pacemaker:</literal> would indicate all resources of class <literal>ocf</literal>, provider <literal>pacemaker</literal>, of any type.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<literallayout class="monospaced"># <literal>pcs resource defaults set create id=pgsql-stickiness meta resource-stickiness=100 rule resource ::pgsql</literal></literallayout>
<simpara>To change the default values in an existing set,
use the <literal>pcs resource defaults set update</literal> command.</simpara>
</section>
<section xml:id="displaying_currently_configured_resource_defaults" remap="_displaying_currently_configured_resource_defaults">
<title>Displaying currently configured resource defaults</title>
<simpara>The <literal>pcs resource defaults</literal> command
displays a list of
currently configured default values for resource
options, including any rules you specified.</simpara>
<simpara>The following example shows the
output of this command after you have reset the default
value of <literal>resource-stickiness</literal> to 100.</simpara>
<literallayout class="monospaced"># <literal>pcs resource defaults</literal>
Meta Attrs: rsc_defaults-meta_attributes
  resource-stickiness=100</literallayout>
<simpara>The following example shows the
output of this command after you have reset the default
value of <literal>resource-stickiness</literal> to 100
for all resources of type <literal>pqsql</literal>
and set the <literal>id</literal> option to <literal>id=pgsql-stickiness</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource defaults</literal>
Meta Attrs: pgsql-stickiness
  resource-stickiness=100
  Rule: boolean-op=and score=INFINITY
    Expression: resource ::pgsql</literallayout>
</section>
<section xml:id="setting_meta_options_on_resource_creation" remap="_setting_meta_options_on_resource_creation">
<title>Setting meta options on resource creation</title>
<simpara>Whether you have reset the default value of a resource meta option or
not, you can set a resource option for a particular
resource to a value
other than the default
when you create the
resource. The following shows the format of the
<literal>pcs resource create</literal> command you
use when specifying a value for a resource
meta option.</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource options</emphasis>] [meta <emphasis>meta_options</emphasis>...]</literallayout>
<simpara>For example, the following command
creates a resource with a <literal>resource-stickiness</literal>
value of 50.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 meta resource-stickiness=50</literal></literallayout>
<simpara>You can also set the value of a resource meta option for an existing resource, group, or cloned resource
with the following command.</simpara>
<literallayout class="monospaced">pcs resource meta <emphasis>resource_id</emphasis> | <emphasis>group_id</emphasis> | <emphasis>clone_id</emphasis> <emphasis>meta_options</emphasis></literallayout>
<simpara>In the following example, there is an existing resource named
<literal>dummy_resource</literal>. This command sets the
<literal>failure-timeout</literal> meta option to 20 seconds,
so that the resource can attempt to restart on the
same node in 20 seconds.</simpara>
<literallayout class="monospaced"># <literal>pcs resource meta dummy_resource failure-timeout=20s</literal></literallayout>
<simpara>After executing this command, you can display the values for the
resource to verify that <literal>failure-timeout=20s</literal>
is set.</simpara>
<literallayout class="monospaced"># <literal>pcs resource config dummy_resource</literal>
 Resource: dummy_resource (class=ocf provider=heartbeat type=Dummy)
  Meta Attrs: failure-timeout=20s
  ...</literallayout>
</section>
</section>
<section xml:id="assembly_resource-groups-configuring-cluster-resources">
<title>Configuring resource groups</title>
<simpara>One of the most common elements of a cluster is a set of resources
that need to be located together, start sequentially, and stop in the
reverse order. To simplify this configuration, Pacemaker
supports the concept of resource groups.</simpara>
<section xml:id="proc_creating-resource-groups-resourceegroups">
<title>Creating a resource group</title>
<simpara>You create a resource group with the following command, specifying
the resources to include in the group. If the group does not exist,
this command creates the group. If the group exists, this command
adds additional resources to the group. The resources will start
in the order you specify them with this command, and will
stop in the reverse order of their starting order.</simpara>
<literallayout class="monospaced">pcs resource group add <emphasis>group_name</emphasis> <emphasis>resource_id</emphasis> [<emphasis>resource_id</emphasis>] ... [<emphasis>resource_id</emphasis>] [--before <emphasis>resource_id</emphasis> | --after <emphasis>resource_id</emphasis>]</literallayout>
<simpara>You can use the <literal role="option">--before</literal> and <literal role="option">--after</literal> options
of this command to specify the position of the added resources relative to
a resource that already exists in the group.</simpara>
<simpara>You can also add a new resource to an existing group when you create
the resource, using the following command. The resource you create
is added to the group named <emphasis>group_name</emphasis>.  If the group <emphasis>group_name</emphasis> does
not exist, it will be created.</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [resource_options] [op <emphasis>operation_action</emphasis> <emphasis>operation_options</emphasis>] --group <emphasis>group_name</emphasis></literallayout>
<simpara>There is no
limit to the number of resources a group can contain.
The fundamental properties of a group are as follows.</simpara>
<itemizedlist>
<listitem>
<simpara>Resources are colocated within a group.</simpara>
</listitem>
<listitem>
<simpara>Resources are started in the order in which you specify them.
If a resource in the group cannot run anywhere, then no resource specified
after that resource is allowed to run.</simpara>
</listitem>
<listitem>
<simpara>Resources are stopped in the reverse order in which you
specify them.</simpara>
</listitem>
</itemizedlist>
<simpara>The following example creates a resource group named
<literal>shortcut</literal> that contains the existing resources
<literal>IPaddr</literal> and <literal>Email</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource group add shortcut IPaddr Email</literal></literallayout>
<simpara>In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>IPaddr</literal> is started first,
then <literal>Email</literal>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>Email</literal> resource is stopped first, then <literal>IPAddr</literal>.</simpara>
</listitem>
<listitem>
<simpara>If <literal>IPaddr</literal> cannot run anywhere, neither can <literal>Email</literal>.</simpara>
</listitem>
<listitem>
<simpara>If <literal>Email</literal> cannot run anywhere, however,
this does not affect <literal>IPaddr</literal>
in any way.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="removing_a_resource_group" remap="_removing_a_resource_group">
<title>Removing a resource group</title>
<simpara>You remove a resource from a group with the following command. If there
are no remaining resources in the group, this command removes the group itself.</simpara>
<literallayout class="monospaced">pcs resource group remove <emphasis>group_name</emphasis> <emphasis>resource_id</emphasis>...</literallayout>
</section>
<section xml:id="displaying_resource_groups" remap="_displaying_resource_groups">
<title>Displaying resource groups</title>
<simpara>The following command lists all currently configured resource
groups.</simpara>
<literallayout class="monospaced">pcs resource group list</literallayout>
</section>
<section xml:id="s2-group_options-HAAR">
<title>Group options</title>
<simpara>You can set the following options for a resource group, and they maintain the same meaning
as when they are set for a single resource:
<literal>priority</literal>, <literal>target-role</literal>, <literal>is-managed</literal>.
For information on resource meta options, see
<link linkend="proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</link>.</simpara>
</section>
<section xml:id="s2-group_stickiness-HAAR">
<title>Group stickiness</title>
<simpara>Stickiness, the measure of how much a resource wants to stay where it
is, is additive in groups. Every active resource of the group will
contribute its stickiness value to the group’s total. So if the
default <literal>resource-stickiness</literal> is 100, and a group has seven members,
five of which are active, then the group as a whole will prefer its
current location with a score of 500.</simpara>
</section>
</section>
<section xml:id="con_determining-resource-behavior-configuring-cluster-resources">
<title>Determining resource behavior</title>
<simpara>You can determine the behavior of a resource in
a cluster by configuring
constraints for that resource.
You can configure the following
categories of constraints:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>location</literal> constraints — A location constraint determines which nodes
a resource can run on. For information on configuring location constraints, see
<link linkend="assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters">Determining which nodes a resource can run on</link>.</simpara>
</listitem>
<listitem>
<simpara><literal>order</literal> constraints — An ordering constraint determines the
order in which the resources run. For information on configuring ordering constraints, see
<link linkend="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</link>.</simpara>
</listitem>
<listitem>
<simpara><literal>colocation</literal> constraints — A colocation constraint determines
where resources will be placed relative to other resources. For information on colocation constraints, see
<link linkend="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">Colocating cluster resources</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>As a shorthand for configuring a set of constraints that
will locate a set of resources together and ensure that
the resources start sequentially and stop in reverse order,
Pacemaker supports the concept of resource groups.
After you have created a resource group, you can configure constraints on the group itself
just as you configure constraints for individual resources.
For information on resource groups, see
<link linkend="assembly_resource-groups-configuring-cluster-resources">Configuring resource groups</link>.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters">
<title>Determining which nodes a resource can run on</title>
<simpara>Location constraints determine which nodes a resource
can run on. You can configure location constraints to
determine whether a resource will prefer or avoid
a specified node.</simpara>
<simpara>In addition to location constraints, the node on
which a resource runs is influenced by the
<literal>resource-stickiness</literal> value for that resource, which
determines to what degree a resource prefers to
remain on the node where it is currently running. For
information on setting the <literal>resource-stickiness</literal> value, see
<link linkend="proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</link>.</simpara>
<section xml:id="proc_configuring-location-constraints-determining-which-node-a-resource-runs-on">
<title>Configuring location constraints</title>
<simpara>You can configure a basic location constraint to specify whether
a resource prefers or avoids a node, with an optional <literal>score</literal>
value to indicate the relative degree of preference for the constraint.</simpara>
<simpara>The following command creates a location constraint
for a resource to prefer the specified node or nodes.
Note that it is possible to create constraints on a particular resource for more than
one node with a single command.</simpara>
<literallayout class="monospaced">pcs constraint location <emphasis>rsc</emphasis> prefers <emphasis>node</emphasis>[=<emphasis>score</emphasis>] [<emphasis>node</emphasis>[=<emphasis>score</emphasis>]] ...</literallayout>
<simpara>The following command creates a location constraint
for a resource to avoid the specified node or nodes.</simpara>
<literallayout class="monospaced">pcs constraint location <emphasis>rsc</emphasis> avoids <emphasis>node</emphasis>[=<emphasis>score</emphasis>] [<emphasis>node</emphasis>[=<emphasis>score</emphasis>]] ...</literallayout>
<simpara><xref linkend="tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on"/>
summarizes the meanings of the basic
options for configuring location constraints.</simpara>
<table xml:id="tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on" frame="all" rowsep="1" colsep="1">
<title>Location Constraint Options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="67*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>rsc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A resource name</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>node</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A node’s name</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>score</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Positive integer value to indicate the degree of preference for whether the given
resource should prefer or avoid the given node.
<literal>INFINITY</literal> is the default <literal>score</literal> value for a resource location
constraint.</simpara><simpara>A value of <literal>INFINITY</literal> for <literal>score</literal>
in a <literal>pcs contraint location <emphasis>rsc</emphasis> prefers</literal> command
indicates that the resource
will prefer that node if the node is available, but does not prevent the
resource from running on another node if the specified node is unavailable.</simpara><simpara>A value of <literal>INFINITY</literal> for <literal>score</literal>
in a <literal>pcs contraint location <emphasis>rsc</emphasis> avoids</literal> command
indicates that the resource
will never run on that node, even if no other node is available.
This is the equivalent of setting a <literal>pcs constraint location add</literal> command with a score of <literal>-INFINITY</literal>.</simpara><simpara>A numeric score (that is, not <literal>INFINITY</literal>) means the constraint is optional, and will be honored unless
some other factor outweighs it. For example, if the resource is already placed on a different node,
and its <literal>resource-stickiness</literal> score is higher than a <literal>prefers</literal> location constraint’s score, then the resource
will be left where it is.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The following command creates a location constraint to specify that
the resource <literal>Webserver</literal> prefers
node <literal>node1</literal>.</simpara>
<literallayout class="monospaced">pcs constraint location Webserver prefers node1</literallayout>
<simpara><literal role="command">pcs</literal> supports regular expressions in location constraints on the command line.
These constraints apply to multiple resources based on the regular expression matching resource name.
This allows you to configure multiple location contraints with a single command line.</simpara>
<simpara>The following command creates a location constraint to specify that
resources <literal>dummy0</literal> to <literal>dummy9</literal> prefer <literal>node1</literal>.</simpara>
<literallayout class="monospaced">pcs constraint location 'regexp%dummy[0-9]' prefers node1</literallayout>
<simpara>Since Pacemaker uses POSIX extended regular expressions as documented at
<link xlink:href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04">
<link xlink:href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04">http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04</link></link>,
you can specify the same constraint with the following command.</simpara>
<literallayout class="monospaced">pcs constraint location 'regexp%dummy[[:digit:]]' prefers node1</literallayout>
</section>
<section xml:id="proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on">
<title>Limiting resource discovery to a subset of nodes</title>
<simpara>Before Pacemaker starts a resource anywhere, it first runs
a one-time monitor operation (often referred to as a "probe") on every
node, to learn whether the resource is already running. This process of
resource discovery can result in errors on nodes that are unable to
execute the monitor.</simpara>
<simpara>When configuring a location constraint on a node, you can
use the <literal role="option">resource-discovery</literal> option of the
<literal role="command">pcs constraint location</literal> command to
indicate a preference for whether
Pacemaker should perform resource discovery on this
node for the specified resource. Limiting resource
discovery to a subset of nodes the resource
is physically capable of running on can significantly
boost performance when a large set of nodes is present.
When <literal>pacemaker_remote</literal> is in use to expand the
node count into the hundreds of nodes range,
this option should be considered.</simpara>
<simpara>The following command shows the format for specifying
the <literal role="option">resource-discovery</literal> option of the
<literal role="command">pcs constraint location</literal> command.
In this command, a positive value for <emphasis>score</emphasis>
corresponds to a basic location constraint
that configures a resource to prefer a node,
while a negative
value for <emphasis>score</emphasis>
corresponds to a basic location`constraint
that configures a resource to avoid a node.
As with basic location constraints, you can use regular expressions for
resources with these constraints as well.</simpara>
<literallayout class="monospaced">pcs constraint location add <emphasis>id</emphasis> <emphasis>rsc</emphasis> <emphasis>node</emphasis> <emphasis>score</emphasis> [resource-discovery=<emphasis>option</emphasis>]</literallayout>
<simpara><xref linkend="tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on"/>
summarizes the meanings of the basic
parameters for configuring constraints for resource discovery.</simpara>
<table xml:id="tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on" frame="all" rowsep="1" colsep="1">
<title>Resource Discovery Constraint Parameters</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Field</simpara></entry>
<entry align="left" valign="top"><simpara>Description</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>id</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user-chosen name for the constraint itself.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>rsc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A resource name</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>node</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A node’s name</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>score</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Integer value to indicate the degree of preference for whether the given
resource should prefer or avoid the given node.
A positive value for score corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for score corresponds to a basic location constraint that configures a resource to avoid a node.</simpara><simpara>A value of <literal>INFINITY</literal> for <literal>score</literal>
indicates that the resource
will prefer that node if the node is available, but does not prevent the
resource from running on another node if the specified node is unavailable.
A value of <literal>-INFINITY</literal> for <literal>score</literal>
indicates that the resource
will never run on that node, even if no other node is available.</simpara><simpara>A numeric score (that is, not <literal>INFINITY</literal> or <literal>-INFINITY</literal>) means the constraint is optional, and will be honored unless
some other factor outweighs it. For example, if the resource is already placed on a different node,
and its <literal>resource-stickiness</literal> score is higher than a <literal>prefers</literal> location constraint’s score, then the resource
will be left where it is.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>resource-discovery</literal> options</simpara></entry>
<entry align="left" valign="top"><simpara>* <literal>always</literal> - Always perform resource discovery for the specified resource on this node.
This is the default <literal>resource-discovery</literal>
value for a resource location constraint.</simpara><simpara>* <literal>never</literal> - Never perform resource discovery for the specified resource on this node.</simpara><simpara>* <literal>exclusive</literal> - Perform resource discovery for the specified resource only
on this node (and other nodes similarly marked as <literal>exclusive</literal>).
Multiple location constraints using <literal>exclusive</literal> discovery for the same
resource across different nodes creates a subset of nodes
<literal>resource-discovery</literal> is exclusive to. If a resource is
marked for <literal>exclusive</literal> discovery on one or more nodes,
that resource is only allowed to be placed within that subset of nodes.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<warning>
<simpara>Setting <literal>resource-discovery</literal>
to <literal>never</literal> or <literal>exclusive</literal> removes Pacemaker’s ability to detect and stop
unwanted instances of a service running where it is not supposed to be.
It is up to the system administrator to make sure that the
service can never be active on nodes without resource discovery (such
as by leaving the relevant software uninstalled).</simpara>
</warning>
</section>
<section xml:id="proc_configuring-location-constraint-strategy.adoc-determining-which-node-a-resource-runs-on">
<title>Configuring a location constraint strategy</title>
<simpara>When using location constraints,
you can configure a general strategy for
specifying which nodes a resource can run on:</simpara>
<itemizedlist>
<listitem>
<simpara>Opt-In Clusters — Configure a cluster in which, by
default, no resource can run anywhere and then selectively
enable allowed nodes for specific resources.</simpara>
</listitem>
<listitem>
<simpara>Opt-Out Clusters — Configure a cluster in which, by
default, all resources can run anywhere
and then create location constraints for resources that
are not allowed to run on specific nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>Whether you should choose to configure your cluster as
an opt-in or opt-out cluster depends on both your
personal preference and the make-up of your cluster. If most of your
resources can run on most of the nodes, then an opt-out arrangement is
likely to result in a simpler configuration. On the other hand, if
most resources can only run on a small subset of nodes an opt-in
configuration might be simpler.</simpara>
<section xml:id="s3-optin-clusters-HAAR">
<title>Configuring an "Opt-In" Cluster</title>
<simpara>To create an opt-in cluster, set the <literal>symmetric-cluster</literal>
cluster property to <literal>false</literal> to prevent
resources from running anywhere by default.</simpara>
<literallayout class="monospaced"># <literal>pcs property set symmetric-cluster=false</literal></literallayout>
<simpara>Enable nodes for individual resources. The following
commands configure location constraints so that
the resource <literal>Webserver</literal> prefers
node <literal>example-1</literal>, the
resource <literal>Database</literal> prefers node
<literal>example-2</literal>, and both resources can
fail over to node <literal>example-3</literal> if their
preferred node fails.
When configuring location constraints for an opt-in cluster, setting a score of
zero allows a resource to run on a node without indicating any preference to
prefer or avoid the node.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver prefers example-1=200</literal>
# <literal>pcs constraint location Webserver prefers example-3=0</literal>
# <literal>pcs constraint location Database prefers example-2=200</literal>
# <literal>pcs constraint location Database prefers example-3=0</literal></literallayout>
</section>
<section xml:id="s3-optout-clusters-HAAR">
<title>Configuring an "Opt-Out" Cluster</title>
<simpara>To create an opt-out cluster, set the <literal>symmetric-cluster</literal>
cluster property to <literal>true</literal> to allow
resources to run everywhere by default.
This is the default configuration if
<literal>symmetric-cluster</literal> is not set explicitly.</simpara>
<literallayout class="monospaced"># <literal>pcs property set symmetric-cluster=true</literal></literallayout>
<simpara>The following commands will then yield a configuration
that is equivalent to the example in
<xref linkend="s3-optin-clusters-HAAR"/>. Both resources can
fail over to node <literal>example-3</literal> if their
preferred node fails, since every node has an implicit
score of 0.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver prefers example-1=200</literal>
# <literal>pcs constraint location Webserver avoids example-2=INFINITY</literal>
# <literal>pcs constraint location Database avoids example-1=INFINITY</literal>
# <literal>pcs constraint location Database prefers example-2=200</literal></literallayout>
<simpara>Note that it is not necessary to specify a score of INFINITY in these commands,
since that is the default value for the score.</simpara>
</section>
</section>
<section xml:id="proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">
<title>Configuring a resource to prefer its current node</title>
<simpara>Resources have a <literal>resource-stickiness</literal> value that you can set as a meta attribute
when you create the resource,
as described in
<link linkend="proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</link>.
The <literal>resource-stickiness</literal> value determines how much a resource wants to remain on
the node where it is currently running. Pacemaker considers the
<literal>resource-stickiness</literal> value in conjunction with other settings (for example,
the score values of location constraints) to determine whether to move a resource
to another node or to leave it in place.</simpara>
<simpara>By default, a resource is created with a <literal>resource-stickiness</literal> value of 0.
Pacemaker’s default behavior when <literal>resource-stickiness</literal> is set to 0 and
there are no location constraints is to move resources so that they
are evenly distributed among the cluster nodes. This may result in healthy
resources moving more often than you desire.
To prevent this behavior, you can set the default <literal>resource-stickiness</literal> value to 1.
This default will apply to all resources in the cluster. This small value
can be easily overridden by other constraints that you create, but it
is enough to prevent Pacemaker from needlessly moving healthy resources around the cluster.</simpara>
<simpara>The following command sets the default <literal>resource-stickiness</literal> value to 1.</simpara>
<literallayout class="monospaced"># <literal>pcs resource defaults resource-stickiness=1</literal></literallayout>
<simpara>If the <literal>resource-stickiness</literal> value is set,
then no resources will move to a newly-added node.
If resource balancing is desired at that point, you
can temporarily set the <literal>resource-stickiness</literal> value back to 0.</simpara>
<simpara>Note that if a location constraint score is higher than the
<literal>resource-stickiness</literal> value, the cluster may still move a healthy
resource to the node where the location constraint points.</simpara>
<simpara>For further information about how Pacemaker determines where to place a resource, see
<link linkend="assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters">Configuring a node placement strategy</link>.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">
<title>Determining the order in which cluster resources are run</title>
<simpara>To determine the
order in which the resources run, you configure
an ordering constraint.</simpara>
<simpara>The following shows the format for the command to configure an ordering constraint.</simpara>
<literallayout class="monospaced">pcs constraint order [<emphasis>action</emphasis>] <emphasis>resource_id</emphasis> then [<emphasis>action</emphasis>] <emphasis>resource_id</emphasis> [<emphasis>options</emphasis>]</literallayout>
<simpara><xref linkend="tb-orderconstraint-options-HAAR"/>,
summarizes the properties and
options for configuring ordering constraints.</simpara>
<table xml:id="tb-orderconstraint-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of an Order Constraint</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>resource_id</simpara></entry>
<entry align="left" valign="top"><simpara>The name of a resource on which an action
is performed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>action</simpara></entry>
<entry align="left" valign="top"><simpara>The action to perform on
a resource.
Possible values of the <emphasis>action</emphasis> property
are as follows:</simpara><simpara>* <literal>start</literal> - Start the resource.</simpara><simpara>* <literal>stop</literal> - Stop the resource.</simpara><simpara>* <literal>promote</literal> - Promote the resource from a slave
resource to a master resource.</simpara><simpara>* <literal>demote</literal> - Demote the resource from a master
resource to a slave resource.</simpara><simpara>If no action is specified, the default action is <literal>start</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kind</literal> option</simpara></entry>
<entry align="left" valign="top"><simpara>How to enforce the constraint. The
possible values of the <literal>kind</literal> option are as follows:</simpara><simpara>* <literal>Optional</literal> - Only applies if both resources are
executing the specified action.
For information on optional ordering, see
<link linkend="proc_configuring-advisory-ordering.adoc-determining-resource-order">Configuring advisory ordering</link>.</simpara><simpara>* <literal>Mandatory</literal> - Always enforce the constraint (default value).
If the first resource you specified is stopping or cannot be started,
the second resource you specified must be stopped.
For information on mandatory ordering, see
<link linkend="proc_configuring-mandatory-ordering.adoc-determining-resource-order">Configuring mandatory ordering</link>.</simpara><simpara>* <literal>Serialize</literal> - Ensure that no two stop/start actions occur concurrently
for the resources you specify. The first and second resource you specify can start in either order, but one must complete starting before the other can be started. A typical use case is when resource startup puts a high load on the host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>symmetrical</literal> option</simpara></entry>
<entry align="left" valign="top"><simpara>If true, the reverse of the constraint applies for the opposite action
(for example, if B starts after A starts, then B stops before A stops).
Ordering constraints for which <literal>kind</literal> is <literal>Serialize</literal> cannot be symmetrical.
The default value is <literal>true</literal> for <literal>Mandatory</literal> and <literal>Optional</literal> kinds, <literal>false</literal> for <literal>Serialize</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Use the following command to remove resources from any ordering constraint.</simpara>
<literallayout class="monospaced">pcs constraint order remove <emphasis>resource1</emphasis> [<emphasis>resourceN</emphasis>]...</literallayout>
<section xml:id="proc_configuring-mandatory-ordering.adoc-determining-resource-order">
<title>Configuring mandatory ordering</title>
<simpara>A mandatory ordering constraint indicates that the second action
should not be initiated for the second resource unless and until the
first action successfully completes for the first resource. Actions
that may be ordered are <literal>stop</literal>, <literal>start</literal>, and additionally for
promotable clones, <literal>demote</literal> and <literal>promote</literal>. For example, "A then B"
(which is equivalent to "start A then start B") means that B will not
be started unless and until A successfully starts.
An ordering constraint is mandatory if the <literal>kind</literal> option for
the constraint is set to <literal>Mandatory</literal> or left as default.</simpara>
<simpara>If the <literal>symmetrical</literal> option is set to <literal>true</literal> or left to default,
the opposite actions will be ordered in reverse. The <literal>start</literal> and <literal>stop</literal> actions are
opposites, and <literal>demote</literal> and <literal>promote</literal> are opposites. For example, a
symmetrical "promote A then start B" ordering implies "stop B then
demote A", which means that A cannot be demoted until and unless B
successfully stops. A symmetrical ordering means that changes in A’s state can cause
actions to be scheduled for B. For example, given "A then B", if A
restarts due to failure, B will be stopped first, then A will be
stopped, then A will be started, then B will be started.</simpara>
<simpara>Note that the cluster reacts to each state change.
If the first resource is restarted and is in a started state again
before the second resource initiated a stop
operation, the second resource will not need to be restarted.</simpara>
</section>
<section xml:id="proc_configuring-advisory-ordering.adoc-determining-resource-order">
<title>Configuring advisory ordering</title>
<simpara>When the <literal>kind=Optional</literal> option is specified for
an ordering constraint, the
constraint is considered optional
and only applies if both resources are executing the specified actions.
Any change in state by the first resource you specify will have no effect on the
second resource you specify.</simpara>
<simpara>The following command configures an advisory ordering constraint
for the resources named <literal>VirtualIP</literal> and <literal>dummy_resource</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint order VirtualIP then dummy_resource kind=Optional</literal></literallayout>
</section>
<section xml:id="proc_configuring-ordered-resource-sets.adocdetermining-resource-order">
<title>Configuring ordered resource sets</title>
<simpara>A common situation is for an administrator to create a chain of ordered
resources, where, for example, resource A starts before resource B which starts before
resource C. If your configuration requires that you create a set of resources
that is colocated and started in order, you can configure a resource
group that contains those resources,
as described in
<link linkend="assembly_resource-groups-configuring-cluster-resources">Configuring resource groups</link>.</simpara>
<simpara>There are some situations, however, where configuring the resources
that need to start in a specified order as a resource group is not appropriate:</simpara>
<itemizedlist>
<listitem>
<simpara>You may need to configure resources to start
in order and the resources are not necessarily colocated.</simpara>
</listitem>
<listitem>
<simpara>You may have a resource C that must start after either resource A or B has started but there is no
relationship between A and B.</simpara>
</listitem>
<listitem>
<simpara>You may have resources C and D that must start after both resources A and B have started, but there is
no relationship between A and B or between C and D.</simpara>
</listitem>
</itemizedlist>
<simpara>In these situations, you can create an ordering constraint on a set or sets of resources with
the <literal role="command">pcs constraint order set</literal> command.</simpara>
<simpara>You can set the following options for a set
of resources with the
<literal role="command">pcs constraint order set</literal> command.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>sequential</literal>, which can be set
to <literal>true</literal> or <literal>false</literal> to indicate whether
the set of resources must be ordered relative to each other.
The default value is <literal>true</literal>.</simpara>
<simpara>Setting <literal>sequential</literal> to <literal>false</literal>
allows a set to be ordered relative to other sets in the
ordering constraint, without its members being ordered relative to
each other. Therefore, this option makes sense only if multiple sets
are listed in the constraint; otherwise, the constraint has no effect.</simpara>
</listitem>
<listitem>
<simpara><literal>require-all</literal>, which can be set
to <literal>true</literal> or <literal>false</literal> to indicate whether
all of the resources in the set must be active before continuing.
Setting <literal>require-all</literal> to
<literal>false</literal> means that only one
resource in the set needs to be started before continuing on to the next set.
Setting <literal>require-all</literal> to <literal>false</literal>
has no effect unless used in conjunction with unordered sets,
which are sets for which <literal>sequential</literal> is set
to <literal>false</literal>.
The default value is <literal>true</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>action</literal>, which can be set to
<literal>start</literal>, <literal>promote</literal>,
<literal>demote</literal> or <literal>stop</literal>, as described
in
<link linkend="tb-orderconstraint-options-HAAR">Properties of an Order Constraint</link>.</simpara>
</listitem>
<listitem>
<simpara><literal>role</literal>, which can be set to
<literal>Stopped</literal>,
<literal>Started</literal>, <literal>Master</literal>, or
<literal>Slave</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>You can set the following constraint options for a set
of resources following the <literal>setoptions</literal>
parameter of the
<literal role="command">pcs constraint order set</literal> command.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>id</literal>,
to provide a name for the constraint you are defining.</simpara>
</listitem>
<listitem>
<simpara><literal>kind</literal>, which indicates how to enforce the constraint, as described in
<link linkend="tb-orderconstraint-options-HAAR">Properties of an Order Constraint</link>.</simpara>
</listitem>
<listitem>
<simpara><literal>symmetrical</literal>,
to set whether the reverse of the constraint applies for the opposite action, as described in
<link linkend="tb-orderconstraint-options-HAAR">Properties of an Order Constraint</link>.</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced">pcs constraint order set <emphasis>resource1 resource2</emphasis> [<emphasis>resourceN</emphasis>]... [<emphasis>options</emphasis>] [set <emphasis>resourceX</emphasis> <emphasis>resourceY</emphasis> ... [<emphasis>options</emphasis>]] [setoptions [<emphasis>constraint_options</emphasis>]]</literallayout>
<simpara>If you have three resources named <literal>D1</literal>, <literal>D2</literal>,
and <literal>D3</literal>, the following command configures them as an
ordered resource set.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint order set D1 D2 D3</literal></literallayout>
<simpara>If you have six resources named <literal>A</literal>, <literal>B</literal>, <literal>C</literal>, <literal>D</literal>, <literal>E</literal>, and <literal>F</literal>, this example configures an ordering
constraint for the set of resources that will start as follows:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>A</literal> and <literal>B</literal> start independently of each other</simpara>
</listitem>
<listitem>
<simpara><literal>C</literal> starts once either <literal>A</literal> or <literal>B</literal> has started</simpara>
</listitem>
<listitem>
<simpara><literal>D</literal> starts once <literal>C</literal> has started</simpara>
</listitem>
<listitem>
<simpara><literal>E</literal> and <literal>F</literal> start independently of each other once <literal>D</literal> has started</simpara>
</listitem>
</itemizedlist>
<simpara>Stopping the resources is not influenced by this constraint since <literal>symmetrical=false</literal> is set.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint order set A B sequential=false require-all=false set C D set E F sequential=false setoptions symmetrical=false</literal></literallayout>
</section>
<section xml:id="proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">
<title>Configuring startup order for resource dependencies not managed by Pacemaker</title>
<simpara>It is possible for a cluster to include resources with dependencies that are not
themselves managed by the cluster. In this case, you must ensure that those dependencies
are started before Pacemaker is started and stopped after Pacemaker is stopped.</simpara>
<simpara>You can configure your startup order to account for this situation by means of
the <literal>systemd</literal> <literal>resource-agents-deps</literal> target.
You can create a <literal>systemd</literal> drop-in unit for this target
and Pacemaker will order itself appropriately relative to this
target.</simpara>
<simpara>For example, if a cluster includes a resource that depends on the external
service <literal>foo</literal> that is not managed by the cluster, perform the following procedure.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the drop-in unit <literal>/etc/systemd/system/resource-agents-deps.target.d/foo.conf</literal>
that contains the following:</simpara>
<literallayout class="monospaced">[Unit]
Requires=foo.service
After=foo.service</literallayout>
</listitem>
<listitem>
<simpara>Run the <literal role="command">systemctl daemon-reload</literal> command.</simpara>
</listitem>
</orderedlist>
<simpara>A cluster dependency specified in this way can be something other than a service.
For example, you may have a dependency on mounting a file system at <literal>/srv</literal>,
in which case you would perform the following procedure:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Ensure that <literal>/srv</literal> is listed in the <literal>/etc/fstab</literal> file. This will be converted automatically
to the <literal>systemd</literal> file <literal>srv.mount</literal> at boot when the configuration of the system manager is reloaded.
For more information, see the <literal>systemd.mount</literal>(5) and the <literal>systemd-fstab-generator</literal>(8) man pages.</simpara>
</listitem>
<listitem>
<simpara>To make sure that Pacemaker starts after the disk is mounted,
create the drop-in unit <literal>/etc/systemd/system/resource-agents-deps.target.d/srv.conf</literal>
that contains the following.</simpara>
<literallayout class="monospaced">[Unit]
Requires=srv.mount
After=srv.mount</literallayout>
</listitem>
<listitem>
<simpara>Run the <literal role="command">systemctl daemon-reload</literal> command.</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">
<title>Colocating cluster resources</title>
<simpara>To specify that
the location of one resource
depends on the location of another resource,
you configure a colocation constraint.</simpara>
<simpara>There is an important side effect of creating a colocation constraint
between two resources: it affects the order in which resources are
assigned to a node. This is because you cannot place resource A
relative to resource B unless you know where resource B is.
So when you are creating colocation constraints, it is important to
consider whether you should colocate resource A with resource B or
resource B with resource A.</simpara>
<simpara>Another thing to keep in mind when creating colocation constraints
is that, assuming resource A is colocated with
resource B, the cluster will also take into account resource
A’s preferences when
deciding which node to choose for resource B.</simpara>
<simpara>The following command creates a colocation constraint.</simpara>
<literallayout class="monospaced">pcs constraint colocation add [master|slave] <emphasis>source_resource</emphasis> with [master|slave] <emphasis>target_resource</emphasis> [<emphasis>score</emphasis>] [<emphasis>options</emphasis>]</literallayout>
<simpara><xref linkend="tb-colocationconstraint-options-HAAR"/>,
summarizes the properties and
options for configuring colocation constraints.</simpara>
<table xml:id="tb-colocationconstraint-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Parameters of a Colocation Constraint</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>source_resource</simpara></entry>
<entry align="left" valign="top"><simpara>The colocation source. If the constraint cannot be satisfied, the
cluster may decide not to allow the resource to run at all.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>target_resource</simpara></entry>
<entry align="left" valign="top"><simpara>The colocation target. The cluster will decide where to put this
resource first and then decide where to put the source resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>score</simpara></entry>
<entry align="left" valign="top"><simpara>Positive values indicate the resource should run on the same
node. Negative values indicate the resources should not run on the
same node.
A value of +<literal>INFINITY</literal>, the default value,
indicates that the <emphasis>source_resource</emphasis>
must run on the same node as the <emphasis>target_resource</emphasis>.
A value of -<literal>INFINITY</literal>
indicates that the <emphasis>source_resource</emphasis>
must not run on the same node as the <emphasis>target_resource</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>influence</literal> option</simpara></entry>
<entry align="left" valign="top"><simpara>(RHEL 8.4 and later) Determines whether the cluster will move both the primary resource
(<emphasis>source_resource</emphasis>) and dependent resources (<emphasis>target_resource)</emphasis> to another node when the dependent resource reaches its migration threshold for
failure, or whether the cluster will leave the dependent resource offline without causing a service switch.</simpara><simpara>The <literal>influence</literal> colocation constraint option can have a value of <literal>true</literal> or <literal>false</literal>.
The default value for this option is determined by the value of the dependent resource’s
<literal>critical</literal> resource meta option, which has a default value of <literal>true</literal>.</simpara><simpara>When this option has a value of <literal>true</literal>, Pacemaker will attempt to keep both the primary
and dependent resource active. If the dependent resource reaches its migration threshold
for failures, both resources will move to another node if possible.</simpara><simpara>When this option has a value of <literal>false</literal>, Pacemaker will avoid moving the primary resource
as a result of the status of the dependent resource. In this case, if the dependent
resource reaches its migration threshold for failures, it will stop if the primary
resource is active and can remain on its current node.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="proc_specifying-mandatory-placement.adoc-colocating-cluster-resources">
<title>Specifying mandatory placement of resources</title>
<simpara>Mandatory placement occurs any time the constraint’s score is
<literal>+INFINITY</literal> or <literal>-INFINITY</literal>. In such cases, if the constraint
cannot be
satisfied, then the <emphasis>source_resource</emphasis> is not permitted to run. For
<literal>score=INFINITY</literal>, this includes cases where the <emphasis>target_resource</emphasis>
is not active.</simpara>
<simpara>If you need <literal>myresource1</literal> to always run on the same machine as
<literal>myresource2</literal>, you would add the following constraint:</simpara>
<literallayout class="monospaced"># <literal>pcs constraint colocation add myresource1 with myresource2 score=INFINITY</literal></literallayout>
<simpara>Because <literal>INFINITY</literal> was used,
if <literal>myresource2</literal> cannot run on any
of the cluster nodes (for whatever reason) then <literal>myresource1</literal> will not
be allowed to run.</simpara>
<simpara>Alternatively, you may want to configure the opposite, a
cluster in which
<literal>myresource1</literal> cannot
run on the same machine as <literal>myresource2</literal>. In this case use
<literal>score=-INFINITY</literal></simpara>
<literallayout class="monospaced"># <literal>pcs constraint colocation add myresource1 with myresource2 score=-INFINITY</literal></literallayout>
<simpara>Again, by specifying <literal>-INFINITY</literal>, the constraint is binding. So if the
only place left to run is where <literal>myresource2</literal> already is, then
<literal>myresource1</literal> may not run anywhere.</simpara>
</section>
<section xml:id="proc_specifying-advisory-placement.adoc-colocating-cluster-resources">
<title>Specifying advisory placement of resources</title>
<simpara>If mandatory placement is about "must" and "must not", then advisory
placement is the "I would prefer if" alternative. For constraints with
scores greater than <literal>-INFINITY</literal> and less than <literal>INFINITY</literal>, the cluster
will try to accommodate your wishes but may ignore them if the
alternative is to stop some of the cluster resources.</simpara>
</section>
<section xml:id="proc_colocating-resource-sets.adoc-colocating-cluster-resources">
<title>Colocating sets of resources</title>
<simpara>If your configuration requires that you create a set of resources
that are colocated and started in order, you can configure a resource
group that contains those resources,
as described in
<link linkend="assembly_resource-groups-configuring-cluster-resources">Configuring resource groups</link>.
There are some situations, however, where configuring the resources
that need to be colocated
as a resource group is not appropriate:</simpara>
<itemizedlist>
<listitem>
<simpara>You may need to
colocate a set of resources
but the resources do not necessarily need to
start in order.</simpara>
</listitem>
<listitem>
<simpara>You may have a resource C that must be colocated with either resource A or B, but there is no
relationship between A and B.</simpara>
</listitem>
<listitem>
<simpara>You may have resources C and D that must be colocated with both resources A and B, but there is
no relationship between A and B or between C and D.</simpara>
</listitem>
</itemizedlist>
<simpara>In these situations, you can create
a colocation constraint on a set or sets of resources with
the <literal role="command">pcs constraint colocation set</literal> command.</simpara>
<simpara>You can set the following options for a set
of resources with
the <literal role="command">pcs constraint colocation set</literal> command.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>sequential</literal>, which can be set
to <literal>true</literal> or <literal>false</literal> to indicate whether
the members of the set must be colocated with each other.</simpara>
<simpara>Setting <literal>sequential</literal> to <literal>false</literal>
allows the members of this set to be colocated with
another set listed later in the constraint, regardless of
which members of this set are active. Therefore, this
option makes sense only if another set is listed after
this one in the constraint; otherwise, the constraint has no effect.</simpara>
</listitem>
<listitem>
<simpara><literal>role</literal>, which can be set to
<literal>Stopped</literal>,
<literal>Started</literal>, <literal>Master</literal>, or
<literal>Slave</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>You can set the following constraint option for a set
of resources following the <literal>setoptions</literal>
parameter of the
<literal role="command">pcs constraint colocation set</literal> command.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>id</literal>,
to provide a name for the constraint you are defining.</simpara>
</listitem>
<listitem>
<simpara><literal>score</literal>, to indicate the degree of preference for
this constraint.
For information on this option, see
<link linkend="tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on">Location Constraint Options</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>When listing members of a set, each member is colocated with the one
before it. For example, "set A B" means "B is colocated with A".
However, when listing multiple sets, each set is colocated with the one
after it. For example, "set C D sequential=false set A B" means
"set C D (where C and D have no relation between each other) is colocated
with set A B (where B is colocated with A)".</simpara>
<simpara>The following command creates a colocation constraint
on a set or sets of resources.</simpara>
<literallayout class="monospaced">pcs constraint colocation set <emphasis>resource1 resource2</emphasis> [<emphasis>resourceN</emphasis>]... [<emphasis>options</emphasis>] [set <emphasis>resourceX</emphasis> <emphasis>resourceY</emphasis> ... [<emphasis>options</emphasis>]] [setoptions [<emphasis>constraint_options</emphasis>]]</literallayout>
</section>
<section xml:id="removing_colocation_constraints" remap="_removing_colocation_constraints">
<title>Removing Colocation Constraints</title>
<simpara>Use the following command to remove colocation constraints
with <emphasis>source_resource</emphasis>.</simpara>
<literallayout class="monospaced">pcs constraint colocation remove <emphasis>source_resource</emphasis> <emphasis>target_resource</emphasis></literallayout>
</section>
</chapter>
<chapter xml:id="assembly_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters">
<title>Displaying resource constraints</title>
<simpara>There are a several commands you can use to display
constraints that have been configured.</simpara>
<section xml:id="proc_displaying-resource-constraints.adoc-displaying-resource-constraints">
<title>Displaying all configured constraints</title>
<simpara>The following command lists all current location, order, and
colocation constraints. If the <literal>--full</literal> option is specified, show the
internal constraint IDs.</simpara>
<literallayout class="monospaced">pcs constraint [list|show] [--full]</literallayout>
<simpara>As of RHEL 8.2, listing resource constraints no longer by default displays expired constraints.
To include expired constaints, use the <literal>--all</literal> option of the <literal>pcs constraint</literal> command.
This will list expired constraints, noting the constraints and their associated rules
as <literal>(expired)</literal> in the display.</simpara>
</section>
<section xml:id="displaying_location_constraints" remap="_displaying_location_constraints">
<title>Displaying location constraints</title>
<simpara>The following command lists all current location constraints.</simpara>
<itemizedlist>
<listitem>
<simpara>If <literal>resources</literal> is specified,
location constraints are displayed per
resource. This is the default behavior.</simpara>
</listitem>
<listitem>
<simpara>If <literal>nodes</literal> is specified, location constraints
are displayed per node.</simpara>
</listitem>
<listitem>
<simpara>If specific resources or nodes are specified, then only information
about those resources or nodes is displayed.</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced">pcs constraint location [show [resources [<emphasis>resource</emphasis>...]] | [nodes [<emphasis>node</emphasis>...]]] [--full]</literallayout>
</section>
<section xml:id="displaying_ordering_constraints" remap="_displaying_ordering_constraints">
<title>Displaying ordering constraints</title>
<simpara>The following command lists all current ordering constraints.</simpara>
<literallayout class="monospaced">pcs constraint order [show]</literallayout>
</section>
<section xml:id="displaying_colocation_constraints" remap="_displaying_colocation_constraints">
<title>Displaying colocation constraints</title>
<simpara>The following command lists all current colocation constraints.</simpara>
<literallayout class="monospaced">pcs constraint colocation [show]</literallayout>
</section>
<section xml:id="displaying_resource_specific_constraints" remap="_displaying_resource_specific_constraints">
<title>Displaying resource-specific constraints</title>
<simpara>The following command lists the constraints that reference
specific resources.</simpara>
<literallayout class="monospaced">pcs constraint ref <emphasis>resource</emphasis> ...</literallayout>
</section>
<section xml:id="displaying_resource_dependencies_red_hat_enterprise_linux_8_2_and_later" remap="_displaying_resource_dependencies_red_hat_enterprise_linux_8_2_and_later">
<title>Displaying resource dependencies (Red Hat Enterprise Linux 8.2 and later)</title>
<simpara>The following command displays the relations between cluster resources in a tree structure.</simpara>
<literallayout class="monospaced">pcs resource relations <emphasis>resource</emphasis> [--full]</literallayout>
<simpara>If the <literal>--full</literal> option is used, the command displays additional information, including
the constraint IDs and the resource types.</simpara>
<simpara>In the following example, there are 3 configured resources: C, D, and E.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint order start C then start D</literal>
Adding C D (kind: Mandatory) (Options: first-action=start then-action=start)
# <literal>pcs constraint order start D then start E</literal>
Adding D E (kind: Mandatory) (Options: first-action=start then-action=start)

# <literal>pcs resource relations C</literal>
C
`- order
   |  start C then start D
   `- D
      `- order
         |  start D then start E
         `- E
# <literal>pcs resource relations D</literal>
D
|- order
|  |  start C then start D
|  `- C
`- order
   |  start D then start E
   `- E
# pcs <literal>resource relations E</literal>
E
`- order
   |  start D then start E
   `- D
      `- order
         |  start C then start D
         `- C</literallayout>
<simpara>In the following example, there are 2 configured resources: A and B.
Resources A and B are part of resource group G.</simpara>
<literallayout class="monospaced"># <literal>pcs resource relations A</literal>
A
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- B
# <literal>pcs resource relations B</literal>
B
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- A
# <literal>pcs resource relations G</literal>
G
`- inner resource(s)
   |  members: A B
   |- A
   `- B</literallayout>
</section>
</chapter>
<chapter xml:id="assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters">
<title>Determining resource location with rules</title>
<simpara>For more complicated location constraints, you can use Pacemaker rules
to determine a resource’s location.</simpara>
<section xml:id="ref_pacemaker-rules.adoc-determining-resource-location-with-rules">
<title>Pacemaker rules</title>
<simpara>Rules can be used to make your configuration more dynamic.
One use of rules might be to assign machines to different
processing groups (using a node attribute) based on time and to then
use that attribute when creating location constraints.</simpara>
<simpara>Each rule can contain a number of expressions, date-expressions and
even other rules. The results of the expressions are combined based
on the rule’s <literal>boolean-op</literal> field to determine if the rule ultimately
evaluates to <literal>true</literal> or <literal>false</literal>. What happens next depends on the
context in which the rule is being used.</simpara>
<table xml:id="tb-rule-props-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of a Rule</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>role</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Limits the rule to apply only when the resource is in that
role. Allowed values: <literal>Started</literal>, <literal>Slave,</literal> and <literal>Master</literal>. NOTE: A rule
with <literal>role="Master"</literal> cannot determine the initial location of a
clone instance. It will only affect which of the active instances
will be promoted.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>score</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The score to apply if the rule evaluates to <literal>true</literal>. Limited to use in
rules that are part of location constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>score-attribute</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The node attribute to look up and use as a score if the rule
evaluates to <literal>true</literal>. Limited to use in rules that are part of
location constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>boolean-op</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How to combine the result of multiple expression objects. Allowed
values: <literal>and</literal> and <literal>or</literal>. The
default value is <literal>and</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="node_attribute_expressions" remap="_node_attribute_expressions">
<title>Node attribute expressions</title>
<simpara>Node attribute expressions are used to control a resource based on the
attributes defined by a node or nodes.</simpara>
<table xml:id="tb-expressions-props-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of an Expression</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>attribute</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The node attribute to test</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Determines how the value(s) should be tested. Allowed values:
<literal>string</literal>, <literal>integer</literal>, <literal>number</literal>(RHEL 8.4 and later), <literal>version</literal>. The
default value is <literal>string</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>operation</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The comparison to perform. Allowed values:</simpara><simpara>* <literal>lt</literal> - True if the node attribute’s value is less than <literal>value</literal></simpara><simpara>* <literal>gt</literal> - True if the node attribute’s value is greater than <literal>value</literal></simpara><simpara>* <literal>lte</literal> - True if the node attribute’s value is less than or equal to <literal>value</literal></simpara><simpara>* <literal>gte</literal> - True if the node attribute’s value is greater than or equal to <literal>value</literal></simpara><simpara>* <literal>eq</literal> - True if the node attribute’s value is equal to <literal>value</literal></simpara><simpara>* <literal>ne</literal> - True if the node attribute’s value is not equal to <literal>value</literal></simpara><simpara>* <literal>defined</literal> - True if the node has the named attribute</simpara><simpara>* <literal>not_defined</literal> - True if the node does not have the named attribute</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>value</literal></simpara></entry>
<entry align="left" valign="top"><simpara>User supplied value for comparison (required unless <literal>operation</literal> is <literal>defined</literal> or <literal>not_defined</literal>)</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>In addition to any attributes added by the administrator, the cluster defines
special, built-in node attributes for each node that can also be used,
as described in
<xref linkend="tb-nodeattributes-HAAR"/>.</simpara>
<table xml:id="tb-nodeattributes-HAAR" frame="all" rowsep="1" colsep="1">
<title>Built-in Node Attributes</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>#uname</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Node name</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#id</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Node ID</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#kind</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Node type. Possible values are <literal>cluster</literal>, <literal>remote</literal>,
and <literal>container</literal>. The value of <literal>kind</literal> is
<literal>remote</literal> for Pacemaker Remote nodes created with
the <literal>ocf:pacemaker:remote</literal> resource,
and <literal>container</literal> for Pacemaker Remote guest nodes and bundle nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#is_dc</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>true</literal> if this node is a Designated Controller (DC), <literal>false</literal> otherwise</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#cluster_name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The value of the <literal>cluster-name</literal> cluster property, if set</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#site_name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The value of the <literal>site-name</literal> node attribute, if set, otherwise
identical to <literal>#cluster-name</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>#role</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The role the relevant promotable clone has on this node.
Valid only within a rule for a location constraint for a promotable clone.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="time_date_based_expressions" remap="_time_date_based_expressions">
<title>Time/date based expressions</title>
<simpara>Date expressions are used to control a
resource or cluster option based on the current date/time. They can
contain an optional date specification.</simpara>
<table xml:id="tb-dateexpress-props-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of a Date Expression</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>start</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A date/time conforming to the ISO8601 specification.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>end</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A date/time conforming to the ISO8601 specification.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>operation</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Compares the current date/time with the start or the end date or both the start
and end date, depending on the context. Allowed values:</simpara><simpara>* <literal>gt</literal> - True if the current date/time is after <literal>start</literal></simpara><simpara>* <literal>lt</literal> - True if the current date/time is before <literal>end</literal></simpara><simpara>* <literal>in_range</literal> - True if the current date/time is after <literal>start</literal> and before <literal>end</literal></simpara><simpara>* <literal>date-spec</literal> - performs a cron-like comparison to the current date/time</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="date_specifications" remap="_date_specifications">
<title>Date specifications</title>
<simpara>Date specifications are used to create cron-like expressions relating
to time. Each field can contain a single number or a single range.
Instead of defaulting to zero, any field not supplied is ignored.</simpara>
<simpara>For example, <literal>monthdays="1"</literal> matches the first day of every month and
<literal>hours="09-17"</literal> matches the hours between 9 am and 5 pm (inclusive).
However, you cannot specify <literal>weekdays="1,2"</literal> or
<literal>weekdays="1-2,5-6"</literal> since they contain multiple ranges.</simpara>
<table xml:id="tb-datespecs-props-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of a Date Specification</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>id</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A unique name for the date</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>hours</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 0-23</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>monthdays</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 0-31 (depending on month and year)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>weekdays</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 1-7 (1=Monday, 7=Sunday)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>yeardays</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 1-366 (depending on the year)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>months</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 1-12</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>weeks</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 1-53 (depending on <literal>weekyear</literal>)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>years</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Year according the Gregorian calendar</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>weekyears</literal></simpara></entry>
<entry align="left" valign="top"><simpara>May differ from Gregorian years; for example, <literal>2005-001 Ordinal</literal> is also
<literal>2005-01-01 Gregorian</literal> is also <literal>2004-W53-6 Weekly</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>moon</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allowed values: 0-7 (0 is new, 4 is full moon).</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules">
<title>Configuring a pacemaker location constraint using rules</title>
<simpara>Use the following command to configure a Pacemaker constraint that uses rules.
If <literal>score</literal> is omitted,
it defaults to INFINITY.
If <literal>resource-discovery</literal> is omitted, it defaults to
<literal>always</literal>.</simpara>
<simpara>For information on the <literal>resource-discovery</literal> option, see
<link linkend="proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on">Limiting resource discovery to a subset of nodes</link>.</simpara>
<simpara>As with basic location constraints, you can use regular expressions for
resources with these constraints as well.</simpara>
<simpara>When using rules to configure location constraints, the value of <literal>score</literal>
can be positive or negative, with a positive value indicating "prefers" and a negative
value indicating "avoids".</simpara>
<literallayout class="monospaced">pcs constraint location <emphasis>rsc</emphasis> rule [resource-discovery=<emphasis>option</emphasis>] [role=master|slave] [score=<emphasis>score</emphasis> | score-attribute=<emphasis>attribute</emphasis>] <emphasis>expression</emphasis></literallayout>
<simpara>The <emphasis>expression</emphasis> option can be one of the following
where <emphasis>duration_options</emphasis> and <emphasis>date_spec_options</emphasis>
are: hours, monthdays,
weekdays, yeardays, months, weeks, years, weekyears, and moon
as described in
<link linkend="tb-datespecs-props-HAAR">Properties of a Date Specification</link>.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>defined|not_defined <emphasis>attribute</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal><emphasis>attribute</emphasis> lt|gt|lte|gte|eq|ne [string|integer|number</literal>(RHEL 8.4 and later)<literal>|version] <emphasis>value</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal>date gt|lt <emphasis>date</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal>date in_range <emphasis>date</emphasis> to <emphasis>date</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal>date in_range <emphasis>date</emphasis> to duration <emphasis>duration_options</emphasis> …</literal></simpara>
</listitem>
<listitem>
<simpara><literal>date-spec <emphasis>date_spec_options</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal><emphasis>expression</emphasis> and|or <emphasis>expression</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal>(<emphasis>expression</emphasis>)</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Note that durations are an alternative way to specify an end for <literal>in_range</literal> operations by means of calculations. For example, you can specify a duration of 19 months.</simpara>
<simpara>The following location constraint configures an expression that is true
if now is any time in the year 2018.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver rule score=INFINITY date-spec years=2018</literal></literallayout>
<simpara>The following command configures an expression that is true from
9 am to 5 pm, Monday through Friday. Note that the hours value of
16 matches up to 16:59:59, as the numeric value (hour) still matches.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver rule score=INFINITY date-spec hours="9-16" weekdays="1-5"</literal></literallayout>
<simpara>The following command configures an expression that is true when there is
a full moon on Friday the thirteenth.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver rule date-spec weekdays=5 monthdays=13 moon=4</literal></literallayout>
<simpara>To remove a rule, use the following command.
If the rule that you are removing is the last rule in its constraint,
the constraint will be removed.</simpara>
<literallayout class="monospaced">pcs constraint rule remove <emphasis>rule_id</emphasis></literallayout>
</section>
</chapter>
<chapter xml:id="assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters">
<title>Managing cluster resources</title>
<simpara>This section describes various commands you can
use to manage cluster resources.</simpara>
<section xml:id="proc_display-configured-resources-managing-cluster-resources">
<title>Displaying configured resources</title>
<simpara>To display a list of all configured resources, use
the following command.</simpara>
<literallayout class="monospaced">pcs resource status</literallayout>
<simpara>For example, if your system is configured with
a resource named <literal>VirtualIP</literal> and a resource
named <literal>WebSite</literal>, the <literal role="command">pcs resource
show</literal> command yields the following output.</simpara>
<literallayout class="monospaced"># <literal>pcs resource status</literal>
 VirtualIP	(ocf::heartbeat:IPaddr2):	Started
 WebSite	(ocf::heartbeat:apache):	Started</literallayout>
<simpara>To display a list of all configured resources and
the parameters configured for those resources, use
the <literal>--full</literal> option of the
<literal role="command">pcs resource config</literal> command,
as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs resource config</literal>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s
 Resource: WebSite (type=apache class=ocf provider=heartbeat)
  Attributes: statusurl=http://localhost/server-status configfile=/etc/httpd/conf/httpd.conf
  Operations: monitor interval=1min</literallayout>
<simpara>To display the configured parameters for a resource, use the following
command.</simpara>
<literallayout class="monospaced">pcs resource config <emphasis>resource_id</emphasis></literallayout>
<simpara>For example, the following command displays the currently configured
parameters for resource <literal>VirtualIP</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource config VirtualIP</literal>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s</literallayout>
</section>
<section xml:id="proc_modify-resource-parameters-managing-cluster-resources">
<title>Modifying resource parameters</title>
<simpara>To modify the parameters of a configured resource, use
the following command.</simpara>
<literallayout class="monospaced">pcs resource update <emphasis>resource_id</emphasis> [<emphasis>resource_options</emphasis>]</literallayout>
<simpara>The following sequence of commands show the initial values of
the configured parameters for resource <literal>VirtualIP</literal>,
the command to change the value of the <literal>ip</literal> parameter,
and the values following the update command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource config VirtualIP</literal>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s
# <literal>pcs resource update VirtualIP ip=192.169.0.120</literal>
# <literal>pcs resource config VirtualIP</literal>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.169.0.120 cidr_netmask=24
  Operations: monitor interval=30s</literallayout>
<note>
<simpara>When you update a resource’s operation with the <literal>pcs resource update</literal>
command, any options you do not specifically call out are reset to their default values.</simpara>
</note>
</section>
<section xml:id="proc_cleanup-cluster-resources-managing-cluster-resources">
<title>Clearing failure status of cluster resources</title>
<simpara>If a resource has failed, a failure message appears
when you display the cluster status. If you resolve that
resource, you can clear that failure status with
the <literal role="command">pcs resource cleanup</literal> command.
This command resets the resource status and <literal>failcount</literal>,
telling the cluster to forget the operation history
of a resource and re-detect its current state.</simpara>
<simpara>The following command cleans up the resource
specified by <emphasis>resource_id</emphasis>.</simpara>
<literallayout class="monospaced">pcs resource cleanup <emphasis>resource_id</emphasis></literallayout>
<simpara>If you do not specify a <emphasis>resource_id</emphasis>, this
command resets the resource status and <literal>failcount</literal>for all resources.</simpara>
<simpara>The <literal role="command">pcs resource cleanup</literal> command
probes only the resources that display as a failed action.
To probe all resources on all nodes you can enter the following command:</simpara>
<literallayout class="monospaced">pcs resource refresh</literallayout>
<simpara>By default, the <literal role="command">pcs resource refresh</literal> command probes
only the nodes where a resource’s state is known.
To probe all resources even if the state is not known, enter the following command:</simpara>
<literallayout class="monospaced">pcs resource refresh --full</literallayout>
</section>
<section xml:id="assembly_moving-cluster-resources-managing-cluster-resources">
<title>Moving resources in a cluster</title>
<simpara>Pacemaker provides a variety of mechanisms for configuring a resource to move from one node to another and to
manually move a resource when needed.</simpara>
<simpara>You can manually move resources in a cluster with the
<literal role="command">pcs resource move</literal> and <literal role="command">pcs resource relocate</literal> commands,
as described in
<link linkend="assembly_manually-move-resources-cluster-maintenance">Manually moving cluster resources</link>.</simpara>
<simpara>In addition to these commands, you can also control the behavior of cluster resources
by enabling, disabling, and banning resources, as described in
<link linkend="proc_disabling-resources-cluster-maintenance">Enabling, disabling, and banning cluster resources</link>.</simpara>
<simpara>You can configure a resource so that it will move to a new node after a defined number of failures,
and you can configure a cluster to move resources when external connectivity is lost.</simpara>
<section xml:id="proc_move-resource-from-failure-moving-cluster-resources">
<title>Moving resources due to failure</title>
<simpara>When you create a resource, you can configure the
resource so that it will move to a new node
after a defined number of failures by setting
the <literal>migration-threshold</literal> option
for that resource.
Once the threshold has been reached, this node will no
longer be allowed to run the failed resource until:</simpara>
<itemizedlist>
<listitem>
<simpara>The administrator
manually resets the resource’s <literal>failcount</literal> using
the <literal role="command">pcs resource cleanup</literal> command.</simpara>
</listitem>
<listitem>
<simpara>The resource’s <literal>failure-timeout</literal> value is reached.</simpara>
</listitem>
</itemizedlist>
<simpara>The value of <literal>migration-threshold</literal> is set to <literal>INFINITY</literal>
by default. <literal>INFINITY</literal> is defined internally as a very large
but finite number.
A value of 0 disables the <literal>migration-threshold</literal> feature.</simpara>
<note>
<simpara>Setting a <literal>migration-threshold</literal> for a resource
is not the same as configuring a resource for migration, in which
the resource moves to another location without loss
of state.</simpara>
</note>
<simpara>The following
example adds a migration threshold of 10
to the resource named <literal>dummy_resource</literal>,
which indicates that the resource
will move to a new node after 10
failures.</simpara>
<literallayout class="monospaced"># <literal>pcs resource meta dummy_resource migration-threshold=10</literal></literallayout>
<simpara>You can add a migration threshold to
the defaults for the whole cluster with the following
command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource defaults migration-threshold=10</literal></literallayout>
<simpara>To determine the resource’s current failure status and
limits, use the <literal role="command">pcs resource failcount show</literal>
command.</simpara>
<simpara>There are two exceptions to the migration threshold concept; they
occur when a resource either fails to start or fails to stop.
If the cluster property <literal>start-failure-is-fatal</literal> is
set to <literal>true</literal> (which is the default),
start failures cause the <literal>failcount</literal> to be set to <literal>INFINITY</literal> and thus always
cause the resource to move immediately.</simpara>
<simpara>Stop failures are slightly different and crucial. If a resource fails
to stop and STONITH is enabled, then the cluster will fence the node
in order to be able to start the resource elsewhere. If STONITH is
not enabled, then the cluster has no way to continue and will not try
to start the resource elsewhere, but will try to stop it again after
the failure timeout.</simpara>
</section>
<section xml:id="proc_move-resource-from-connectivity-moving-cluster-resources">
<title>Moving resources due to connectivity changes</title>
<simpara>Setting up the cluster to move resources when external connectivity is
lost is a two step process.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a <literal>ping</literal> resource to the cluster. The
<literal>ping</literal> resource uses the system utility
of the same name to test if a
list of machines (specified by DNS host name or IPv4/IPv6 address) are
reachable and uses the results to maintain a node attribute
called <literal>pingd</literal>.</simpara>
</listitem>
<listitem>
<simpara>Configure a location constraint for the resource that will
move the resource to a different node when connectivity is lost.</simpara>
</listitem>
</orderedlist>
<simpara><xref linkend="tb-resource-props-summary-HAAR"/>
describes the properties you can set for a <literal>ping</literal>
resource.</simpara>
<table xml:id="tb-pingoptions-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of a ping resources</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>dampen</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The time to wait (dampening) for further changes to occur.
This prevents a resource from bouncing around the cluster when cluster
nodes notice the loss of connectivity at slightly different times.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>multiplier</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The number of connected ping nodes gets multiplied by this value to
get a score. Useful when there are multiple ping nodes configured.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>host_list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The machines to contact in order to determine the current
connectivity status. Allowed values include resolvable DNS host
names, IPv4 and IPv6 addresses. The entries in the host list are space separated.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The following example command creates a <literal>ping</literal> resource
that verifies connectivity to <literal>gateway.example.com</literal>.
In practice, you would verify connectivity to your network gateway/router.
You configure the <literal>ping</literal> resource as a clone
so that the resource will run on all cluster nodes.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create ping ocf:pacemaker:ping dampen=5s multiplier=1000 host_list=gateway.example.com clone</literal></literallayout>
<simpara>The following example configures a location constraint
rule for the existing resource named <literal>Webserver</literal>.
This will cause the <literal>Webserver</literal> resource
to move to a host that is able to ping <literal>gateway.example.com</literal>
if the host that it is currently running on cannot ping
<literal>gateway.example.com</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location Webserver rule score=-INFINITY pingd lt 1 or not_defined pingd</literal></literallayout>
<literallayout class="monospaced"> Module included in the following assemblies:
//
// &lt;List assemblies here, each on a new line&gt;
// rhel-8-docs/enterprise/assemblies/assembly_managing-cluster-resources.adoc</literallayout>
</section>
</section>
<section xml:id="proc_disabling-monitor-operationmanaging-cluster-resources">
<title>Disabling a monitor operation</title>
<simpara>The easiest way to stop a recurring monitor is to delete it.
However, there can be times when you only want to disable it temporarily.
In such cases, add <literal>enabled="false"</literal> to the
operation’s definition.
When you want to reinstate the monitoring operation,
set <literal>enabled="true"</literal> to the operation’s definition.</simpara>
<simpara>When you update a resource’s operation
with the <literal>pcs resource update</literal> command,
any options you do not specifically call out are reset to their default values.
For example, if you have
configured a monitoring operation with a custom timeout value of 600, running the following
commands will reset the timeout value to the default value of 20 (or whatever you have set
the default value to with the <literal>pcs resource op defaults</literal> command).</simpara>
<literallayout class="monospaced"># <literal>pcs resource update resourceXZY op monitor enabled=false</literal>
# <literal>pcs resource update resourceXZY op monitor enabled=true</literal></literallayout>
<simpara>In order to maintain the original value of 600 for this option, when you reinstate the
monitoring operation you must specify that value, as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs resource update resourceXZY op monitor timeout=600 enabled=true</literal></literallayout>
</section>
<section xml:id="proc_tagging-cluster-resources-managing-cluster-resources">
<title>Configuring and managing cluster resource tags (RHEL 8.3 and later)</title>
<simpara>As of Red Hat Enterprise Linux 8.3, you can use the <literal>pcs</literal> command to
tag cluster resources. This allows you to enable, disable, manage, or unmanage
a specified set of resources with a single command.</simpara>
<section xml:id="tagging_cluster_resources_for_administration_by_category" remap="_tagging_cluster_resources_for_administration_by_category">
<title>Tagging cluster resources for administration by category</title>
<simpara>The following procedure tags two resources with a resource tag and disables
the tagged resources.
In this example, the existing resources to be tagged
are named <literal>d-01</literal> and <literal>d-02</literal>.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a tag named <literal>special-resources</literal> for resources <literal>d-01</literal> and <literal>d-02</literal>.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs tag create special-resources d-01 d-02</literal></literallayout>
</listitem>
<listitem>
<simpara>Display the resource tag configuration.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs tag config</literal>
special-resources
  d-01
  d-02</literallayout>
</listitem>
<listitem>
<simpara>Disable all resources that are tagged with the <literal>special-resources</literal> tag.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs resource disable special-resources</literal></literallayout>
</listitem>
<listitem>
<simpara>Display the status of the resources to confirm that resources <literal>d-01</literal> and <literal>d-02</literal> are disabled.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs resource</literal>
  * d-01        (ocf::pacemaker:Dummy): Stopped (disabled)
  * d-02        (ocf::pacemaker:Dummy): Stopped (disabled)</literallayout>
</listitem>
</orderedlist>
<simpara>In addition to the <literal>pcs resource disable</literal> command,
the <literal>pcs resource enable</literal>, <literal>pcs resource manage</literal>, and <literal>pcs resource unmanage</literal> commands
support the administration of tagged resources.</simpara>
<simpara>After you have created a resource tag:</simpara>
<itemizedlist>
<listitem>
<simpara>You can delete a resource tag with the <literal>pcs tag delete</literal> command.</simpara>
</listitem>
<listitem>
<simpara>You can modify resource tag configuration for an existing resource tag with the <literal>pcs tag update</literal> command.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="deleting_a_tagged_cluster_resource" remap="_deleting_a_tagged_cluster_resource">
<title>Deleting a tagged cluster resource</title>
<simpara>You cannot delete a tagged cluster resource with the <literal>pcs</literal> command. To delete a tagged
resource, use the following procedure.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Remove the resource tag.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>The following command removes the resource tag <literal>special-resources</literal> from all resources with that tag,</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs tag remove special-resources</literal>
[root@node-01]# <literal>pcs tag</literal>
 No tags defined</literallayout>
</listitem>
<listitem>
<simpara>The following command removes the resource tag <literal>special-resources</literal> from the resource <literal>d-01</literal> only.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs tag update special-resources remove d-01</literal></literallayout>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete the resource.</simpara>
<literallayout class="monospaced">[root@node-01]# <literal>pcs resource delete d-01</literal>
Attempting to stop: d-01... Stopped</literallayout>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters">
<title>Creating cluster resources that are active on multiple nodes (cloned resources)</title>
<simpara>You can clone a cluster resource so that the resource can be active on multiple
nodes. For example, you can use cloned resources to configure multiple
instances of an IP resource to distribute throughout a cluster for
node balancing. You can clone any resource provided the resource
agent supports it.
A clone consists of one resource or one resource group.</simpara>
<note>
<simpara>Only resources that can be active on multiple nodes at the same
time are suitable for cloning. For example, a <literal>Filesystem</literal>
resource mounting a non-clustered file system such as <literal>ext4</literal>
from a shared memory device should not be cloned. Since
the <literal>ext4</literal> partition is not cluster aware, this file system is
not suitable for read/write operations occurring from multiple
nodes at the same time.</simpara>
</note>
<section xml:id="proc_creating-cloned-resource-creating-multinode-resources">
<title>Creating and removing a cloned resource</title>
<simpara>You can create a resource and a clone of that resource at the same time
with the following command.</simpara>
<simpara>RHEL 8.4 and later:</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource options</emphasis>] [meta <emphasis>resource meta options</emphasis>] clone [<emphasis>clone_id</emphasis>] [<emphasis>clone options</emphasis>]</literallayout>
<simpara>RHEL 8.3 and earlier:</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource options</emphasis>] [meta <emphasis>resource meta options</emphasis>] clone [<emphasis>clone options</emphasis>]</literallayout>
<simpara>By default, the name of the clone will be <literal><emphasis>resource_id</emphasis>-clone</literal>. As of RHEL 8.4, you can
set a custom name for the clone by specifying a value for the <emphasis>clone_id</emphasis> option.</simpara>
<simpara>You cannot create a resource
group and a clone of that resource group in a single command.</simpara>
<simpara>Alternately, you can create a clone of a previously-created
resource or resource group with the following command.</simpara>
<simpara>RHEL 8.4 and later:</simpara>
<literallayout class="monospaced">pcs resource clone <emphasis>resource_id</emphasis> | <emphasis>group_id</emphasis> [<emphasis>clone_id</emphasis>] [<emphasis>clone options</emphasis>]...</literallayout>
<simpara>RHEL 8.3 and earlier:</simpara>
<literallayout class="monospaced">pcs resource clone <emphasis>resource_id</emphasis> | <emphasis>group_id</emphasis> [<emphasis>clone options</emphasis>]...</literallayout>
<simpara>By default, the name of the clone will be <literal><emphasis>resource_id</emphasis>-clone</literal>
or <literal><emphasis>group_name</emphasis>-clone</literal>.
As of RHEL 8.4, you can
set a custom name for the clone by specifying a value for the <emphasis>clone_id</emphasis> option.</simpara>
<note>
<simpara>You need to configure resource configuration changes on one node only.</simpara>
</note>
<note>
<simpara>When configuring constraints, always use the name of the group or clone.</simpara>
</note>
<simpara>When you create a clone of a resource, by default the clone takes on the name
of the resource with <literal>-clone</literal> appended to the name.
The following commands creates a resource of type <literal>apache</literal>
named <literal>webfarm</literal> and a clone of that resource
named <literal>webfarm-clone</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create webfarm apache clone</literal></literallayout>
<note>
<simpara>When you create a resource or resource group clone that will be ordered after another clone, you should almost
always set the <literal>interleave=true</literal> option. This ensures that copies of the
dependent clone can stop or start when the clone it depends on has stopped or
started on the same node. If you do not set this option, if a cloned resource B
depends on a cloned resource A and a node leaves the cluster, when the node returns
to the cluster and resource A starts on that node, then all of the copies of resource B on all
of the nodes will restart.
This is because when a dependent cloned resource does not have the <literal>interleave</literal> option
set, all instances of that resource depend on any running instance of the resource it depends on.</simpara>
</note>
<simpara>Use the following command to remove a clone of a resource or a resource group.
This does not remove the resource or resource group itself.</simpara>
<literallayout class="monospaced">pcs resource unclone <emphasis>resource_id</emphasis> | <emphasis>clone_id</emphasis> | <emphasis>group_name</emphasis></literallayout>
<simpara><xref linkend="tb-resourcecloneoptions-HAAR"/>
describes the options you can specify for
a cloned resource.</simpara>
<table xml:id="tb-resourcecloneoptions-HAAR" frame="all" rowsep="1" colsep="1">
<title>Resource Clone Options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>priority, target-role, is-managed</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Options inherited from resource that is being cloned,
as described in
<xref linkend="tb-resource-options-HAAR"/>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>clone-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How many copies of the resource to start. Defaults to the number of
nodes in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>clone-node-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How many copies of the resource can be started on a single node;
the default value is <literal>1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>notify</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When stopping or starting a copy of the clone, tell all the other
copies beforehand and when the action was successful. Allowed values:
<literal>false</literal>, <literal>true</literal>. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>globally-unique</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Does each copy of the clone perform a different function? Allowed
values: <literal>false</literal>, <literal>true</literal></simpara><simpara>If the value of this option is <literal>false</literal>,
these resources behave
identically everywhere they are running and thus there
can be only one
copy of the clone active per machine.</simpara><simpara>If the value of this option is <literal>true</literal>,
a copy of the clone
running on one machine is not equivalent to another instance,
whether that instance is running on another node or on the
same node. The default value is <literal>true</literal> if
the value of <literal>clone-node-max</literal> is greater
than one; otherwise the default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ordered</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Should the copies be started in series (instead of in
parallel). Allowed values: <literal>false</literal>, <literal>true</literal>.
The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>interleave</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Changes the behavior of ordering constraints (between clones)
so that copies of the first clone can start or stop
as soon as the copy
on the same node of the second clone has started or stopped
(rather than waiting until every instance of the second clone has started
or stopped).
Allowed values: <literal>false</literal>, <literal>true</literal>. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>clone-min</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If a value is specified, any clones which are ordered after this clone will not be able to start
until the specified number of instances of the original clone are running, even
if the <literal>interleave</literal> option is set to <literal>true</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>To achieve a stable allocation pattern, clones are slightly sticky by
default, which indicates that they have a slight preference
for staying on the node where they are running. If no value for <literal>resource-stickiness</literal> is provided, the clone
will use a value of 1. Being a small value, it causes minimal
disturbance to the score calculations of other resources but is enough
to prevent Pacemaker from needlessly moving copies around the cluster.
For information on setting the <literal>resource-stickiness</literal> resource meta option, see
<link linkend="proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</link>.</simpara>
</section>
<section xml:id="proc_configuring-clone-constraints-creating-multinode-resources">
<title>Configuring clone resource constraints</title>
<simpara>In most cases, a clone will have a single copy on each active cluster
node. You can, however, set <literal>clone-max</literal>
for the resource clone to a value that is less than the total
number of nodes in the cluster.
If this is the case, you can indicate which nodes the
cluster should preferentially assign copies to with resource location
constraints. These constraints are written no differently to those
for regular resources except that the clone’s id must be used.</simpara>
<simpara>The following command creates a location constraint for the
cluster to preferentially assign
resource clone <literal>webfarm-clone</literal> to <literal>node1</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint location webfarm-clone prefers node1</literal></literallayout>
<simpara>Ordering constraints behave slightly differently for clones.
In the example below, because the <literal>interleave</literal> clone option is left
to default as <literal>false</literal>, no instance of <literal>webfarm-stats</literal> will
start until all instances of <literal>webfarm-clone</literal> that need to be
started have done so. Only if no copies of
<literal>webfarm-clone</literal> can be started then <literal>webfarm-stats</literal> will be prevented
from being active. Additionally, <literal>webfarm-clone</literal> will wait for <literal>webfarm-stats</literal>
to be stopped before stopping itself.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint order start webfarm-clone then webfarm-stats</literal></literallayout>
<simpara>Colocation of a regular (or group) resource with a clone means that
the resource can run on any machine with an active copy of the clone.
The cluster will choose a copy based on where the clone is running and
the resource’s own location preferences.</simpara>
<simpara>Colocation between clones is also possible. In such cases, the set of
allowed locations for the clone is limited to nodes on which the clone
is (or will be) active. Allocation is then performed as normally.</simpara>
<simpara>The following command creates a colocation constraint to ensure
that the resource <literal>webfarm-stats</literal> runs on the same node
as an active copy of <literal>webfarm-clone</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs constraint colocation add webfarm-stats with webfarm-clone</literal></literallayout>
</section>
<section xml:id="assembly_creating-promotable-clone-resources-creating-multinode-resources">
<title>Creating promotable clone resources</title>
<simpara>Promotable clone resources are clone resources with the
<literal>promotable</literal> meta attribute set to <literal>true</literal>.
They allow the instances to be in one of two operating modes; these are
called <literal>Master</literal> and <literal>Slave</literal>.
The names of the modes do not have specific meanings, except for the
limitation that when an instance is started, it
must come up in the <literal>Slave</literal> state.</simpara>
<section xml:id="proc_creating-promotable-resource-creating-promotable-clone-resources">
<title>Creating a promotable resource</title>
<simpara>You can create a resource as a
promotable clone with the following single command.</simpara>
<simpara>RHEL 8.4 and later:</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource options</emphasis>] promotable [<emphasis>clone_id</emphasis>] [<emphasis>clone options</emphasis>]</literallayout>
<simpara>RHEL 8.3 and earlier:</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> [<emphasis>standard</emphasis>:[<emphasis>provider</emphasis>:]]<emphasis>type</emphasis> [<emphasis>resource options</emphasis>] promotable [<emphasis>clone options</emphasis>]</literallayout>
<simpara>By default, the name of the promotable clone will be <literal><emphasis>resource_id</emphasis>-clone</literal>.
As of RHEL 8.4, you can
set a custom name for the clone by specifying a value for the <emphasis>clone_id</emphasis> option.</simpara>
<simpara>Alternately, you can create a promotable resource from
a previously-created
resource or resource group with the following command.</simpara>
<simpara>RHEL 8.4 and later:</simpara>
<literallayout class="monospaced">pcs resource promotable <emphasis>resource_id</emphasis> [<emphasis>clone_id</emphasis>] [<emphasis>clone options</emphasis>]</literallayout>
<simpara>RHEL 8.3 and earlier:</simpara>
<literallayout class="monospaced">pcs resource promotable <emphasis>resource_id</emphasis> [<emphasis>clone options</emphasis>]</literallayout>
<simpara>By default, the name of the
promotable clone will be <literal><emphasis>resource_id</emphasis>-clone</literal>
or <literal><emphasis>group_name</emphasis>-clone</literal>.
As of RHEL 8.4, you can
set a custom name for the clone by specifying a value for the <emphasis>clone_id</emphasis> option.</simpara>
<simpara><xref linkend="tb-promotablecloneoptions-HAAR"/>
describes the extra clone options you can specify for
a promotable resource.</simpara>
<table xml:id="tb-promotablecloneoptions-HAAR" frame="all" rowsep="1" colsep="1">
<title>Extra Clone Options Available for Promotable Clones</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>promoted-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How many copies of the resource can be promoted;
default 1.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>promoted-node-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How many copies of the resource can be promoted on
a single node; default 1.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="proc_configuring-promotable-resource-constraints-creating-promotable-clone-resources">
<title>Configuring promotable resource constraints</title>
<simpara>In most cases, a promotable resources will have a single copy on each
active cluster node. If this is not the case, you can indicate which
nodes the cluster should preferentially assign copies to with resource
location constraints. These constraints are written no differently than
those for regular resources.</simpara>
<simpara>You can create a colocation constraint which specifies whether the
resources are operating in a master or slave role.
The following command creates
a resource colocation constraint.</simpara>
<literallayout class="monospaced">pcs constraint colocation add [master|slave] <emphasis>source_resource</emphasis> with [master|slave] <emphasis>target_resource</emphasis> [<emphasis>score</emphasis>] [<emphasis>options</emphasis>]</literallayout>
<simpara>For information on colocation constraints, see
<link linkend="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">Colocating cluster resources</link>.</simpara>
<simpara>When configuring an ordering constraint that includes promotable resources, one
of the actions that you
can specify for the resources is
<literal>promote</literal>, indicating that the resource be
promoted from slave role to master role. Additionally, you
can specify an action of <literal>demote</literal>, indicated that
the resource be demoted from master role to slave role.</simpara>
<simpara>The command for configuring an order constraint is as follows.</simpara>
<literallayout class="monospaced">pcs constraint order [<emphasis>action</emphasis>] <emphasis>resource_id</emphasis> then [<emphasis>action</emphasis>] <emphasis>resource_id</emphasis> [<emphasis>options</emphasis>]</literallayout>
<simpara>For information on resource order constraints, see
ifdef::
<link linkend="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</link>.</simpara>
</section>
</section>
<section xml:id="proc_recovering-promoted-node-creating-multinode-resources">
<title>Demoting a promoted resource on failure (RHEL 8.3 and later)</title>
<simpara>You can configure a promotable resource so that when a <literal>promote</literal> or <literal>monitor</literal> action
fails for that resource, or the partition in which the resource is running loses quorum,
the resource will be demoted but will not be fully stopped. This can prevent the
need for manual intervention in situations where fully stopping the resource would require it.</simpara>
<itemizedlist>
<listitem>
<simpara>To configure a promotable resource to be demoted when a <literal>promote</literal> action fails, set the <literal>on-fail</literal> operation
meta option to <literal>demote</literal>, as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op add my-rsc promote on-fail="demote"</literal></literallayout>
</listitem>
<listitem>
<simpara>To configure a promotable resource to be demoted when a <literal>monitor</literal> action
fails, set <literal>interval</literal> to a nonzero value, set the <literal>on-fail</literal> operation
meta option to <literal>demote</literal>, and set <literal>role</literal> to <literal>Master</literal>, as in the following example.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op add my-rsc monitor interval="10s" on-fail="demote" role="Master"</literal></literallayout>
</listitem>
<listitem>
<simpara>To configure a cluster so that when a cluster partition loses quorum any promoted resources
will be demoted but left running and all other resources will be stopped, set the
<literal>no-quorum-policy</literal> cluster property to <literal>demote</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Specifying a <literal>demote</literal> meta-attribute for an operation does not
affect how promotion of a resource is determined. If the
affected node still has the highest promotion score,
it will be selected to be promoted again.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_clusternode-management-configuring-and-managing-high-availability-clusters">
<title>Managing cluster nodes</title>
<simpara>The following sections describe the commands you use to manage
cluster nodes, including commands to start and stop
cluster services and to add and remove cluster nodes.</simpara>
<section xml:id="proc_cluster-stop-clusternode-management">
<title>Stopping cluster services</title>
<simpara>The following command stops cluster services on the specified node
or nodes. As with the <literal role="command">pcs cluster start</literal>,
the <literal>--all</literal> option
stops cluster services on all nodes
and if you do not specify any nodes, cluster services are
stopped on the local node only.</simpara>
<literallayout class="monospaced">pcs cluster stop [--all | <emphasis>node</emphasis>] [...]</literallayout>
<simpara>You can force a stop of cluster services on the local node with
the following command, which performs a <literal role="command">kill -9</literal>
command.</simpara>
<literallayout class="monospaced">pcs cluster kill</literallayout>
</section>
<section xml:id="proc_cluster-enable-clusternode-management">
<title>Enabling and disabling cluster services</title>
<simpara>Use the following command to enables the cluster services,
which configures the cluster services
to run on startup on the specified node or nodes.
Enabling allows nodes to
automatically rejoin the cluster after they have
been fenced, minimizing the time the cluster is at less than full strength.
If the cluster services are not enabled, an administrator
can manually investigate what went wrong before starting the
cluster services manually, so that, for example, a node with hardware
issues in not allowed back into the cluster when it is likely to fail again.</simpara>
<itemizedlist>
<listitem>
<simpara>If you specify the <literal>--all</literal> option,
the command enables cluster services on all nodes.</simpara>
</listitem>
<listitem>
<simpara>If you do not specify any nodes, cluster services are
enabled on the local node only.</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced">pcs cluster enable [--all | <emphasis>node</emphasis>] [...]</literallayout>
<simpara>Use the following command to configure the cluster services
not to run on startup on the specified node or nodes.</simpara>
<itemizedlist>
<listitem>
<simpara>If you specify the <literal>--all</literal> option,
the command disables cluster services on all nodes.</simpara>
</listitem>
<listitem>
<simpara>If you do not specify any nodes, cluster services are
disabled on the local node only.</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced">pcs cluster disable [--all | <emphasis>node</emphasis>] [...]</literallayout>
</section>
<section xml:id="proc_cluster-nodeadd-clusternode-management">
<title>Adding cluster nodes</title>
<note>
<simpara>It is highly recommended that you add nodes to existing
clusters only during a production maintenance window. This
allows you to perform appropriate resource and deployment
testing for the new node and its fencing configuration.</simpara>
</note>
<simpara>Use the following procedure to add a new node to
an existing cluster.  This procedure adds standard clusters nodes
running <literal>corosync</literal>. For information on integrating non-corosync nodes
into a cluster, see
<link linkend="assembly_remote-node-management-configuring-and-managing-high-availability-clusters">Integrating non-corosync nodes into a cluster: the pacemaker_remote service</link>.</simpara>
<simpara>In this example, the existing
cluster nodes are
<literal>clusternode-01.example.com</literal>,
<literal>clusternode-02.example.com</literal>, and
<literal>clusternode-03.example.com</literal>. The
new node is
<literal>newnode.example.com</literal>.</simpara>
<simpara>On the new node to add to the cluster, perform the following tasks.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the cluster packages. If the cluster uses SBD, the Booth ticket manager, or
a quorum device, you must manually install the respective packages (<literal>sbd</literal>,
<literal>booth-site</literal>, <literal>corosync-qdevice</literal>) on the
new node as well.</simpara>
<literallayout class="monospaced">[root@newnode ~]# <literal>yum install -y pcs fence-agents-all</literal></literallayout>
<simpara>In addition to the cluster packages, you will also need to install and configure
all of the services that you are running in the cluster, which you have
installed on the existing cluster nodes. For example, if you
are running an Apache HTTP server in a Red Hat high availability cluster, you
will need to install the server on the node you are adding, as well as the <literal>wget</literal> tool
that checks the status of the server.</simpara>
</listitem>
<listitem>
<simpara>If you are running the <literal role="command">firewalld</literal> daemon,
execute the following commands to enable the ports that are required by
the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
# <literal>firewall-cmd  --add-service=high-availability</literal></literallayout>
</listitem>
<listitem>
<simpara>Set a password for the user ID <literal>hacluster</literal>. It is recommended
that you use the same password for each node in the cluster.</simpara>
<literallayout class="monospaced">[root@newnode ~]# <literal>passwd hacluster</literal>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</literallayout>
</listitem>
<listitem>
<simpara>Execute the following
commands to start the <literal>pcsd</literal>
service and to enable <literal>pcsd</literal>
at system start.</simpara>
<literallayout class="monospaced"># <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
</listitem>
</orderedlist>
<simpara>On a node in the existing cluster, perform the following tasks.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Authenticate user <literal>hacluster</literal> on the new cluster node.</simpara>
<literallayout class="monospaced">[root@clusternode-01 ~]# <literal>pcs host auth newnode.example.com</literal>
Username: hacluster
Password:
newnode.example.com: Authorized</literallayout>
</listitem>
<listitem>
<simpara>Add the new node to the existing cluster.
This command also syncs the cluster configuration
file <literal>corosync.conf</literal> to all nodes
in the cluster, including the new node you are adding.</simpara>
<literallayout class="monospaced">[root@clusternode-01 ~]# <literal>pcs cluster node add newnode.example.com</literal></literallayout>
</listitem>
</orderedlist>
<simpara>On the new node to add to the cluster, perform the following tasks.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Start and enable cluster services on the new node.</simpara>
<literallayout class="monospaced">[root@newnode ~]# <literal>pcs cluster start</literal>
Starting Cluster...
[root@newnode ~]# <literal>pcs cluster enable</literal></literallayout>
</listitem>
<listitem>
<simpara>Ensure that you configure and test a fencing device for the new cluster node.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_cluster-noderemove-clusternode-management">
<title>Removing cluster nodes</title>
<simpara>The following command shuts down the specified node
and removes it from the cluster configuration file, <literal>corosync.conf</literal>,
on all of the other nodes in the cluster.</simpara>
<literallayout class="monospaced">pcs cluster node remove <emphasis>node</emphasis></literallayout>
</section>
<section xml:id="proc_add-nodes-to-multiple-ip-cluster-clusternode-management">
<title>Adding a node to a cluster with multiple links</title>
<simpara>When adding a node to a cluster with multiple links, you must specify addresses for all links.
The following example adds the node <literal>rh80-node3</literal> to a cluster,
specifying IP address 192.168.122.203 for the first link and 192.168.123.203 as the second link.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster node add rh80-node3 addr=192.168.122.203 addr=192.168.123.203</literal></literallayout>
</section>
<section xml:id="proc_changing-links-in-multiple-ip-cluster-clusternode-management">
<title>Adding and modifying links in an existing cluster (RHEL 8.1 and later)</title>
<simpara>In most cases, you can add or modify the links
in an existing cluster without restarting the cluster.</simpara>
<section xml:id="adding_and_removing_links_in_an_existing_cluster" remap="_adding_and_removing_links_in_an_existing_cluster">
<title>Adding and removing links in an existing cluster</title>
<simpara>To add a new link to a running cluster, use the <literal>pcs cluster link add</literal> command.</simpara>
<itemizedlist>
<listitem>
<simpara>When adding a link, you must specify an address for each node.</simpara>
</listitem>
<listitem>
<simpara>Adding and removing a link is only possible when you are using the knet transport protocol.</simpara>
</listitem>
<listitem>
<simpara>At least one link in the cluster must be defined at any time.</simpara>
</listitem>
<listitem>
<simpara>The maximum number of links in a cluster is 8, numbered 0-7. It does not  matter which links are defined,
so, for example, you can define only links 3, 6 and 7.</simpara>
</listitem>
<listitem>
<simpara>When you add a link without specifying its link number, <literal>pcs</literal> uses the lowest link available.</simpara>
</listitem>
<listitem>
<simpara>The link numbers of currently configured links are contained in the <literal>corosync.conf</literal> file. To
display the <literal>corosync.conf</literal> file, run  the <literal>pcs cluster corosync</literal> command or
(for RHEL 8.4 and later) the <literal>pcs cluster config show</literal> command.</simpara>
</listitem>
</itemizedlist>
<simpara>The following command adds link number 5 to a three node cluster.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=5</literal></literallayout>
<simpara>To remove an existing link, use the <literal>pcs cluster link delete</literal> or <literal>pcs cluster link remove</literal> command.
Either of the following commands will remove link number 5 from the cluster.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link delete 5</literal>

[root@node1 ~] # <literal>pcs cluster link remove 5</literal></literallayout>
</section>
<section xml:id="modifying_a_link_in_a_cluster_with_multiple_links" remap="_modifying_a_link_in_a_cluster_with_multiple_links">
<title>Modifying a link in a cluster with multiple links</title>
<simpara>If there are multiple links in the cluster and you want to change
one of them, perform the following procedure.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Remove the link you want to change.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 2</literal></literallayout>
</listitem>
<listitem>
<simpara>Add the link back to the cluster with the updated addresses and options.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying_the_link_addresses_in_a_cluster_with_a_single_link" remap="_modifying_the_link_addresses_in_a_cluster_with_a_single_link">
<title>Modifying the link addresses in a cluster with a single link</title>
<simpara>If your cluster uses only one link and you want to modify
that link to use different addresses, perform the following
procedure. In this example, the original link is link 1.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a new link with the new addresses and options.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2</literal></literallayout>
</listitem>
<listitem>
<simpara>Remove the original link.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 1</literal></literallayout>
</listitem>
</orderedlist>
<simpara>Note that you cannot specify addresses that are currently in use when adding links to a cluster. This means,
for example, that if you have a two-node cluster with one link and you want to change the address for
one node only, you cannot use the above procedure to add a new link that specifies one new address
and one existing address. Instead, you can add a temporary link before removing the existing link and adding
it back with the updated address, as in the following example.</simpara>
<simpara>In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>The link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address
10.0.5.12 for node 2.</simpara>
</listitem>
<listitem>
<simpara>You would like to change the address for node 2 to 10.0.5.31.</simpara>
</listitem>
</itemizedlist>
<simpara>To update only one of the addresses for a two-node cluster with a single link, use the following procedure.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a new temporary link to the existing cluster, using addresses that are not currently in use.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2</literal></literallayout>
</listitem>
<listitem>
<simpara>Remove the original link.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 1</literal></literallayout>
</listitem>
<listitem>
<simpara>Add the new, modified link.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.11 node2=10.0.5.31 options linknumber=1</literal></literallayout>
</listitem>
<listitem>
<simpara>Remove the temporary link you created</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 2</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link" remap="_modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link">
<title>Modifying the link options for a link in a cluster with a single link</title>
<simpara>If your cluster uses only one link and you want to modify the
options for that link but you do not want to change the address to
use, you can add a temporary link before removing and updating the link to modify.</simpara>
<simpara>In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>The link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address
10.0.5.12 for node 2.</simpara>
</listitem>
<listitem>
<simpara>You would like to change the link option <literal>link_priority</literal> to 11.</simpara>
</listitem>
</itemizedlist>
<simpara>Use the following procedure to modify the link option in a cluster with a single link.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a new temporary link to the existing cluster, using addresses that are not currently in use.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2</literal></literallayout>
</listitem>
<listitem>
<simpara>Remove the original link.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 1</literal></literallayout>
</listitem>
<listitem>
<simpara>Add back the original link with the updated options.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 options linknumber=1 link_priority=11</literal></literallayout>
</listitem>
<listitem>
<simpara>Remove the temporary link.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link remove 2</literal></literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying_a_link_when_adding_a_new_link_is_not_possible" remap="_modifying_a_link_when_adding_a_new_link_is_not_possible">
<title>Modifying a link when adding a new link is not possible</title>
<simpara>If for some reason adding a new link is not possible in your configuration
and your only option is to modify a single existing link,
you can use the following procedure, which requires that you
shut your cluster down.</simpara>
<simpara>The following example procedure updates link number 1
in the cluster
and sets the <literal>link_priority</literal> option for the link to 11.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Stop the cluster services for the cluster.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster stop --all</literal></literallayout>
</listitem>
<listitem>
<simpara>Update the link addresses and options.</simpara>
<simpara>The <literal>pcs cluster link update</literal> command does not require that you specify all of the node addresses and
options. Instead, you can specify only the addresses to change. This example modifies the addresses for <literal>node1</literal> and <literal>node3</literal>
and the <literal>link_priority</literal> option only.</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster link update 1 node1=10.0.5.11 node3=10.0.5.31 options link_priority=11</literal></literallayout>
<simpara>To remove an option, you can set the option to a null
value with the <literal><emphasis>option</emphasis>=</literal> format.</simpara>
</listitem>
<listitem>
<simpara>Restart the cluster</simpara>
<literallayout class="monospaced">[root@node1 ~] # <literal>pcs cluster start --all</literal></literallayout>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="proc_configuring-large-clusters-clusternode-management">
<title>Configuring a large cluster with many resources</title>
<simpara>If the cluster you are deploying consists of a large number of nodes and many
resources, you may need to modify the default values of the following parameters for
your cluster.</simpara>
<variablelist>
<varlistentry>
<term>The <literal>cluster-ipc-limit</literal> cluster property</term>
<listitem>
<simpara>The <literal>cluster-ipc-limit</literal> cluster property is the maximum IPC message backlog before one cluster daemon will disconnect another. When a large number of resources are cleaned up or otherwise modified simultaneously in a large cluster, a large number of CIB updates arrive at once. This could cause slower clients to be evicted if the Pacemaker service does not have time to process all of the configuration updates before the CIB event queue threshold is reached.</simpara>
<simpara>The recommended value of <literal>cluster-ipc-limit</literal> for use in large clusters is the number of resources in the cluster multiplied by the number of nodes. This value can be raised if you see "Evicting client" messages for cluster daemon PIDs in the logs.</simpara>
<simpara>You can increase the value of <literal>cluster-ipc-limit</literal> from its default value of 500 with the <literal>pcs property set</literal> command. For example, for a ten-node cluster with 200 resources you can set the value of <literal>cluster-ipc-limit</literal> to 2000 with the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs property set cluster-ipc-limit=2000</literal></literallayout>
</listitem>
</varlistentry>
<varlistentry>
<term>The <literal>PCMK_ipc_buffer</literal> Pacemaker parameter</term>
<listitem>
<simpara>On very large deployments, internal Pacemaker messages may exceed the size of the message buffer. When this occurs, you will see a message in the system logs of the following format:</simpara>
<literallayout class="monospaced"><literal>Compressed message exceeds <emphasis>X</emphasis>% of configured IPC limit (<emphasis>X</emphasis> bytes); consider setting PCMK_ipc_buffer to <emphasis>X</emphasis> or higher</literal></literallayout>
<simpara>When you see this message, you can increase the value of <literal>PCMK_ipc_buffer</literal> in the <literal>/etc/sysconfig/pacemaker</literal> configuration file on each node. For example, to increase the value of <literal>PCMK_ipc_buffer</literal> from its default value to 13396332 bytes, change the uncommented <literal>PCMK_ipc_buffer</literal> field in the <literal>/etc/sysconfig/pacemaker</literal> file on each node in the cluster as follows.</simpara>
<literallayout class="monospaced"><literal>PCMK_ipc_buffer=13396332</literal></literallayout>
<simpara>To apply this change, run the following comand.</simpara>
<literallayout class="monospaced"># <literal>systemctl restart pacemaker</literal></literallayout>
</listitem>
</varlistentry>
</variablelist>
</section>
</chapter>
<chapter xml:id="assembly_cluster-permissions-configuring-and-managing-high-availability-clusters">
<title>Setting user permissions for a Pacemaker cluster</title>
<simpara>You can grant permission for specific users other than
user <literal>hacluster</literal> to manage a Pacemaker cluster.
There are two sets of permissions that you can grant to individual users:</simpara>
<itemizedlist>
<listitem>
<simpara>Permissions that allow individual users to manage the cluster through the
Web UI and to run <literal role="command">pcs</literal> commands that connect to
nodes over a network.
Commands that connect to nodes over a network include commands
to set up a cluster, or to add or remove nodes from a cluster.</simpara>
</listitem>
<listitem>
<simpara>Permissions for
local users to allow read-only or read-write access to the cluster
configuration.
Commands that do not require connecting over a network
include commands that edit the cluster configuration,
such as those that create resources and configure constraints.</simpara>
</listitem>
</itemizedlist>
<simpara>In situations where both sets of permissions have been assigned,
the permissions for commands that connect over a network are applied first,
and then permissions for editing the cluster configuration on the local
node are applied. Most <literal role="command">pcs</literal> commands do not require
network access and in those cases the network permissions will not apply.</simpara>
<section xml:id="proc_setting-cluster-access-over-network-cluster-permissions">
<title>Setting permissions for node access over a network</title>
<simpara>To grant permission for specific users to manage the cluster through the Web UI
and to run <literal role="command">pcs</literal> commands that connect to nodes over a network,
add those users to the group <literal>haclient</literal>. This must be done on every node in
the cluster.</simpara>
</section>
<section xml:id="proc_setting-local-cluster-permissions-cluster-permissions">
<title>Setting local permissions using ACLs</title>
<simpara>You can use the
<literal role="command">pcs acl</literal> command to set permissions for
local users to allow read-only or read-write access to the cluster
configuration by using access control lists (ACLs).</simpara>
<simpara>By default, ACLs are not enabled. When ACLs are not enabled,
the root user and any user who is a member
of the group <literal>haclient</literal> on all nodes has full local read/write
access to the cluster configuration while users who are not members
of <literal>haclient</literal> have no access.
When ACLs are enabled, however, even users who are members
of the <literal>haclient</literal> group have access only to what has been
granted to that user by the ACLs.</simpara>
<simpara>Setting permissions for local users is a two step process:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Execute the <literal role="command">pcs acl role create…</literal> command to
create a <emphasis>role</emphasis> which defines the permissions for that role.</simpara>
</listitem>
<listitem>
<simpara>Assign the role you created to a user with the <literal role="command">pcs acl user create</literal> command. If you
assign multiple roles to the same user, any <literal>deny</literal> permission takes
precedence, then <literal>write</literal>, then <literal>read</literal>.</simpara>
</listitem>
</orderedlist>
<simpara>The following example procedure provides read-only access for a cluster configuration to
a local user named <literal>rouser</literal>. Note that it is also possible to restrict access
to certain portions of the configuration only.</simpara>
<warning>
<simpara>It is important to perform this procedure as root or to save all of the configuration
updates to a working file which you can then push to the active CIB when you are finished.
Otherwise, you can lock yourself out of making any further changes. For information
on saving configuration updates to a working file, see
<link linkend="proc_configure-testfile-pcs-operation">Saving a configuration change to a working file</link>.</simpara>
</warning>
<orderedlist numeration="arabic">
<listitem>
<simpara>This procedure requires that the user <literal>rouser</literal> exists on the local system and
that the user <literal>rouser</literal> is a member of the group <literal>haclient</literal>.</simpara>
<literallayout class="monospaced"># <literal>adduser rouser</literal>
# <literal>usermod -a -G haclient rouser</literal></literallayout>
</listitem>
<listitem>
<simpara>Enable Pacemaker ACLs with the <literal role="command">pcs acl enable</literal> command.</simpara>
<literallayout class="monospaced"># <literal>pcs acl enable</literal></literallayout>
</listitem>
<listitem>
<simpara>Create a role named <literal>read-only</literal> with read-only permissions for the cib.</simpara>
<literallayout class="monospaced"># <literal>pcs acl role create read-only description="Read access to cluster" read xpath /cib</literal></literallayout>
</listitem>
<listitem>
<simpara>Create the user <literal>rouser</literal> in the pcs ACL system and assign that
user the <literal>read-only</literal> role.</simpara>
<literallayout class="monospaced"># <literal>pcs acl user create rouser read-only</literal></literallayout>
</listitem>
<listitem>
<simpara>View the current ACLs.</simpara>
<literallayout class="monospaced"># <literal>pcs acl</literal>
User: rouser
  Roles: read-only
Role: read-only
  Description: Read access to cluster
  Permission: read xpath /cib (read-only-read)</literallayout>
</listitem>
<listitem>
<simpara>On each node where <literal>rouser</literal> will run <literal>pcs</literal> commands, log in as <literal>rouser</literal> and authenticate to the local <literal>pcsd</literal> service. This is
required in order to run certain <literal>pcs</literal> commands, such as <literal>pcs status</literal>, as the ACL user.</simpara>
<literallayout class="monospaced">[rouser ~]$ <literal>pcs client local-auth</literal></literallayout>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters">
<title>Resource monitoring operations</title>
<simpara>To ensure that resources remain healthy, you can
add a monitoring operation to a resource’s definition.
If you do not specify a monitoring operation for
a resource, by default the <literal role="command">pcs</literal> command will create
a monitoring operation, with an interval that is determined
by the resource agent. If the resource agent does not
provide a default monitoring interval, the pcs command
will create a monitoring operation with an interval of
60 seconds.</simpara>
<simpara><xref linkend="tb-resource-operation-HAAR"/>
summarizes the properties of a resource monitoring operation.</simpara>
<table xml:id="tb-resource-operation-HAAR" frame="all" rowsep="1" colsep="1">
<title>Properties of an Operation</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>id</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Unique name for the action.
The system assigns this when
you configure an operation.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The action to perform. Common values: <literal>monitor</literal>, <literal>start</literal>, <literal>stop</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If set to a nonzero value, a recurring operation is created that
repeats at this frequency, in seconds. A nonzero value makes sense
only when the action <literal>name</literal> is set to <literal>monitor</literal>.
A recurring monitor action will be executed immediately after a resource
start completes, and subsequent monitor actions are scheduled starting
at the time the previous monitor action completed.
For example, if a monitor action with <literal>interval=20s</literal>
is executed at 01:00:00, the next monitor action does
not occur at 01:00:20, but at 20 seconds after the first monitor action completes.</simpara><simpara>If set to zero, which is the default value, this parameter
allows you to provide values
to be used for operations created by the cluster.
For example, if the <literal>interval</literal> is set
to zero, the <literal>name</literal> of the operation
is set to <literal>start</literal>, and the <literal>timeout</literal>
value is set to 40, then Pacemaker will use
a timeout of 40 seconds when starting this resource.
A <literal>monitor</literal> operation with a zero interval
allows you to set the
<literal>timeout</literal>/<literal>on-fail</literal>/<literal>enabled</literal> values for the
probes that Pacemaker does
at startup to get the current status of all resources when
the defaults are not desirable.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If the operation does not complete in the amount of time
set by this parameter, abort the operation and consider it
failed.
The default value is the value of <literal>timeout</literal>
if set with
the <literal role="command">pcs resource op defaults</literal> command,
or 20 seconds if it is not set.
If you find that your system includes a resource that requires more
time than the system allows to perform an operation
(such as <literal>start</literal>, <literal>stop</literal>, or <literal>monitor</literal>),
investigate the cause and if the lengthy execution
time is expected you can increase this value.</simpara><simpara>The <literal>timeout</literal> value is not a delay of any kind, nor does the cluster wait
the entire timeout period if the operation returns before the timeout period has completed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>on-fail</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The action to take if this action ever fails. Allowed values:</simpara><simpara>* <literal>ignore</literal> - Pretend the resource did not fail</simpara><simpara>* <literal>block</literal> - Do not perform any further operations on the resource</simpara><simpara>* <literal>stop</literal> - Stop the resource and do not start it elsewhere</simpara><simpara>* <literal>restart</literal> - Stop the resource and start it again (possibly on a different node)</simpara><simpara>* <literal>fence</literal> - STONITH the node on which the resource failed</simpara><simpara>* <literal>standby</literal> - Move <emphasis>all</emphasis> resources away from the node on which the resource failed</simpara><simpara>* <literal>demote</literal> - When a <literal>promote</literal> action fails for the resource, the resource
will be demoted but will not be fully stopped. When a <literal>monitor</literal> action fails
for a resource,
if <literal>interval</literal> is set to a nonzero value and <literal>role</literal> is set to <literal>Master</literal>
the resource will be demoted but will not be fully stopped.</simpara><simpara>The default for the <literal>stop</literal> operation is <literal>fence</literal> when STONITH is
enabled and <literal>block</literal> otherwise. All other operations default to <literal>restart</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>enabled</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If <literal>false</literal>, the operation is treated as if it does not exist. Allowed
values: <literal>true</literal>, <literal>false</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="proc_configuring-resource-monitoring-operations-resource-monitoring-operations">
<title>Configuring resource monitoring operations</title>
<simpara>You can configure monitoring operations when you create
a resource, using the following command.</simpara>
<literallayout class="monospaced">pcs resource create <emphasis>resource_id</emphasis> <emphasis>standard:provider:type|type</emphasis> [<emphasis>resource_options</emphasis>] [op <emphasis>operation_action</emphasis> <emphasis>operation_options</emphasis> [<emphasis>operation_type</emphasis> <emphasis>operation_options</emphasis>]...]</literallayout>
<simpara>For example, the following command creates an <literal>IPaddr2</literal> resource with
a monitoring operation.
The new resource is called <literal>VirtualIP</literal> with
an IP address of 192.168.0.99 and a netmask of 24 on <literal>eth2</literal>.
A monitoring operation
will be performed every 30 seconds.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2 op monitor interval=30s</literal></literallayout>
<simpara>Alternately, you can add a monitoring operation to
an existing resource with the following command.</simpara>
<literallayout class="monospaced">pcs resource op add <emphasis>resource_id</emphasis> <emphasis>operation_action</emphasis> [<emphasis>operation_properties</emphasis>]</literallayout>
<simpara>Use the following command to delete a configured resource
operation.</simpara>
<literallayout class="monospaced">pcs resource op remove <emphasis>resource_id</emphasis> <emphasis>operation_name</emphasis> <emphasis>operation_properties</emphasis></literallayout>
<note>
<simpara>You must specify the exact operation properties
to properly remove an existing operation.</simpara>
</note>
<simpara>To change the values of a monitoring option,
you can update the resource.
For example, you can
create a <literal>VirtualIP</literal> with the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</literal></literallayout>
<simpara>By default, this command creates these operations.</simpara>
<literallayout class="monospaced">Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
            stop interval=0s timeout=20s (VirtualIP-stop-timeout-20s)
            monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)</literallayout>
<simpara>To change the stop timeout operation, execute the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource update VirtualIP op stop interval=0s timeout=40s</literal>

# <literal>pcs resource show VirtualIP</literal>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
  Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
              monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)
              stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</literallayout>
</section>
<section xml:id="proc_configuring-global-resource-operation-defaults-resource-monitoring-operations">
<title>Configuring global resource operation defaults</title>
<simpara>As of Red Hat Enterprise Linux 8.3, you can change the default value
of a resource operation for all resources with the
<literal>pcs resource op defaults update</literal> command.
The following command sets a global
default of a <literal>timeout</literal> value of 240 seconds for all
monitoring operations.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults update timeout=240s</literal></literallayout>
<simpara>The original <literal>pcs resource op defaults <emphasis>name</emphasis>=<emphasis>value</emphasis></literal> command, which set resource
operation defaults for all resources in
previous RHEL 8 releases, remains supported unless there is more than one set of defaults configured. However,
<literal>pcs resource op defaults update</literal> is now the preferred version of the command.</simpara>
<section xml:id="overriding_resource_specific_operation_values" remap="_overriding_resource_specific_operation_values">
<title>Overriding resource-specific operation values</title>
<simpara>Note that a cluster resource will
use the global default only when the option is not specified in the
cluster resource definition. By default, resource agents define the
<literal>timeout</literal> option for all operations. For the
global operation timeout value to be honored, you must create the
cluster resource without the <literal>timeout</literal> option explicitly
or you must remove the <literal>timeout</literal> option by updating the
cluster resource, as in the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource update VirtualIP op monitor interval=10s</literal></literallayout>
<simpara>For example, after setting a global
default of a <literal>timeout</literal> value of 240 seconds for all
monitoring operations and updating the cluster resource <literal>VirtualIP</literal>
to remove the timeout value for the <literal>monitor</literal> operation, the
resource <literal>VirtualIP</literal> will then have timeout
values for <literal>start</literal>, <literal>stop</literal>,
and <literal>monitor</literal> operations of 20s, 40s and 240s, respectively.
The global default value for timeout operations
is applied here only on the <literal>monitor</literal> operation, where
the default <literal>timeout</literal> option was removed by the previous command.</simpara>
<literallayout class="monospaced"># <literal>pcs resource show VirtualIP</literal>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
   Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
   Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
               monitor interval=10s (VirtualIP-monitor-interval-10s)
               stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</literallayout>
</section>
<section xml:id="changing_the_default_value_of_a_resource_operation_for_sets_of_resources_rhel_8_3_and_later" remap="_changing_the_default_value_of_a_resource_operation_for_sets_of_resources_rhel_8_3_and_later">
<title>Changing the default value of a resource operation for sets of resources (RHEL 8.3 and later)</title>
<simpara>As of Red Hat Enterprise Linux 8.3, you can
create multiple sets of resource operation defaults with the
<literal>pcs resource op defaults set create</literal> command, which allows you
to specify a rule that contains <literal>resource</literal> and operation expressions.
In RHEL 8.3, only <literal>resource</literal> and operation expressions, including <literal>and</literal>, <literal>or</literal> and parentheses, are allowed in rules that
you specify with this command. In RHEL 8.4 and later, all of the other
rule expressions supported by Pacemaker are allowed as well.</simpara>
<simpara>With this comand, you can configure a default resource operation value
for all resources of a particular type.
For example, it is now possible to configure implicit <literal>podman</literal> resources created
by Pacemaker when bundles are in use.</simpara>
<simpara>The following command sets a default timeout value of 90s for all operations for all
<literal>podman</literal> resources.  In this example, <literal>::podman</literal> means a resource of any class,
any provider, of type <literal>podman</literal>.</simpara>
<simpara>The <literal>id</literal> option, which names the set of resource
operation defaults, is not mandatory. If you do not set
this option, <literal>pcs</literal> will generate an ID automatically. Setting this value allows you to provide
a more descriptive name.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults set create id=podman-timeout meta timeout=90s rule resource ::podman</literal></literallayout>
<simpara>The following command sets a default timeout value of 120s for the <literal>stop</literal> operation for all
resources.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults set create id=stop-timeout meta timeout=120s rule op stop</literal></literallayout>
<simpara>It is possible to set the default timeout value for a specific operation for
all resources of a particular type.
The following example sets a default timeout value of 120s for the <literal>stop</literal> operation for
all <literal>podman</literal> resources.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults set create id=podman-stop-timeout meta timeout=120s rule resource ::podman and op stop</literal></literallayout>
</section>
<section xml:id="displaying_currently_configured_resource_operation_default_values" remap="_displaying_currently_configured_resource_operation_default_values">
<title>Displaying currently configured resource operation default values</title>
<simpara>The <literal>pcs resource op defaults</literal> command
displays a list of
currently configured default values for resource operations,
including any rules you specified.</simpara>
<simpara>The following command displays the default
operation values for a cluster which has been configured with a
default timeout value of 90s for all operations for all <literal>podman</literal> resources,
and for which an ID for the set of resource operation defaults has been set
as <literal>podman-timeout</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults</literal>
Meta Attrs: podman-timeout
  timeout=90s
  Rule: boolean-op=and score=INFINITY
    Expression: resource ::podman</literallayout>
<simpara>The following command displays the default
operation values for a cluster which has been configured with a
default timeout value of 120s for the <literal>stop</literal> operation for all <literal>podman</literal> resources,
and for which an ID for the set of resource operation defaults has been set
as <literal>podman-stop-timeout</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs resource op defaults</literal>
Meta Attrs: podman-stop-timeout
  timeout=120s
  Rule: boolean-op=and score=INFINITY
    Expression: resource ::podman
    Expression: op stop</literallayout>
</section>
</section>
<section xml:id="proc_configuring-multiple-monitoring-operations-resource-monitoring-operations">
<title>Configuring multiple monitoring operations</title>
<simpara>You can configure a single resource with as many monitor
operations as a resource agent supports.
In this way you can do
a superficial health check every minute and progressively more intense ones at higher intervals.</simpara>
<note>
<simpara>When configuring multiple monitor operations, you
must ensure that no two operations are performed at the same
interval.</simpara>
</note>
<simpara>To configure additional monitoring operations for a resource that supports more
in-depth checks at different levels, you add an
<literal>OCF_CHECK_LEVEL=<emphasis>n</emphasis></literal> option.</simpara>
<simpara>For example, if you configure the following <literal>IPaddr2</literal> resource,
by default this creates a monitoring operation with an interval of
10 seconds and a timeout value of 20 seconds.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</literal></literallayout>
<simpara>If the Virtual IP supports a different check with a depth of 10, the following
command causes Pacemaker to perform the more advanced monitoring check
every 60 seconds in addition to the normal Virtual IP check
every 10 seconds. (As noted, you should not configure the
additional monitoring operation with a 10-second interval as well.)</simpara>
<literallayout class="monospaced"># <literal>pcs resource op add VirtualIP monitor interval=60s OCF_CHECK_LEVEL=10</literal></literallayout>
</section>
</chapter>
<chapter xml:id="assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters">
<title>Pacemaker cluster properties</title>
<simpara>Cluster properties control how the cluster behaves
when confronted with situations that may occur
during cluster operation.</simpara>
<section xml:id="ref_cluster-properties-options-controlling-cluster-behavior">
<title>Summary of cluster properties and options</title>
<simpara><xref linkend="tb-clusterprops-HAAR"/>
summaries the Pacemaker cluster properties,
showing the default values of the properties
and the possible values you can set for those properties.</simpara>
<simpara>There are additional cluster properties that determine fencing
behavior.  For information on these properties, see
<link linkend="ref_advanced-fence-device-properties-configuring-fencing">Advanced fencing configuration options</link>.</simpara>
<note>
<simpara>In addition to the properties described in this table,
there are additional cluster properties that are exposed by the cluster
software. For these properties, it is recommended that you not
change their values from their defaults.</simpara>
</note>
<table xml:id="tb-clusterprops-HAAR" frame="all" rowsep="1" colsep="1">
<title>Cluster Properties</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="29*"/>
<colspec colname="col_3" colwidth="43*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>batch-limit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>0</simpara></entry>
<entry align="left" valign="top"><simpara>The number of resource actions that the cluster is allowed to execute in parallel. The
"correct" value will depend on the speed and load of your network and
cluster nodes. The default value of 0 means that the cluster will dynamically
impose a limit when any node has a high CPU load.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>migration-limit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>-1 (unlimited)</simpara></entry>
<entry align="left" valign="top"><simpara>The number of migration jobs that the cluster is allowed to execute in
parallel on a node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>no-quorum-policy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>stop</simpara></entry>
<entry align="left" valign="top"><simpara>What to do when the cluster does not have quorum. Allowed values:</simpara><simpara>* ignore - continue all resource management</simpara><simpara>* freeze - continue resource management, but do not
recover resources from nodes not in the affected partition</simpara><simpara>* stop - stop all resources in the affected cluster partition</simpara><simpara>* suicide - fence all nodes in the affected cluster partition</simpara><simpara>* demote - if a cluster partition loses quorum, demote any promoted resources and stop
all other resources</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>symmetric-cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether resources can run on any node by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-delay</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Round trip delay over the network (excluding action execution). The
"correct" value will depend on the speed and load of your network and
cluster nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stop-orphan-resources</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether deleted resources should be stopped.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stop-orphan-actions</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether deleted actions should be canceled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>start-failure-is-fatal</literal></simpara></entry>
<entry align="left" valign="top"><simpara>true</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether a failure to start a resource on a particular
node prevents further start attempts on that node.
When set to <literal>false</literal>, the cluster will
decide whether to try starting on the same node again based on the
resource’s current failure count and migration threshold.
For information on setting the <literal>migration-threshold</literal>
option for a resource, see
<link linkend="proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</link>.</simpara><simpara>Setting <literal>start-failure-is-fatal</literal> to <literal>false</literal> incurs the
risk that this will allow one faulty node that is unable to
start a resource to hold
up all dependent actions.
This is why <literal>start-failure-is-fatal</literal> defaults to true.
The risk of setting <literal>start-failure-is-fatal=false</literal> can be mitigated
by setting a low migration threshold so that other actions can proceed after
that many failures.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pe-error-series-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>-1 (all)</simpara></entry>
<entry align="left" valign="top"><simpara>The number of scheduler inputs resulting in ERRORs to save. Used when reporting problems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pe-warn-series-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>-1 (all)</simpara></entry>
<entry align="left" valign="top"><simpara>The number of scheduler inputs resulting in WARNINGs to save. Used when reporting problems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pe-input-series-max</literal></simpara></entry>
<entry align="left" valign="top"><simpara>-1 (all)</simpara></entry>
<entry align="left" valign="top"><simpara>The number of "normal" scheduler inputs to save. Used when reporting problems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-infrastructure</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>The messaging stack on which Pacemaker is currently running. Used for informational and
diagnostic purposes; not user-configurable.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>dc-version</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>Version of Pacemaker on the cluster’s Designated Controller (DC). Used for diagnostic purposes;
not user-configurable.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-recheck-interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>15 minutes</simpara></entry>
<entry align="left" valign="top"><simpara>Polling interval for time-based changes to options, resource parameters and constraints.
Allowed values: Zero disables polling, positive values are an
interval in seconds (unless other SI units are specified, such as 5min). Note that this value
is the maximum time between checks; if a cluster event occurs sooner than
the time specified by this value, the check will be done sooner.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>maintenance-mode</literal></simpara></entry>
<entry align="left" valign="top"><simpara>false</simpara></entry>
<entry align="left" valign="top"><simpara>Maintenance Mode tells the cluster to go to a "hands off" mode, and not start or stop
any services until told otherwise. When maintenance mode is completed,
the cluster does a sanity check of the current state of any services,
and then stops or starts any that need it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>shutdown-escalation</literal></simpara></entry>
<entry align="left" valign="top"><simpara>20min</simpara></entry>
<entry align="left" valign="top"><simpara>The time after which to give up trying to shut down gracefully
and just exit.
Advanced use only.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>stop-all-resources</literal></simpara></entry>
<entry align="left" valign="top"><simpara>false</simpara></entry>
<entry align="left" valign="top"><simpara>Should the cluster stop all resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>enable-acl</literal></simpara></entry>
<entry align="left" valign="top"><simpara>false</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether the cluster can use access control lists, as set
with the <literal role="command">pcs acl</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>placement-strategy</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>default</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates whether and how the cluster will take utilization attributes into account
when determining resource placement on cluster nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>priority-fencing-delay</literal></simpara></entry>
<entry align="left" valign="top"><simpara>0 (disabled)</simpara></entry>
<entry align="left" valign="top"><simpara>(RHEL 8.3 and later) Allows you to configure a two-node cluster so that in a split-brain situation the node with the fewest resources running is the node that gets fenced.</simpara><simpara>The <literal>priority-fencing-delay</literal> property can be set to a time duration. The default value for this property is 0 (disabled). If this property is set to a non-zero value, and the <literal>priority</literal> meta-attribute is configured for at least one resource, then in a split-brain situation the node with the highest combined priority of all resources running on it will be more likely to survive.</simpara><simpara>For example, if you set <literal>pcs resource defaults priority=1</literal> and <literal>pcs property set priority-fencing-delay=15s</literal> and no other priorities are set, then the node running the most resources will be more likely to survive because the other node will wait 15 seconds before initiating fencing. If a particular resource is more important than the rest, you can give it a higher priority.</simpara><simpara>The node running the master role of a promotable clone gets an extra 1 point if a priority has been configured for that clone.</simpara><simpara>Any delay set with the <literal>priority-fencing-delay</literal> property will be added to any delay from the <literal>pcmk_delay_base</literal> and <literal>pcmk_delay_max</literal> fence device properties. This behavior allows some delay when both nodes have equal priority, or both nodes need to be fenced for some reason other than node loss (for example, if <literal>on-fail=fencing</literal> is set for a resource monitor operation). If used in combination, it is recommended that you set the <literal>priority-fencing-delay</literal> property to a value that is significantly greater than the maximum delay from <literal>pcmk_delay_base</literal> and <literal>pcmk_delay_max</literal>, to be sure the prioritized node is preferred (twice the value would be completely safe).</simpara><simpara>Only fencing scheduled by Pacemaker itself will observe <literal>priority-fencing-delay</literal>.
Fencing scheduled by external code such as <literal>dlm_controld</literal> will not provide the
necessary information to the fence device.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="setting-cluster-properties-controlling-cluster-behavior">
<title>Setting and removing cluster properties</title>
<simpara>To set the value of a cluster property, use the
following <emphasis role="strong"><phrase role="application">pcs</phrase></emphasis> command.</simpara>
<literallayout class="monospaced">pcs property set <emphasis>property</emphasis>=<emphasis>value</emphasis></literallayout>
<simpara>For example, to set the value of <literal>symmetric-cluster</literal> to
<literal>false</literal>, use the following command.</simpara>
<literallayout class="monospaced"># <literal>pcs property set symmetric-cluster=false</literal></literallayout>
<simpara>You can remove a cluster property from
the configuration with the following command.</simpara>
<literallayout class="monospaced">pcs property unset <emphasis>property</emphasis></literallayout>
<simpara>Alternately, you can remove a cluster property from a configuration by leaving the
value field of the <literal role="command">pcs property set</literal> command
blank. This restores that property to its default
value. For example, if you have previously set the
<literal>symmetric-cluster</literal> property to <literal>false</literal>,
the following command removes the value you have set
from the configuration and restores the value of
<literal>symmetric-cluster</literal> to <literal>true</literal>,
which is its default value.</simpara>
<literallayout class="monospaced"># <literal>pcs property set symmetic-cluster=</literal></literallayout>
</section>
<section xml:id="proc_querying-cluster-property-settings-controlling-cluster-behavior">
<title>Querying cluster property settings</title>
<simpara>In most cases, when you use the <literal role="command">pcs</literal> command to
display values of the various cluster components, you can
use <literal role="command">pcs list</literal> or <literal role="command">pcs show</literal>
interchangeably. In the following examples, <literal role="command">pcs list</literal>
is the format used to display an entire list of all settings for
more than one property, while <literal role="command">pcs show</literal> is
the format used to display the values of a specific property.</simpara>
<simpara>To display
the values of the property settings that
have been set for the cluster,
use the following <emphasis role="strong"><phrase role="application">pcs</phrase></emphasis> command.</simpara>
<literallayout class="monospaced">pcs property list</literallayout>
<simpara>To display all of the values of
the property settings for the cluster,
including the default values of the
property settings that have not been
explicitly set, use the following command.</simpara>
<literallayout class="monospaced">pcs property list --all</literallayout>
<simpara>To display the current value of a specific
cluster property, use the following command.</simpara>
<literallayout class="monospaced">pcs property show <emphasis>property</emphasis></literallayout>
<simpara>For example, to display the current value of the <literal>cluster-infrastructure</literal>
property,
execute the following command:</simpara>
<literallayout class="monospaced"># <literal>pcs property show cluster-infrastructure</literal>
Cluster Properties:
 cluster-infrastructure: cman</literallayout>
<simpara>For informational purposes, you can display a list of all of the default values for
the properties, whether they have been set to a value other
than the default or not, by using the following command.</simpara>
<literallayout class="monospaced">pcs property [list|show] --defaults</literallayout>
</section>
</chapter>
<chapter xml:id="assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters">
<title>Configuring resources to remain stopped on clean node shutdown (RHEL 8.2 and later)</title>
<simpara>When a cluster node shuts down, Pacemaker’s default response is to stop all
resources running on that node and recover them elsewhere, even if the shutdown
is a clean shutdown. As of RHEL 8.2, you can configure Pacemaker so that
when a node shuts down cleanly, the resources attached to the node
will be locked to the node and unable to start elsewhere until
they start again when the node that has shut down rejoins the cluster.
This allows you to power down nodes during maintenance windows when
service outages are acceptable without causing that node’s resources
to fail over to other nodes in the cluster.</simpara>
<section xml:id="ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped">
<title>Cluster properties to configure resources to remain stopped on clean node shutdown</title>
<simpara>The ability to prevent resources from failing over on a clean node shutdown
is implemented by means of the following cluster properties.</simpara>
<variablelist>
<varlistentry>
<term><literal>shutdown-lock</literal></term>
<listitem>
<simpara>When this cluster property is set to the default value of <literal>false</literal>, the cluster will
recover resources that are active on nodes being cleanly shut down.
When this property is set to <literal>true</literal>, resources that are active on the nodes
being cleanly shut down are unable to start elsewhere
until they start on the node again after it rejoins the cluster.</simpara>
<simpara>The <literal>shutdown-lock</literal> property will work for either cluster nodes or remote nodes,
but not guest nodes.</simpara>
<simpara>If <literal>shutdown-lock</literal> is set to <literal>true</literal>, you can remove the lock on one cluster resource
when a node is down so that the resource can start elsewhere by performing a manual
refresh on the node with the following command.</simpara>
<simpara><literal role="command">pcs resource refresh <emphasis>resource</emphasis> node=<emphasis>nodename</emphasis></literal></simpara>
<simpara>Note that once the resources are unlocked, the cluster is free
to move the resources elsewhere. You can control the likelihood of
this occurring by using stickiness values or location preferences for
the resource.</simpara>
<note>
<simpara>A manual refresh will work with remote nodes only if you first run the following commands:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Run the <literal role="command">systemctl stop pacemaker_remote</literal> command on the remote node to stop the node.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal role="command">pcs resource disable <emphasis>remote-connection-resource</emphasis></literal> command.</simpara>
</listitem>
</orderedlist>
<simpara>You can then perform a manual refresh on the remote node.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>shutdown-lock-limit</literal></term>
<listitem>
<simpara>When this cluster property is set to a time other than the default value of 0,
resources will be available for recovery on other nodes if the
node does not rejoin within the specified time since the shutdown was initiated.</simpara>
<note>
<simpara>The <literal>shutdown-lock-limit</literal> property will work with remote nodes
only if you first run the following commands:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Run the <literal role="command">systemctl stop pacemaker_remote</literal> command on the remote
node to stop the node.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal role="command">pcs resource disable <emphasis>remote-connection-resource</emphasis></literal> command.</simpara>
</listitem>
</orderedlist>
<simpara>After you run these commands, the resources that had been running on the
remote node will be available for recovery on
other nodes when the amount of time specified as the <literal>shutdown-lock-limit</literal> has passed.</simpara>
</note>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="proc_setting-shutdown-lock-configuring-resources-to-remain-stopped">
<title>Setting the shutdown-lock cluster property</title>
<simpara>The following example sets the <literal>shutdown-lock</literal> cluster property to <literal>true</literal> in an example
cluster and shows the effect this has when the node is shut down and started again.
This example cluster consists of three nodes: <literal>z1.example.com</literal>, <literal>z2.example.com</literal>, and
<literal>z3.example.com</literal>.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>shutdown-lock</literal> property to to <literal>true</literal> and verify its value. In this example
the <literal>shutdown-lock-limit</literal> property maintains its default value of 0.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs property set shutdown-lock=true</literal>
[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs property list --all | grep shutdown-lock</literal>
 shutdown-lock: true
 shutdown-lock-limit: 0</literallayout>
</listitem>
<listitem>
<simpara>Check the status of the cluster. In this example, resources <literal>third</literal> and <literal>fifth</literal>
are running on <literal>z1.example.com</literal>.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs status</literal>
...
Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z2.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com
...</literallayout>
</listitem>
<listitem>
<simpara>Shut down <literal>z1.example.com</literal>, which will stop the resources that are running on
that node.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~] <literal># pcs cluster stop z1.example.com</literal>
Stopping Cluster (pacemaker)...
Stopping Cluster (corosync)...</literallayout>
</listitem>
<listitem>
<simpara>Running the <literal>pcs status</literal> command shows
that node <literal>z1.example.com</literal> is offline and that
the resources that had been running on <literal>z1.example.com</literal>
are <literal>LOCKED</literal> while the node is down.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs status</literal>
...

Node List:
 * Online: [ z2.example.com z3.example.com ]
 * OFFLINE: [ z1.example.com ]

Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)

...</literallayout>
</listitem>
<listitem>
<simpara>Start cluster services again on <literal>z1.example.com</literal> so that it rejoins the cluster.
Locked resources should get started on that node, although once they start they will not
not necessarily remain on the same node.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs cluster start z1.example.com</literal>
Starting Cluster...</literallayout>
</listitem>
<listitem>
<simpara>In this example, resouces <literal>third</literal> and <literal>fifth</literal> are recovered on
node <literal>z1.example.com</literal>.</simpara>
<literallayout class="monospaced">[<link xlink:href="mailto:root@z3.example.com">root@z3.example.com</link> ~]# <literal>pcs status</literal>
...

Node List:
 * Online: [ z1.example.com z2.example.com z3.example.com ]

Full List of Resources:
..
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com

...</literallayout>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters">
<title>Configuring a node placement strategy</title>
<simpara>Pacemaker decides where to place a resource according to the resource allocation scores
on every node. The resource will be allocated to the node where the resource has the highest score.
This allocation score is derived from a combination of factors, including resource
constraints, <literal>resource-stickiness</literal> settings, prior failure history
of a resource on each node, and utilization of each node.</simpara>
<simpara>If the resource allocation scores on all the nodes are equal,
by the default placement strategy Pacemaker will choose a node with
the least number of allocated resources for balancing the load.
If the number of resources on each node is equal,
the first eligible node listed in the CIB will be chosen to run the resource.</simpara>
<simpara>Often, however, different resources use significantly different proportions
of a node’s capacities (such
as memory or I/O). You cannot always balance the load ideally
by taking into account only the number of resources allocated
to a node. In addition, if resources are placed such that their
combined requirements exceed the provided capacity, they may fail to
start completely or they may run run with degraded performance.
To take these factors into account, Pacemaker allows you to configure the following
components:</simpara>
<itemizedlist>
<listitem>
<simpara>the capacity a particular node provides</simpara>
</listitem>
<listitem>
<simpara>the capacity a particular resource requires</simpara>
</listitem>
<listitem>
<simpara>an overall strategy for placement of resources</simpara>
</listitem>
</itemizedlist>
<section xml:id="configuring-utilization-attributes-configuring-node-placement-strategy">
<title>Utilization attributes and placement strategy</title>
<simpara>To configure the capacity that a node provides or a resource requires, you
can use <emphasis>utilization attributes</emphasis> for nodes and resources.
You do this by setting a utilization variable for a resource and assigning
a value to that variable to indicate what the resource requires,
and then setting that same utilization variable for a node and assigning
a value to that variable to indicate what that node provides.</simpara>
<simpara>You can name utilization attributes according to your preferences
and define as many name and value pairs as your configuration needs.
The values of utilization attributes must be integers.</simpara>
<section xml:id="configuring_node_and_resource_capacity" remap="_configuring_node_and_resource_capacity">
<title>Configuring node and resource capacity</title>
<simpara>The following example configures a utilization attribute of
CPU capacity for two nodes, setting this attribute as the variable <literal>cpu</literal>.
It also configures a utilization attribute of RAM capacity,
setting this attribute as the variable <literal>memory</literal>.
In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>Node 1 is defined as providing a CPU capacity of two and a RAM capacity of 2048</simpara>
</listitem>
<listitem>
<simpara>Node 2 is defined as providing a CPU capacity of four and a RAM capacity of 2048</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced"># <literal>pcs node utilization node1 cpu=2 memory=2048</literal>
# <literal>pcs node utilization node2 cpu=4 memory=2048</literal></literallayout>
<simpara>The following example specifies the same utilization attributes that
three different resources require. In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>resource <literal>dummy-small</literal> requires a CPU capacity of 1 and a RAM capacity of 1024</simpara>
</listitem>
<listitem>
<simpara>resource <literal>dummy-medium</literal> requires a CPU capacity of 2 and a RAM capacity of 2048</simpara>
</listitem>
<listitem>
<simpara>resource <literal>dummy-large</literal> requires a CPU capacity of 1 and a RAM capacity of 3072</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced"># <literal>pcs resource utilization dummy-small cpu=1 memory=1024</literal>
# <literal>pcs resource utilization dummy-medium cpu=2 memory=2048</literal>
# <literal>pcs resource utilization dummy-large cpu=3 memory=3072</literal></literallayout>
<simpara>A node is considered eligible for a resource if it has sufficient
free capacity to satisfy the resource’s requirements, as defined by
the utilization attributes.</simpara>
</section>
<section xml:id="configuring_placement_strategy" remap="_configuring_placement_strategy">
<title>Configuring placement strategy</title>
<simpara>After you have configured the capacities your nodes provide
and the capacities your resources require, you need to set
the <literal>placement-strategy</literal> cluster property,
otherwise the capacity configurations have no effect.</simpara>
<simpara>Four values are available for the <literal>placement-strategy</literal> cluster property:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>default</literal> — Utilization values are not taken into account at all. Resources
are allocated according to allocation scores. If scores are equal,
resources are evenly distributed across nodes.</simpara>
</listitem>
<listitem>
<simpara><literal>utilization</literal> — Utilization values are taken into account only when deciding whether a
node is considered eligible (that is, whether it has sufficient free capacity
to satisfy the resource’s requirements). Load-balancing is still
done based on the number of resources allocated to a node.</simpara>
</listitem>
<listitem>
<simpara><literal>balanced</literal> — Utilization values are taken into account when deciding
whether a node is eligible to serve a resource and when load-balancing,
so an attempt is made to spread the resources in a way
that optimizes resource performance.</simpara>
</listitem>
<listitem>
<simpara><literal>minimal</literal> — Utilization values are taken into account only when deciding whether a node
is eligible to serve a resource. For load-balancing, an attempt is made
to concentrate the resources on as few nodes as possible,
thereby enabling possible power savings on the remaining nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>The following example command sets the value of <literal>placement-strategy</literal> to
<literal>balanced</literal>. After running this command,
Pacemaker will ensure the load from your resources will be distributed
evenly throughout the cluster, without the need for complicated sets of colocation constraints.</simpara>
<literallayout class="monospaced"># <literal>pcs property set placement-strategy=balanced</literal></literallayout>
</section>
</section>
<section xml:id="pacemaker-resource-allocation-configuring-node-placement-strategy">
<title>Pacemaker resource allocation</title>
<simpara>The following subsections summarize how Pacemaker allocates resources.</simpara>
<section xml:id="node_preference" remap="_node_preference">
<title>Node Preference</title>
<simpara>Pacemaker determines which node is preferred when allocating resources according
to the following strategy.</simpara>
<itemizedlist>
<listitem>
<simpara>The node with the highest node weight gets consumed first.
Node weight is a score maintained by the cluster to represent node health.</simpara>
</listitem>
<listitem>
<simpara>If multiple nodes have the same node weight:</simpara>
<itemizedlist>
<listitem>
<simpara>If the <literal>placement-strategy</literal> cluster property
is <literal>default</literal> or <literal>utilization</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>The node that has the least number
of allocated resources gets consumed first.</simpara>
</listitem>
<listitem>
<simpara>If the numbers of allocated resources are equal, the
first eligible node listed in the CIB gets consumed first.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If the <literal>placement-strategy</literal> cluster property
is <literal>balanced</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>The node that has the most
free capacity gets consumed first.</simpara>
</listitem>
<listitem>
<simpara>If the free capacities of the nodes are equal, the node that has the least
number of allocated resources gets consumed first.</simpara>
</listitem>
<listitem>
<simpara>If the free capacities of the nodes are equal and the number of allocated resources is equal,
the first eligible
node listed in the CIB gets consumed first.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If the <literal>placement-strategy</literal> cluster property
is <literal>minimal</literal>, the first eligible node listed in the CIB gets consumed first.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="node_capacity" remap="_node_capacity">
<title>Node Capacity</title>
<simpara>Pacemaker determines which node has the most
free capacity according
to the following strategy.</simpara>
<itemizedlist>
<listitem>
<simpara>If only one type of utilization attribute has been defined, free
capacity is a simple numeric comparison.</simpara>
</listitem>
<listitem>
<simpara>If multiple types of utilization attributes have been defined, then the
node that is numerically highest in the most attribute types has
the most free capacity. For example:</simpara>
<itemizedlist>
<listitem>
<simpara>If NodeA has more free CPUs, and NodeB has more free memory, then their free capacities are equal.</simpara>
</listitem>
<listitem>
<simpara>If NodeA has more free CPUs, while NodeB has more free memory and storage, then NodeB has more free capacity.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="resource_allocation_preference" remap="_resource_allocation_preference">
<title>Resource Allocation Preference</title>
<simpara>Pacemaker determines which resource is allocated first according
to the following strategy.</simpara>
<itemizedlist>
<listitem>
<simpara>The resource that has the highest priority gets allocated first. You can set a resource’s priority
when you create the resource.</simpara>
</listitem>
<listitem>
<simpara>If the priorities of the resources are equal,
the resource that has the highest score on the node where it
is running gets allocated first, to prevent resource shuffling.</simpara>
</listitem>
<listitem>
<simpara>If the resource scores on the nodes where the resources are running
are equal or the resources are not running,
the resource that has the highest score on the preferred node gets allocated first.
If the resource scores on the preferred node are equal in this case, the first
runnable resource listed in
the CIB gets allocated first.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="resource-placement-strategy-guidelines-configuring-node-placement-strategy">
<title>Resource placement strategy guidelines</title>
<simpara>To ensure that Pacemaker’s placement strategy for resources
works most effectively, you should
take the following considerations into account when configuring your system.</simpara>
<itemizedlist>
<listitem>
<simpara>Make sure that you have sufficient physical capacity.</simpara>
<simpara>If the physical capacity of your nodes is being used to near
maximum under normal conditions, then problems could occur
during failover. Even without the utilization feature,
you may start to experience timeouts and secondary failures.</simpara>
</listitem>
<listitem>
<simpara>Build some buffer into the capabilities you configure for the nodes.</simpara>
<simpara>Advertise slightly more node resources than you physically have, on the
assumption the that a Pacemaker resource will not use 100% of the configured
amount of CPU, memory, and so forth all the time.
This practice is sometimes called overcommit.</simpara>
</listitem>
<listitem>
<simpara>Specify resource priorities.</simpara>
<simpara>If the cluster is going to sacrifice services, it should be the ones
you care about least. Ensure that resource priorities are properly
set so that your most important resources are scheduled first.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="node-utilization-resource-agent-configuring-node-placement-strategy">
<title>The NodeUtilization resource agent</title>
<simpara>The NodeUtilization agent can detect the system parameters of available CPU, host
memory availability, and hypervisor memory availability and add these parameters into the CIB.
You can run the agent as a clone resource to have it automatically populate
these parameters on each node.</simpara>
<simpara>For information on the <literal>NodeUtilization</literal> resource agent and the resource options
for this agent, run the <literal role="command">pcs resource describe NodeUtilization</literal> command.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters">
<title>Configuring a virtual domain as a resource</title>
<simpara>You can configure a virtual domain that is managed by
the <literal>libvirt</literal> virtualization framework
as a cluster resource
with the <literal role="command">pcs resource create</literal> command,
specifying <literal>VirtualDomain</literal> as the resource type.</simpara>
<simpara>When configuring a virtual domain as a resource, take
the following considerations into account:</simpara>
<itemizedlist>
<listitem>
<simpara>A virtual domain should be stopped before you configure it as a cluster resource.</simpara>
</listitem>
<listitem>
<simpara>Once a virtual domain is a cluster resource, it should not be started, stopped,
or migrated except through the cluster tools.</simpara>
</listitem>
<listitem>
<simpara>Do not configure a virtual domain that you have configured as a cluster resource to
start when its host boots.</simpara>
</listitem>
<listitem>
<simpara>All nodes allowed to run a virtual domain
must have access to the necessary configuration files and storage
devices for that virtual domain.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want the cluster to manage services within the virtual domain itself,
you can configure the virtual domain as a guest node.</simpara>
<section xml:id="ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource">
<title>Virtual domain resource options</title>
<simpara><xref linkend="tb-virtdomain-options-HAAR"/>
describes
the resource options you can configure for a <literal>VirtualDomain</literal> resource.</simpara>
<table xml:id="tb-virtdomain-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Resource Options for Virtual Domain Resources</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="38*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="38*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>config</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>(required)
Absolute path to the <literal>libvirt</literal> configuration file for this virtual domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>hypervisor</literal></simpara></entry>
<entry align="left" valign="top"><simpara>System dependent</simpara></entry>
<entry align="left" valign="top"><simpara>Hypervisor URI to connect to. You can determine the
system’s default URI by running the <literal role="command">virsh --quiet uri</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>force_stop</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Always forcefully shut down ("destroy") the domain on stop. The default behavior
is to resort to a forceful shutdown only after a graceful shutdown attempt has failed.
You should set this to <literal>true</literal> only if your
virtual domain (or your virtualization back end) does not support graceful shutdown.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>migration_transport</literal></simpara></entry>
<entry align="left" valign="top"><simpara>System dependent</simpara></entry>
<entry align="left" valign="top"><simpara>Transport used to connect to the remote hypervisor while migrating.
If this parameter is omitted, the resource will use <literal>libvirt</literal>'s
default transport to connect to the remote hypervisor.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>migration_network_suffix</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>Use a dedicated migration network. The migration URI is composed by adding this
parameter’s value to the end of the node name. If the node name is
a fully qualified domain name (FQDN),
insert the suffix immediately prior to the first period (.) in the FQDN.
Ensure that this composed host name is locally resolvable and
the associated IP address is reachable through the favored network.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>monitor_scripts</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>To additionally monitor services within the virtual domain, add this parameter with a list of
scripts to monitor. <emphasis>Note</emphasis>: When monitor scripts are used,
the <literal>start</literal> and <literal>migrate_from</literal> operations will
complete only when all monitor scripts have completed successfully. Be sure
to set the timeout of these operations to accommodate this delay</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>autoset_utilization_cpu</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>true</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If set to <literal>true</literal>, the agent will detect the number of
<literal>domainU</literal>'s <literal>vCPU</literal>s from <literal>virsh</literal>, and
put it into the CPU utilization of the resource when the monitor is executed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>autoset_utilization_hv_memory</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>true</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If set it true, the agent will detect the number of
<literal>Max memory</literal> from <literal>virsh</literal>, and put it
into the <literal>hv_memory</literal> utilization of the source when the monitor is executed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>migrateport</literal></simpara></entry>
<entry align="left" valign="top"><simpara>random highport</simpara></entry>
<entry align="left" valign="top"><simpara>This port will be used in the <literal>qemu</literal> migrate URI.
If unset, the port will be a random highport.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>snapshot</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>Path to the snapshot directory where the virtual machine image will be stored.
When this parameter is set, the virtual machine’s RAM state will be saved to
a file in the snapshot directory when stopped. If on start a state file is present
for the domain, the domain will be restored to the same state it
was in right before it stopped last. This option is incompatible
with the <literal>force_stop</literal> option.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>In addition to the <literal>VirtualDomain</literal> resource options,
you can configure the <literal>allow-migrate</literal>
metadata option to allow live migration of the resource to
another node. When this option is set to <literal>true</literal>,
the resource can be migrated without loss of state. When
this option is set to <literal>false</literal>, which is
the default state, the virtual domain will be shut down
on the first node and then restarted on the second node when it is moved
from one node to the other.</simpara>
</section>
<section xml:id="proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource">
<title>Creating the virtual domain resource</title>
<simpara>Use the following procedure to create a <literal>VirtualDomain</literal> resource in a cluster for a virtual
machine you have previously created:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To create the <literal>VirtualDomain</literal> resource agent for the management
of the virtual machine, Pacemaker requires the virtual machine’s <literal>xml</literal> configuration
file to be dumped to a file on disk. For example, if you created
a virtual machine named <literal>guest1</literal>, dump the
<literal>xml</literal> file to a file somewhere on one of the cluster nodes that will be allowed
to run the guest. You can use a file name of your choosing;
this example uses <literal>/etc/pacemaker/guest1.xml</literal>.</simpara>
<literallayout class="monospaced"># <literal>virsh dumpxml guest1 &gt; /etc/pacemaker/guest1.xml</literal></literallayout>
</listitem>
<listitem>
<simpara>Copy the virtual machine’s <literal>xml</literal> configuration file to all of the other
cluster nodes that will be allowed to run the guest, in the same location on each node.</simpara>
</listitem>
<listitem>
<simpara>Ensure that all of the nodes allowed to run the virtual domain have access to
the necessary storage devices for that virtual domain.</simpara>
</listitem>
<listitem>
<simpara>Separately test that the virtual domain can start and stop on each node that will run the
virtual domain.</simpara>
</listitem>
<listitem>
<simpara>If it is running, shut down the guest node. Pacemaker will start the
node when it is configured in the cluster. The virtual machine should not
be configured to start automatically when the host boots.</simpara>
</listitem>
<listitem>
<simpara>Configure the <literal>VirtualDomain</literal> resource with the <literal role="command">pcs resource create</literal>
command. For example,
the following command configures a <literal>VirtualDomain</literal>
resource named <literal>VM</literal>. Since the <literal>allow-migrate</literal>
option is set to <literal>true</literal> a <literal>pcs move VM <emphasis>nodeX</emphasis></literal>
command would be done as a live migration.</simpara>
<simpara>In this example <literal>migration_transport</literal> is set to <literal>ssh</literal>. Note that for SSH migration
to work properly, keyless logging must work between nodes.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create VM VirtualDomain config=/etc/pacemaker/guest1.xml migration_transport=ssh meta allow-migrate=true</literal></literallayout>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters">
<title>Cluster quorum</title>
<simpara>A Red Hat Enterprise Linux High Availability Add-On cluster uses
the <literal>votequorum</literal> service, in conjunction
with fencing, to avoid split brain
situations. A number of votes is assigned to each system in
the cluster, and cluster operations are allowed to proceed only
when a majority of votes is present. The service must be loaded
into all nodes or none; if it is loaded into a subset of cluster
nodes, the results will be unpredictable.
For information on the configuration and operation of
the <literal>votequorum</literal> service, see
the <literal role="command">votequorum</literal>(5) man page.</simpara>
<section xml:id="ref_quorum-options-configuring-cluster-quorum">
<title>Configuring quorum options</title>
<simpara>There are some special features of quorum configuration that
you can set when you create a cluster with the <literal role="command">pcs cluster
setup</literal> command.
<xref linkend="tb-quorumoptions-HAAR"/>
summarizes these options.</simpara>
<table xml:id="tb-quorumoptions-HAAR" frame="all" rowsep="1" colsep="1">
<title>Quorum Options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="44*"/>
<colspec colname="col_2" colwidth="56*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>auto_tie_breaker</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When enabled, the cluster can suffer up to
50% of the nodes failing at the same time,
in a deterministic fashion.
The cluster partition,
or the set of nodes that are still in contact
with the <literal>nodeid</literal> configured in
<literal>auto_tie_breaker_node</literal>
(or lowest <literal>nodeid</literal> if not set),
will remain quorate. The other nodes will be inquorate.</simpara><simpara>The <literal role="option">auto_tie_breaker</literal> option is
principally used for clusters with an even number of nodes,
as it allows the cluster to continue operation with an even split. For more complex failures,
such as multiple, uneven splits, it is recommended that you use a quorum device,
as described in <link linkend="assembly_configuring-quorum-devices-configuring-cluster-quorum">Quorum devices</link>.</simpara><simpara>The <literal role="option">auto_tie_breaker</literal> option
is incompatible with quorum devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>wait_for_all</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When enabled, the cluster will be quorate for the first time
only after all nodes have been visible at least once at the
same time.</simpara><simpara>The <literal role="option">wait_for_all</literal> option is primarily used
for two-node clusters
and for even-node clusters using the quorum device <literal>lms</literal>
(last man standing) algorithm.</simpara><simpara>The <literal role="option">wait_for_all</literal> option is automatically enabled when a
cluster has two nodes, does not use a quorum device, and <literal role="option">auto_tie_breaker</literal>
is disabled. You can override this by explicitly setting <literal role="option">wait_for_all</literal>
to 0.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>last_man_standing</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When enabled, the cluster can dynamically
recalculate <literal>expected_votes</literal>
and quorum under specific circumstances.
You must enable <literal>wait_for_all</literal>
when you enable this option. The
<literal>last_man_standing</literal> option
is incompatible with quorum devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>last_man_standing_window</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The time, in milliseconds, to wait before
recalculating <literal>expected_votes</literal>
and quorum after a cluster loses nodes.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For further information about configuring and using
these options, see
the <literal role="command">votequorum</literal>(5) man page.</simpara>
</section>
<section xml:id="proc_modifying-quorum-options-configuring-cluster-quorum">
<title>Modifying quorum options</title>
<simpara>You can modify general quorum options for your cluster with the <literal role="command">pcs quorum update</literal>
command. Executing this command requires that the cluster be stopped. For information
on the quorum options, see the <literal role="command">votequorum</literal>(5) man page.</simpara>
<simpara>The format of the <literal role="command">pcs quorum update</literal> command
is as follows.</simpara>
<literallayout class="monospaced">pcs quorum update [auto_tie_breaker=[0|1]] [last_man_standing=[0|1]] [last_man_standing_window=[<emphasis>time-in-ms</emphasis>] [wait_for_all=[0|1]]</literallayout>
<simpara>The following series of commands modifies the <literal>wait_for_all</literal> quorum
option and displays the updated status of the option. Note that
the system does not allow you to execute this command while
the cluster is running.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum update wait_for_all=1</literal>
Checking corosync is not running on nodes...
Error: node1: corosync is running
Error: node2: corosync is running

[root@node1:~]# <literal>pcs cluster stop --all</literal>
node2: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
node2: Stopping Cluster (corosync)...

[root@node1:~]# <literal>pcs quorum update wait_for_all=1</literal>
Checking corosync is not running on nodes...
node2: corosync is not running
node1: corosync is not running
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded

[root@node1:~]# <literal>pcs quorum config</literal>
Options:
  wait_for_all: 1</literallayout>
</section>
<section xml:id="proc_displaying-quorum-configuration-status-configuring-cluster-quorum">
<title>Displaying quorum configuration and status</title>
<simpara>Once a cluster is running, you can enter the following cluster quorum
commands.</simpara>
<simpara>The following command shows the quorum configuration.</simpara>
<literallayout class="monospaced">pcs quorum [config]</literallayout>
<simpara>The following command shows the quorum runtime status.</simpara>
<literallayout class="monospaced">pcs quorum status</literallayout>
</section>
<section xml:id="proc_running-inquorate-clusters-configuring-cluster-quorum">
<title>Running inquorate clusters</title>
<simpara>If you take nodes out of a cluster for a long period of time and
the loss of those nodes would cause quorum loss, you can
change the value of the <literal>expected_votes</literal> parameter
for the live cluster with the <literal role="command">pcs quorum expected-votes</literal>
command. This allows the cluster to continue operation when it does
not have quorum.</simpara>
<warning>
<simpara>Changing the expected votes in a live cluster should be done with extreme
caution. If less than 50% of the cluster is running because you
have manually changed the expected votes, then the other nodes in the
cluster could be started separately and run cluster services, causing data
corruption and other unexpected results. If you change this value,
you should ensure that the <literal>wait_for_all</literal> parameter is
enabled.</simpara>
</warning>
<simpara>The following command sets the expected votes in the live cluster to
the specified value. This affects the live cluster only and does
not change the configuration file; the value of <literal>expected_votes</literal>
is reset to the value in the configuration file in the event of a reload.</simpara>
<literallayout class="monospaced">pcs quorum expected-votes <emphasis>votes</emphasis></literallayout>
<simpara>In a situation in which you know that the cluster is inquorate but you
want the cluster to proceed with resource management, you can use the
<literal role="command">pcs quorum unblock</literal>
command to prevent the cluster from waiting for all nodes
when establishing quorum.</simpara>
<note>
<simpara>This command should be used with extreme caution.
Before issuing this command, it is imperative that you
ensure that nodes that are not currently
in the cluster are switched off and have no access to shared resources.</simpara>
</note>
<literallayout class="monospaced"># <literal>pcs quorum unblock</literal></literallayout>
</section>
<section xml:id="assembly_configuring-quorum-devices-configuring-cluster-quorum">
<title>Quorum devices</title>
<simpara>You can allow a cluster to sustain more
node failures than standard quorum rules allows by configuring
a separate quorum device which acts as a third-party
arbitration device for the cluster.
A quorum device is recommended for clusters with an even number of nodes.
With two-node clusters, the use of a quorum device can better determine which
node survives in a split-brain situation.</simpara>
<simpara>You must take the following into account when configuring a quorum
device.</simpara>
<itemizedlist>
<listitem>
<simpara>It is recommended that a quorum device be run on a different physical network
at the same site as the cluster that uses the quorum device.
Ideally, the quorum device host should be in a separate rack than
the main cluster, or at least on a separate PSU and not on the
same network segment as the corosync ring or rings.</simpara>
</listitem>
<listitem>
<simpara>You cannot use
more than one quorum device in a cluster at the same
time.</simpara>
</listitem>
<listitem>
<simpara>Although you cannot use more than one quorum device in a
cluster at the same time, a single quorum device may be used by several
clusters at the same time.
Each cluster using
that quorum device can use different algorithms and quorum options,
as those are stored on the cluster nodes themselves. For example,
a single quorum device can be used by one cluster with an <literal>ffsplit</literal> (fifty/fifty split)
algorithm and by a second cluster with an <literal>lms</literal> (last man standing) algorithm.</simpara>
</listitem>
<listitem>
<simpara>A quorum device should not be run on an existing cluster node.</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_installing-quorum-device-packages-configuring-quorum-devices">
<title>Installing quorum device packages</title>
<simpara>Configuring a quorum device for a cluster requires that you install the following
packages:</simpara>
<itemizedlist>
<listitem>
<simpara>Install <literal>corosync-qdevice</literal> on the nodes of an existing cluster.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>yum install corosync-qdevice</literal>
[root@node2:~]# <literal>yum install corosync-qdevice</literal></literallayout>
</listitem>
<listitem>
<simpara>Install <literal>pcs</literal> and <literal>corosync-qnetd</literal> on the quorum device host.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>yum install pcs corosync-qnetd</literal></literallayout>
</listitem>
<listitem>
<simpara>Start the <literal>pcsd</literal> service and enable <literal>pcsd</literal>
at system start on the quorum device host.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>systemctl start pcsd.service</literal>
[root@qdevice:~]# <literal>systemctl enable pcsd.service</literal></literallayout>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_configuring-quorum-device-configuring-quorum-devices">
<title>Configuring a quorum device</title>
<simpara>The following procedure configures a quorum device and adds
it to the cluster.
In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>The node used for a quorum device is <literal>qdevice</literal>.</simpara>
</listitem>
<listitem>
<simpara>The quorum device model is <literal>net</literal>, which is currently the
only supported model. The <literal>net</literal> model supports the
following algorithms:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ffsplit</literal>: fifty-fifty split. This provides exactly one
vote to the partition with the highest number of active nodes.</simpara>
</listitem>
<listitem>
<simpara><literal>lms</literal>: last-man-standing. If the node is the only one left in the
cluster that can see the <literal>qnetd</literal> server, then it returns a vote.</simpara>
<warning>
<simpara>The LMS algorithm allows the cluster to remain quorate even
with only one remaining node,
but it also means that the voting power of
the quorum device is great since it is
the same as number_of_nodes - 1.
Losing connection with the quorum device
means losing number_of_nodes - 1 votes,
which means that only a cluster with all nodes active can
remain quorate (by overvoting the quorum
device); any other cluster becomes inquorate.</simpara>
</warning>
<simpara>For more detailed information on the implementation of these algorithms,
see the <literal>corosync-qdevice</literal>(8) man page.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The cluster nodes are <literal>node1</literal> and <literal>node2</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>The following procedure configures a quorum device and adds
that quorum device to a cluster.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On the node that you will use to host your quorum device, configure
the quorum device with the following command. This command
configures and starts the quorum device model <literal>net</literal> and configures the
device to start on boot.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>pcs qdevice setup model net --enable --start</literal>
Quorum device 'net' initialized
quorum device enabled
Starting quorum device...
quorum device started</literallayout>
<simpara>After configuring the quorum device, you can check its status. This should show that the
<literal>corosync-qnetd</literal> daemon is running and, at this point,
there are no clients connected to it. The <literal role="option">--full</literal> command option
provides detailed output.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>pcs qdevice status net --full</literal>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              0
Connected clusters:             0
Maximum send/receive size:      32768/32768 bytes</literallayout>
</listitem>
<listitem>
<simpara>Enable the ports on the firewall needed by the <literal>pcsd</literal> daemon
and the <literal>net</literal> quorum device by enabling the
<literal>high-availability</literal> service on <literal>firewalld</literal> with following commands.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>firewall-cmd --permanent --add-service=high-availability</literal>
[root@qdevice:~]# <literal>firewall-cmd --add-service=high-availability</literal></literallayout>
</listitem>
<listitem>
<simpara>From one of the nodes in the existing cluster, authenticate user <literal>hacluster</literal>
on the node that is hosting the quorum device. This allows <literal>pcs</literal> on the cluster
to connect to <literal>pcs</literal> on the <literal>qdevice</literal> host, but does not allow <literal>pcs</literal> on the <literal>qdevice</literal> host to
connect to <literal>pcs</literal> on the cluster.</simpara>
<literallayout class="monospaced">[root@node1:~] # <literal>pcs host auth qdevice</literal>
Username: hacluster
Password:
qdevice: Authorized</literallayout>
</listitem>
<listitem>
<simpara>Add the quorum device to the cluster.</simpara>
<simpara>Before adding the quorum device, you can check the current
configuration and status for the quorum device for later
comparison. The output for these commands indicates that the
cluster is not yet using a quorum device, and the <literal>Qdevice</literal>
membership status for each node is <literal>NR</literal> (Not Registered).</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum config</literal>
Options:</literallayout>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum status</literal>
Quorum information
------------------
Date:             Wed Jun 29 13:15:36 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           1
Flags:            2Node Quorate

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1         NR node1 (local)
         2          1         NR node2</literallayout>
<simpara>The following command adds the quorum device that you
have previously created to the cluster. You cannot use
more than one quorum device in a cluster at the same
time. However, one quorum device can be used by
several clusters at the same time.
This example command configures the quorum device to use
the <literal>ffsplit</literal> algorithm. For information on the configuration options for
the quorum device, see the <literal>corosync-qdevice</literal>(8) man
page.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum device add model net host=qdevice</literal> \
<literal>algorithm=ffsplit</literal>
Setting up qdevice certificates on nodes...
node2: Succeeded
node1: Succeeded
Enabling corosync-qdevice...
node1: corosync-qdevice enabled
node2: corosync-qdevice enabled
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Starting corosync-qdevice...
node1: corosync-qdevice started
node2: corosync-qdevice started</literallayout>
</listitem>
<listitem>
<simpara>Check the configuration status of the quorum device.</simpara>
<simpara>From the cluster side, you can execute the following commands to see
how the configuration has changed.</simpara>
<simpara>The <literal role="command">pcs quorum config</literal> shows the quorum device that
has been configured.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum config</literal>
Options:
Device:
  Model: net
    algorithm: ffsplit
    host: qdevice</literallayout>
<simpara>The <literal role="command">pcs quorum status</literal> command shows the quorum runtime
status, indicating that the quorum device is in use. The meanings of
of the <literal>Qdevice</literal> membership information status values for
each cluster node are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>A/NA</literal> — The quorum device is alive or not alive, indicating whether there is
a heartbeat between <literal>qdevice</literal> and <literal>corosync</literal>. This should always indicate that
the quorum device is alive.</simpara>
</listitem>
<listitem>
<simpara><literal>V/NV</literal> — <literal>V</literal> is set when the quorum device has given a vote to a node. In this
example, both nodes are set to <literal>V</literal> since they can communicate with each other.
If the cluster were to split into two single-node clusters, one of the nodes
would be set to <literal>V</literal> and the other node would be set to <literal>NV</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>MW/NMW</literal> — The quorum device <literal>master_ wins</literal> flag is set or not set. By default
the flag is not set and the value is <literal>NMW</literal> (Not Master Wins). For information on the <literal>master_wins</literal> flag see the
<literal>votequorum_qdevice_master_wins</literal>(3) man page.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum status</literal>
Quorum information
------------------
Date:             Wed Jun 29 13:17:02 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1    A,V,NMW node1 (local)
         2          1    A,V,NMW node2
         0          1            Qdevice</literallayout>
<simpara>The <literal role="command">pcs quorum device status</literal> shows the quorum device
runtime status.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum device status</literal>
Qdevice information
-------------------
Model:                  Net
Node ID:                1
Configured node list:
    0   Node ID = 1
    1   Node ID = 2
Membership node list:   1, 2

Qdevice-net information
----------------------
Cluster name:           mycluster
QNetd host:             qdevice:5403
Algorithm:              ffsplit
Tie-breaker:            Node with lowest node ID
State:                  Connected</literallayout>
<simpara>From the quorum device side, you can execute the following status command,
which shows the status of the <literal>corosync-qnetd</literal> daemon.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>pcs qdevice status net --full</literal>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              2
Connected clusters:             1
Maximum send/receive size:      32768/32768 bytes
Cluster "mycluster":
    Algorithm:          ffsplit
    Tie-breaker:        Node with lowest node ID
    Node ID 2:
        Client address:         ::ffff:192.168.122.122:50028
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 1:
        Client address:         ::ffff:192.168.122.121:48786
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)</literallayout>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_managing-quorum-device-service-configuring-quorum-devices">
<title>Managing the Quorum Device Service</title>
<simpara>PCS provides the ability to manage the quorum device service on the local host
(<literal role="command">corosync-qnetd</literal>), as shown in the following example commands. Note that
these commands affect only the <literal>corosync-qnetd</literal> service.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>pcs qdevice start net</literal>
[root@qdevice:~]# <literal>pcs qdevice stop net</literal>
[root@qdevice:~]# <literal>pcs qdevice enable net</literal>
[root@qdevice:~]# <literal>pcs qdevice disable net</literal>
[root@qdevice:~]# <literal>pcs qdevice kill net</literal></literallayout>
</section>
<section xml:id="proc_managing-quorum-device-settingsconfiguring-quorum-devices">
<title>Managing the quorum device settings in a cluster</title>
<simpara>The following sections describe the PCS commands that you can use
to manage the quorum device settings in a cluster.</simpara>
<section xml:id="s3-changeqdevice-HAAR">
<title>Changing quorum device settings</title>
<simpara>You can change the setting of a quorum device with
the <literal role="command">pcs quorum device update</literal> command.</simpara>
<warning>
<simpara>To change the <literal>host</literal> option of
quorum device model <literal>net</literal>, use
the <literal role="command">pcs quorum device remove</literal>
and the <literal role="command">pcs quorum device add</literal> commands
to set up the configuration properly, unless the old and
the new host are the same machine.</simpara>
</warning>
<simpara>The following command changes the quorum device algorithm to <literal>lms</literal>.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum device update model algorithm=lms</literal>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Reloading qdevice configuration on nodes...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
node1: corosync-qdevice started
node2: corosync-qdevice started</literallayout>
</section>
<section xml:id="removing_a_quorum_device" remap="_removing_a_quorum_device">
<title>Removing a quorum device</title>
<simpara>Use the following command to remove a quorum device configured
on a cluster node.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum device remove</literal>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Disabling corosync-qdevice...
node1: corosync-qdevice disabled
node2: corosync-qdevice disabled
Stopping corosync-qdevice...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
Removing qdevice certificates from nodes...
node1: Succeeded
node2: Succeeded</literallayout>
<simpara>After you have removed a quorum device, you should see the following error message
when displaying the quorum device status.</simpara>
<literallayout class="monospaced">[root@node1:~]# <literal>pcs quorum device status</literal>
Error: Unable to get quorum status: corosync-qdevice-tool: Can't connect to QDevice socket (is QDevice running?): No such file or directory</literallayout>
</section>
<section xml:id="destroying_a_quorum_device" remap="_destroying_a_quorum_device">
<title>Destroying a quorum device</title>
<simpara>To disable and stop a quorum device on the quorum device
host and delete all of its configuration
files, use the following command.</simpara>
<literallayout class="monospaced">[root@qdevice:~]# <literal>pcs qdevice destroy net</literal>
Stopping quorum device...
quorum device stopped
quorum device disabled
Quorum device 'net' configuration files removed</literallayout>
</section>
</section>
</section>
</chapter>
<chapter xml:id="assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters">
<title>Triggering scripts for cluster events</title>
<simpara>A Pacemaker cluster is an event-driven system, where an event might be a resource
or node failure, a configuration change, or a resource starting or stopping.
You can configure Pacemaker cluster alerts to take some external action when a cluster
event occurs by means of
alert agents, which are external programs that the cluster calls in
the same manner as the cluster calls resource agents to handle resource
configuration and operation.</simpara>
<simpara>The cluster passes information about the event to the agent
by means of environment variables. Agents can do anything with this
information, such as send an email message or log to a file
or update a monitoring system.</simpara>
<itemizedlist>
<listitem>
<simpara>Pacemaker provides several sample alert agents, which
are installed in <literal>/usr/share/pacemaker/alerts</literal> by default.
These sample scripts may be copied and used as is, or they may be used
as templates to be edited to suit your purposes.
Refer to the source code of the sample agents for the full set of
attributes they support.</simpara>
</listitem>
<listitem>
<simpara>If the sample alert agents do not meet your needs, you can write your own alert agents for a Pacemaker alert to call.</simpara>
</listitem>
</itemizedlist>
<section xml:id="using-sample-alert-agents-configuring-pacemaker-alert-agents">
<title>Installing and configuring sample alert agents</title>
<simpara>When you use one of the sample alert agents, you should review the script
to ensure that it suits your needs. These sample
agents are provided as a starting point for custom scripts for
specific cluster environments. Note that while Red Hat supports the interfaces that
the alert agents scripts use to communicate with Pacemaker, Red Hat does not provide support
for the custom agents themselves.</simpara>
<simpara>To use one of the sample alert agents, you must install the agent on each node in the cluster.
For example, the following command installs the <literal>alert_file.sh.sample</literal>
script as <literal>alert_file.sh</literal>.</simpara>
<literallayout class="monospaced"># <literal>install --mode=0755 /usr/share/pacemaker/alerts/alert_file.sh.sample /var/lib/pacemaker/alert_file.sh</literal></literallayout>
<simpara>After you have installed the script, you can create an alert that uses the script.</simpara>
<simpara>The following example configures an alert that uses the installed <literal>alert_file.sh</literal>
alert agent to log events to a file.
Alert agents run as the user
<literal>hacluster</literal>, which has a minimal set of permissions.</simpara>
<simpara>This example creates the log file <literal>pcmk_alert_file.log</literal> that will be used to record
the events. It then creates the alert agent and adds the path to the log file as its recipient.</simpara>
<literallayout class="monospaced"># <literal>touch /var/log/pcmk_alert_file.log</literal>
# <literal>chown hacluster:haclient /var/log/pcmk_alert_file.log</literal>
# <literal>chmod 600 /var/log/pcmk_alert_file.log</literal>
# <literal>pcs alert create id=alert_file description="Log events to a file." path=/var/lib/pacemaker/alert_file.sh</literal>
# <literal>pcs alert recipient add alert_file id=my-alert_logfile value=/var/log/pcmk_alert_file.log</literal></literallayout>
<simpara>The following example
installs the <literal>alert_snmp.sh.sample</literal>
script as <literal>alert_snmp.sh</literal> and
configures an alert that uses the installed <literal>alert_snmp.sh</literal>
alert agent to send cluster events as SNMP traps. By default, the script
will send all events except successful monitor calls to the SNMP server.
This example configures the timestamp format as a meta option.
After configuring the alert, this example configures a recipient for the alert
and displays the alert configuration.</simpara>
<literallayout class="monospaced"># <literal>install --mode=0755 /usr/share/pacemaker/alerts/alert_snmp.sh.sample /var/lib/pacemaker/alert_snmp.sh</literal>
# <literal>pcs alert create id=snmp_alert path=/var/lib/pacemaker/alert_snmp.sh meta timestamp-format="%Y-%m-%d,%H:%M:%S.%01N"</literal>
# <literal>pcs alert recipient add snmp_alert value=192.168.1.2</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: snmp_alert (path=/var/lib/pacemaker/alert_snmp.sh)
  Meta options: timestamp-format=%Y-%m-%d,%H:%M:%S.%01N.
  Recipients:
   Recipient: snmp_alert-recipient (value=192.168.1.2)</literallayout>
<simpara>The following example installs the
<literal>alert_smtp.sh</literal> agent and then
configures an alert that uses the installed
alert agent to send cluster events as email messages. After configuring the alert,
this example configures a recipient and displays the alert configuration.</simpara>
<literallayout class="monospaced"># <literal>install --mode=0755 /usr/share/pacemaker/alerts/alert_smtp.sh.sample /var/lib/pacemaker/alert_smtp.sh</literal>
# <literal>pcs alert create id=smtp_alert path=/var/lib/pacemaker/alert_smtp.sh options email_sender=donotreply@example.com</literal>
# <literal>pcs alert recipient add smtp_alert value=admin@example.com</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: smtp_alert (path=/var/lib/pacemaker/alert_smtp.sh)
  Options: email_sender=<link xlink:href="mailto:donotreply@example.com">donotreply@example.com</link>
  Recipients:
   Recipient: smtp_alert-recipient (value=<link xlink:href="mailto:admin@example.com">admin@example.com</link>)</literallayout>
</section>
<section xml:id="creating_a_cluster_alert" remap="_creating_a_cluster_alert">
<title>Creating a cluster alert</title>
<simpara>The following command creates a cluster alert. The options that you
configure are agent-specific configuration values that are passed
to the alert agent script at the path you specify as additional environment variables.
If you do not specify a value for <literal>id</literal>, one will be generated.</simpara>
<literallayout class="monospaced">pcs alert create path=<emphasis>path</emphasis> [id=<emphasis>alert-id</emphasis>] [description=<emphasis>description</emphasis>] [options [<emphasis>option</emphasis>=<emphasis>value</emphasis>]...] [meta [<emphasis>meta-option</emphasis>=<emphasis>value</emphasis>]...]</literallayout>
<simpara>Multiple alert agents may be configured; the cluster will call all of them for each event.
Alert agents will be called only on cluster nodes. They will be called for events involving Pacemaker Remote nodes, but they will never be called on those nodes.</simpara>
<simpara>The following example creates a simple alert that will call <literal>myscript.sh</literal> for each event.</simpara>
<literallayout class="monospaced"># <literal>pcs alert create id=my_alert path=/path/to/myscript.sh</literal></literallayout>
</section>
<section xml:id="displaying_modifying_and_removing_cluster_alerts" remap="_displaying_modifying_and_removing_cluster_alerts">
<title>Displaying, modifying, and removing cluster alerts</title>
<simpara>The following command shows all configured alerts along with the values of the configured
options.</simpara>
<literallayout class="monospaced">pcs alert [config|show]</literallayout>
<simpara>The following command updates an existing alert with the specified <emphasis>alert-id</emphasis> value.</simpara>
<literallayout class="monospaced">pcs alert update <emphasis>alert-id</emphasis> [path=<emphasis>path</emphasis>] [description=<emphasis>description</emphasis>] [options [<emphasis>option</emphasis>=<emphasis>value</emphasis>]...] [meta [<emphasis>meta-option</emphasis>=<emphasis>value</emphasis>]...]</literallayout>
<simpara>The following command removes an alert with the specified <emphasis>alert-id</emphasis> value.</simpara>
<literallayout class="monospaced">pcs alert remove <emphasis>alert-id</emphasis></literallayout>
<simpara>Alternately, you can run the <literal>pcs alert delete</literal> command, which is identical to the
<literal>pcs alert remove</literal> command.  Both the <literal>pcs alert delete</literal> and the <literal>pcs alert remove</literal>
commands allow you to specify more than one alert to be deleted.</simpara>
</section>
<section xml:id="configuring_alert_recipients" remap="_configuring_alert_recipients">
<title>Configuring alert recipients</title>
<simpara>Usually alerts are directed towards a recipient. Thus each alert may
be additionally configured with one or more recipients.
The cluster will call the agent separately for each recipient.</simpara>
<simpara>The recipient may be anything the alert agent can recognize: an IP address,
an email address, a file name, or whatever the particular agent supports.</simpara>
<simpara>The following command adds a new recipient to the specified alert.</simpara>
<literallayout class="monospaced">pcs alert recipient add <emphasis>alert-id</emphasis> value=<emphasis>recipient-value</emphasis> [id=<emphasis>recipient-id</emphasis>] [description=<emphasis>description</emphasis>] [options [<emphasis>option</emphasis>=<emphasis>value</emphasis>]...] [meta [<emphasis>meta-option</emphasis>=<emphasis>value</emphasis>]...]</literallayout>
<simpara>The following command updates an existing alert recipient.</simpara>
<literallayout class="monospaced">pcs alert recipient update <emphasis>recipient-id</emphasis> [value=<emphasis>recipient-value</emphasis>] [description=<emphasis>description</emphasis>] [options [<emphasis>option</emphasis>=<emphasis>value</emphasis>]...] [meta [<emphasis>meta-option</emphasis>=<emphasis>value</emphasis>]...]</literallayout>
<simpara>The following command removes the specified alert recipient.</simpara>
<literallayout class="monospaced">pcs alert recipient remove <emphasis>recipient-id</emphasis></literallayout>
<simpara>Alternately, you can run the <literal>pcs alert recipient delete</literal> command, which is identical to
the <literal>pcs alert recipient remove</literal> command.  Both the <literal>pcs alert recipient remove</literal> and the
<literal>pcs alert recipient delete</literal> commands allow you to remove more than one alert recipient.</simpara>
<simpara>The following example command adds
the alert recipient <literal>my-alert-recipient</literal> with a recipient ID
of <literal>my-recipient-id</literal> to the
alert <literal>my-alert</literal>. This will configure the cluster to call the alert script
that has been configured for <literal>my-alert</literal>
for each event, passing the recipient <literal>some-address</literal> as an environment variable.</simpara>
<literallayout class="monospaced">#  <literal>pcs alert recipient add my-alert value=my-alert-recipient id=my-recipient-id options value=some-address</literal></literallayout>
</section>
<section xml:id="alert_meta_options" remap="_alert_meta_options">
<title>Alert meta options</title>
<simpara>As with resource agents, meta options can be configured for alert agents to affect how Pacemaker calls them.
<xref linkend="tb-alert-meta-HAAR"/>
describes the alert meta options.
Meta options can be configured per alert agent as well as per recipient.</simpara>
<table xml:id="tb-alert-meta-HAAR" frame="all" rowsep="1" colsep="1">
<title>Alert Meta Options</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Meta-Attribute</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>timestamp-format</literal></simpara></entry>
<entry align="left" valign="top"><simpara>%H:%M:%S.%06N</simpara></entry>
<entry align="left" valign="top"><simpara>Format the cluster will use when sending the event’s timestamp to the agent. This is a string as used with the <literal>date</literal>(1) command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>30s</simpara></entry>
<entry align="left" valign="top"><simpara>If the alert agent does not complete within this amount of time, it will be terminated.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The following example configures an alert that calls the script
<literal>myscript.sh</literal> and then adds two recipients to the alert.
The first recipient has an ID of <literal>my-alert-recipient1</literal>
and the second recipient has an ID of <literal>my-alert-recipient2</literal>.
The script will get called twice for each event, with each call using a 15-second timeout.
One call will be passed to the recipient <literal>someuser@example.com</literal> with a timestamp
in the format %D %H:%M, while the other call will be passed to the recipient <literal>otheruser@example.com</literal> with
a timestamp in the format %c.</simpara>
<literallayout class="monospaced"># <literal>pcs alert create id=my-alert path=/path/to/myscript.sh meta timeout=15s</literal>
# <literal>pcs alert recipient add my-alert value=someuser@example.com id=my-alert-recipient1 meta timestamp-format="%D %H:%M"</literal>
# <literal>pcs alert recipient add my-alert value=otheruser@example.com id=my-alert-recipient2 meta timestamp-format="%c"</literal></literallayout>
</section>
<section xml:id="alert_configuration_command_examples" remap="_alert_configuration_command_examples">
<title>Alert configuration command examples</title>
<simpara>The following sequential examples show some basic alert configuration commands to show
the format to use to create alerts, add recipients, and display the
configured alerts.</simpara>
<simpara>Note that while you must install the alert agents themselves on each node in
a cluster, you need to run the <literal>pcs</literal> commands only once.</simpara>
<simpara>The following commands create a simple alert, add two recipients to the alert, and
display the configured values.</simpara>
<itemizedlist>
<listitem>
<simpara>Since no alert ID value is specified, the system creates an alert ID
value of <literal>alert</literal>.</simpara>
</listitem>
<listitem>
<simpara>The first recipient creation command specifies a recipient of <literal>rec_value</literal>.
Since this command
does not specify a recipient ID, the value of <literal>alert-recipient</literal> is
used as the recipient ID.</simpara>
</listitem>
<listitem>
<simpara>The second recipient creation command specifies a recipient of <literal>rec_value2</literal>.
This command specifies a recipient ID of <literal>my-recipient</literal> for the recipient.</simpara>
</listitem>
</itemizedlist>
<literallayout class="monospaced"># <literal>pcs alert create path=/my/path</literal>
# <literal>pcs alert recipient add alert value=rec_value</literal>
# <literal>pcs alert recipient add alert value=rec_value2 id=my-recipient</literal>
# <literal>pcs alert config</literal>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)</literallayout>
<simpara>This following commands add a second alert and a recipient for that alert. The alert ID for the second
alert is <literal>my-alert</literal> and the recipient value is <literal>my-other-recipient</literal>.
Since no recipient ID is specified, the system provides a recipient id of <literal>my-alert-recipient</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs alert create id=my-alert path=/path/to/script description=alert_description options option1=value1 opt=val meta timeout=50s timestamp-format="%H%B%S"</literal>
# <literal>pcs alert recipient add my-alert value=my-other-recipient</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=value1
  Meta options: timestamp-format=%H%B%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)</literallayout>
<simpara>The following commands modify the alert values for the alert <literal>my-alert</literal> and for
the recipient <literal>my-alert-recipient</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs alert update my-alert options option1=newvalue1 meta timestamp-format="%H%M%S"</literal>
# <literal>pcs alert recipient update my-alert-recipient options option1=new meta timeout=60s</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format=%H%M%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</literallayout>
<simpara>The following command removes the recipient <literal>my-alert-recipient</literal> from <literal>alert</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs alert recipient remove my-recipient</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format="%M%B%S" timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</literallayout>
<simpara>The following command removes <literal>myalert</literal> from the configuration.</simpara>
<literallayout class="monospaced"># <literal>pcs alert remove myalert</literal>
# <literal>pcs alert</literal>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)</literallayout>
</section>
<section xml:id="writing_an_alert_agent" remap="_writing_an_alert_agent">
<title>Writing an alert agent</title>
<simpara>There are three types of Pacemaker alerts: node alerts, fencing alerts, and resource alerts.
The environment variables that are passed to the alert agents can differ, depending
on the type of alert.
<xref linkend="tb-alert-environmentvariables-HAAR"/>
describes the environment variables that are passed to alert agents and specifies
when the environment variable is associated with a specific alert type.</simpara>
<table xml:id="tb-alert-environmentvariables-HAAR" frame="all" rowsep="1" colsep="1">
<title>Environment Variables Passed to Alert Agents</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="44*"/>
<colspec colname="col_2" colwidth="56*"/>
<thead>
<row>
<entry align="left" valign="top">Environment Variable</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_kind</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The type of alert (node, fencing, or resource)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_version</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The version of Pacemaker sending the alert</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_recipient</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The configured recipient</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_node_sequence</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A sequence number increased whenever an alert is being issued on the local node, which
can be used to reference the order in which alerts have been issued by Pacemaker.
An alert for an event that happened later in time reliably has a higher sequence
number than alerts for earlier events. Be aware that this number has no cluster-wide meaning.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_timestamp</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A timestamp created prior to executing the agent, in the format specified
by the <literal>timestamp-format</literal>
meta option. This allows the agent to have a reliable,
high-precision time of when the event occurred,
regardless of when the agent itself was invoked (which could
potentially be delayed due to system load or other circumstances).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_node</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Name of affected node</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_desc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Detail about event. For node alerts, this is the node’s current state (member or lost).
For fencing alerts, this is a summary of the requested fencing operation,
including origin, target, and fencing operation error code, if any.
For resource alerts, this is a readable string equivalent of <literal>CRM_alert_status</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_nodeid</literal></simpara></entry>
<entry align="left" valign="top"><simpara>ID of node whose status changed (provided with node alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_task</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The requested fencing or resource operation
(provided with fencing and resource alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_rc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The numerical return code of the fencing or resource operation
(provided with fencing and resource alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_rsc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the affected resource (resource alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The interval of the resource operation (resource alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_target_rc</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The expected numerical return code of the operation (resource alerts only)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CRM_alert_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A numerical code used by Pacemaker to represent the operation
result (resource alerts only)</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>When writing an alert agent, you must take the following concerns into account.</simpara>
<itemizedlist>
<listitem>
<simpara>Alert agents may be called with no recipient (if none is configured),
so the agent must be able to handle this situation, even if it only exits in that case.
Users may modify the configuration in stages, and add a recipient later.</simpara>
</listitem>
<listitem>
<simpara>If more than one recipient is configured for an alert, the alert agent will be
called once per recipient. If an agent is not able to run concurrently,
it should be configured with only a single recipient.
The agent is free, however, to interpret the recipient as a list.</simpara>
</listitem>
<listitem>
<simpara>When a cluster event occurs, all alerts are fired off at
the same time as separate processes. Depending on how many alerts and recipients
are configured and on what is done within the alert agents,
a significant load burst may occur. The agent could be
written to take this into consideration, for example by queueing resource-intensive actions
into some other instance, instead of directly executing them.</simpara>
</listitem>
<listitem>
<simpara>Alert agents are run as the <literal>hacluster</literal> user, which has a minimal set
of permissions. If an agent requires additional privileges, it is recommended to configure
<literal role="command">sudo</literal>
to allow the agent to run the necessary commands as another user with the appropriate privileges.</simpara>
</listitem>
<listitem>
<simpara>Take care to validate and sanitize user-configured parameters, such as
<literal>CRM_alert_timestamp</literal> (whose content is specified by the user-configured
<literal>timestamp-format</literal>), <literal>CRM_alert_recipient</literal>, and all alert options.
This is necessary to protect against configuration errors. In addition, if some
user can modify the CIB without having <literal>hacluster</literal>-level access to the cluster nodes,
this is a potential security concern as well, and you should avoid the possibility of code injection.</simpara>
</listitem>
<listitem>
<simpara>If a cluster contains resources with operations for which the <literal>on-fail</literal> parameter is
set to <literal>fence</literal>, there will be multiple fence notifications on failure,
one for each resource for which this parameter is set plus one additional notification.
Both the <literal>pacemaker-fenced</literal> and <literal>pacemaker-controld</literal> will send
notifications. Pacemaker performs only one actual fence operation in this case, however,
no matter how many notifications are sent.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The alerts interface is designed to be backward compatible with the external scripts
interface used by the <literal>ocf:pacemaker:ClusterMon</literal> resource.
To preserve this compatibility, the environment variables passed to alert
agents are available prepended with <literal>CRM_notify_</literal> as
well as <literal>CRM_alert_</literal>.
One break in compatibility is that the <literal>ClusterMon</literal> resource ran external scripts
as the root user, while alert agents are run as the <literal>hacluster</literal> user.</simpara>
</note>
</section>
</chapter>
<chapter xml:id="assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters">
<title>Configuring multi-site clusters with Pacemaker</title>
<simpara>When a cluster spans
more than one site, issues with network connectivity between the sites
can lead to split-brain situations. When connectivity drops, there is
no way for a node on one site to determine whether a node on another
site has failed or is still functioning with a failed site interlink.
In addition, it can be problematic to provide high availability
services across two sites which are too far apart to keep
synchronous.</simpara>
<simpara>To address these issues, Pacemaker provides
full support for the ability to configure
high availability clusters that span multiple sites through the
use of a Booth cluster ticket manager.</simpara>
<section xml:id="con_booth-cluster-ticket-manager-configuring-multisite-cluster">
<title>Overview of Booth cluster ticket manager</title>
<simpara>The Booth <emphasis>ticket manager</emphasis> is a distributed service that is meant
to be run on a different physical network than the networks
that connect the cluster nodes at particular sites.
It yields another, loose cluster, a <emphasis>Booth formation</emphasis>,
that sits on top of the regular clusters at the sites.
This aggregated communication layer facilitates
consensus-based decision processes for individual Booth tickets.</simpara>
<simpara>A Booth <emphasis>ticket</emphasis> is a singleton in the Booth formation and represents
a time-sensitive, movable unit of authorization. Resources can
be configured to require a certain ticket to run. This can
ensure that resources are run at only one site at a time,
for which a ticket or tickets have been granted.</simpara>
<simpara>You can think of a Booth formation as
an overlay cluster consisting of clusters running
at different sites, where all the original clusters are
independent of each other.
It is the Booth service
which communicates to the clusters
whether they have been granted a ticket,
and it is Pacemaker that determines
whether to run resources in a cluster based on a Pacemaker
ticket constraint. This means that when using the ticket manager,
each of the clusters can run its own resources as well
as shared resources. For example there can be resources
A, B and C running only in one cluster, resources D, E, and F running
only in the other cluster, and resources G and H running in either
of the two clusters as determined by a ticket.
It is also possible to have an additional resource J that could
run in either of the two clusters as determined by a separate ticket.</simpara>
</section>
<section xml:id="proc-configuring-multisite-with-booth-configuring-multisite-cluster">
<title>Configuring multi-site clusters with Pacemaker</title>
<simpara>The following procedure provides an outline of the steps
you follow to configure a multi-site configuration that uses the Booth ticket
manager.</simpara>
<simpara>These example commands use the following arrangement:</simpara>
<itemizedlist>
<listitem>
<simpara>Cluster 1 consists of the nodes <literal>cluster1-node1</literal> and <literal>cluster1-node2</literal></simpara>
</listitem>
<listitem>
<simpara>Cluster 1 has a floating IP address assigned to it of 192.168.11.100</simpara>
</listitem>
<listitem>
<simpara>Cluster 2 consists of <literal>cluster2-node1</literal> and <literal>cluster2-node2</literal></simpara>
</listitem>
<listitem>
<simpara>Cluster 2 has a floating IP address assigned to it of 192.168.22.100</simpara>
</listitem>
<listitem>
<simpara>The arbitrator node is <literal>arbitrator-node</literal> with an ip address of 192.168.99.100</simpara>
</listitem>
<listitem>
<simpara>The name of the Booth ticket that this configuration uses is <literal>apacheticket</literal></simpara>
</listitem>
</itemizedlist>
<simpara>These example commands assume that the cluster resources for
an Apache service have been configured as part of the resource
group <literal>apachegroup</literal> for each cluster.
It is not
required that the resources and resource groups be the same on each cluster to
configure a ticket constraint for those resources, since the
Pacemaker instance for each cluster is independent, but that is a common
failover scenario.</simpara>
<simpara>Note that at any time in the configuration procedure you can
enter the <literal role="command">pcs booth config</literal> command to display
the booth configuration for the current node or cluster
or the <literal role="command">pcs booth status</literal> command to
display the current status of booth on the local node.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the <literal>booth-site</literal> Booth ticket manager package on each node of both clusters.</simpara>
<literallayout class="monospaced">[root@cluster1-node1 ~]# <literal>yum install -y booth-site</literal>
[root@cluster1-node2 ~]# <literal>yum install -y booth-site</literal>
[root@cluster2-node1 ~]# <literal>yum install -y booth-site</literal>
[root@cluster2-node2 ~]# <literal>yum install -y booth-site</literal></literallayout>
</listitem>
<listitem>
<simpara>Install the <literal>pcs</literal>, <literal>booth-core</literal>, and <literal>booth-arbitrator</literal>
packages on the arbitrator node.</simpara>
<literallayout class="monospaced">[root@arbitrator-node ~]# <literal>yum install -y pcs booth-core booth-arbitrator</literal></literallayout>
</listitem>
<listitem>
<simpara>If you are running the <literal>firewalld</literal> daemon, execute the following commands
on all nodes in both clusters as well as on the arbitrator node
to enable the ports that are required by the Red Hat High Availability Add-On.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>`
# <literal>firewall-cmd --add-service=high-availability</literal>`</literallayout>
<simpara>You may need to modify which ports are open to suit local conditions.
For more information on the ports that are required by the Red Hat High-Availability Add-On, see
<link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#proc_enabling-ports-for-high-availability-creating-high-availability-cluster">Enabling ports for the High Availability Add-On</link>.</simpara>
</listitem>
<listitem>
<simpara>Create a Booth configuration on one node of one cluster. The addresses you specify
for each cluster and for the arbitrator must be IP addresses. For each cluster,
you specify a floating IP address.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs booth setup sites 192.168.11.100 192.168.22.100 arbitrators 192.168.99.100</literal></literallayout>
<simpara>This command creates the configuration files
<literal>/etc/booth/booth.conf</literal> and <literal>/etc/booth/booth.key</literal> on the
node from which it is run.</simpara>
</listitem>
<listitem>
<simpara>Create a ticket for the Booth configuration. This is the ticket that you will use
to define the resource constraint that will allow resources to run only when
this ticket has been granted to the cluster.</simpara>
<simpara>This basic failover configuration procedure uses only one ticket, but you can
create additional tickets for more complicated scenarios where each ticket is
associated with a different resource or resources.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs booth ticket add apacheticket</literal></literallayout>
</listitem>
<listitem>
<simpara>Synchronize the Booth configuration to all nodes in the current cluster.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs booth sync</literal></literallayout>
</listitem>
<listitem>
<simpara>From the arbitrator node, pull the Booth configuration to the arbitrator.
If you have not previously done so, you must first authenticate <literal>pcs</literal>
to the node from which you are pulling the configuration.</simpara>
<literallayout class="monospaced">[arbitrator-node ~] # <literal>pcs host auth cluster1-node1</literal>
[arbitrator-node ~] # <literal>pcs booth pull cluster1-node1</literal></literallayout>
</listitem>
<listitem>
<simpara>Pull the Booth configuration to the other cluster and synchronize to all
the nodes of that cluster.
As with the arbitrator node, if you have not previously done so, you must first authenticate <literal>pcs</literal>
to the node from which you are pulling the configuration.</simpara>
<literallayout class="monospaced">[cluster2-node1 ~] # <literal>pcs host auth cluster1-node1</literal>
[cluster2-node1 ~] # <literal>pcs booth pull cluster1-node1</literal>
[cluster2-node1 ~] # <literal>pcs booth sync</literal></literallayout>
</listitem>
<listitem>
<simpara>Start and enable Booth on the arbitrator.</simpara>
<note>
<simpara>You must not manually start or enable Booth on
any of the nodes of the clusters since Booth runs as a Pacemaker resource in those clusters.</simpara>
</note>
<literallayout class="monospaced">[arbitrator-node ~] # <literal>pcs booth start</literal>
[arbitrator-node ~] # <literal>pcs booth enable</literal></literallayout>
</listitem>
<listitem>
<simpara>Configure Booth to run as a cluster resource on both cluster sites.
This creates a resource group with <literal>booth-ip</literal>
and <literal>booth-service</literal> as members of that group.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs booth create ip 192.168.11.100</literal>
[cluster2-node1 ~] # <literal>pcs booth create ip 192.168.22.100</literal></literallayout>
</listitem>
<listitem>
<simpara>Add a ticket constraint to the resource group you have defined for each cluster.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs constraint ticket add apacheticket apachegroup</literal>
[cluster2-node1 ~] # <literal>pcs constraint ticket add apacheticket apachegroup</literal></literallayout>
<simpara>You can enter the following command to display the currently configured ticket
constraints.</simpara>
<literallayout class="monospaced">pcs constraint ticket [show]</literallayout>
</listitem>
<listitem>
<simpara>Grant the ticket you created for this setup to the first cluster.</simpara>
<simpara>Note that it is not necessary to have defined ticket constraints before granting a ticket.
Once you have initially granted a ticket to a cluster, then Booth takes over ticket management
unless you override this manually with the <literal role="command">pcs booth ticket revoke</literal> command.
For information on the <literal role="command">pcs booth</literal> administration commands,
see the PCS help screen
for the <literal role="command">pcs booth</literal> command.</simpara>
<literallayout class="monospaced">[cluster1-node1 ~] # <literal>pcs booth ticket grant apacheticket</literal></literallayout>
</listitem>
</orderedlist>
<simpara>It is possible to add or remove tickets at any time, even after completing this
procedure. After adding or removing a ticket, however, you must synchronize the
configuration files to the other nodes and clusters as well as to the
arbitrator and grant the ticket as is shown in this procedure.</simpara>
<simpara>For information on additional Booth administration commands that you can
use for cleaning up and removing Booth configuration files, tickets,
and resources,
see the PCS help screen
for the <literal role="command">pcs booth</literal> command.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_remote-node-management-configuring-and-managing-high-availability-clusters">
<title>Integrating non-corosync nodes into a cluster: the pacemaker_remote service</title>
<simpara>The <literal>pacemaker_remote</literal> service
allows nodes not running <literal>corosync</literal> to
integrate into the cluster and have the cluster manage their resources just as
if they were real cluster nodes.</simpara>
<simpara>Among the capabilities that the <literal>pacemaker_remote</literal> service provides are the
following:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>pacemaker_remote</literal> service allows you to scale beyond the
Red Hat support limit of 32 nodes for RHEL 8.1.</simpara>
</listitem>
<listitem>
<simpara>The <literal>pacemaker_remote</literal> service allows you to manage a virtual environment as a cluster
resource and also to manage individual services within the virtual environment as cluster resources.</simpara>
</listitem>
</itemizedlist>
<simpara>The following terms are used to describe the
<literal>pacemaker_remote</literal> service.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis>cluster node</emphasis> — A node running the High Availability
services (<literal>pacemaker</literal>
and <literal>corosync</literal>).</simpara>
</listitem>
<listitem>
<simpara><emphasis>remote node</emphasis> — A node
running <literal>pacemaker_remote</literal>
to remotely integrate into the cluster without
requiring <literal>corosync</literal> cluster membership.
A remote node is configured as a cluster resource that uses
the <literal>ocf:pacemaker:remote</literal> resource agent.</simpara>
</listitem>
<listitem>
<simpara><emphasis>guest node</emphasis> — A virtual guest node running
the <literal>pacemaker_remote</literal> service.
The virtual guest resource is managed by
the cluster; it is both started by the cluster and integrated into the cluster as a remote node.</simpara>
</listitem>
<listitem>
<simpara><emphasis>pacemaker_remote</emphasis> — A service daemon
capable of performing remote application management within remote
nodes and KVM guest
nodes in a Pacemaker cluster environment.
This service is an enhanced version of
Pacemaker’s local executor daemon (<literal>pacemaker-execd</literal>) that is capable of managing
resources remotely on a node not running corosync.</simpara>
</listitem>
</itemizedlist>
<simpara>A Pacemaker cluster running the <literal>pacemaker_remote</literal> service
has the following characteristics.</simpara>
<itemizedlist>
<listitem>
<simpara>Remote nodes and guest nodes run the <literal>pacemaker_remote</literal> service
(with very little
configuration required on the virtual machine side).</simpara>
</listitem>
<listitem>
<simpara>The cluster stack (<literal>pacemaker</literal> and <literal>corosync</literal>), running on the cluster nodes, connects to
the <literal>pacemaker_remote</literal> service on the remote nodes, allowing them
to integrate into the cluster.</simpara>
</listitem>
<listitem>
<simpara>The cluster stack (<literal>pacemaker</literal> and <literal>corosync</literal>), running on the cluster nodes,
launches the guest nodes and immediately connects to
the <literal>pacemaker_remote</literal> service on the guest nodes, allowing them
to integrate into the cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>The key difference between the cluster nodes and the remote and guest nodes that
the cluster nodes manage is
that the remote and guest nodes are not running the cluster stack. This means the remote and
guest nodes have the following limitations:</simpara>
<itemizedlist>
<listitem>
<simpara>they do not take place in quorum</simpara>
</listitem>
<listitem>
<simpara>they do not execute fencing device actions</simpara>
</listitem>
<listitem>
<simpara>they are not eligible to be the cluster’s Designated Controller (DC)</simpara>
</listitem>
<listitem>
<simpara>they do not themselves run the full range of <literal role="command">pcs</literal> commands</simpara>
</listitem>
</itemizedlist>
<simpara>On the other hand, remote nodes and guest nodes are not bound to
the scalability limits associated with the cluster stack.</simpara>
<simpara>Other than these noted limitations, the remote and guest nodes
behave just like cluster nodes in respect to resource management, and the
remote and guest nodes can themselves be fenced. The cluster is fully capable
of managing and monitoring resources on each remote and guest node: You can build constraints
against them, put them in standby, or perform any other action you perform
on cluster nodes with the <literal role="command">pcs</literal> commands.
Remote and guest nodes appear in cluster status output just as
cluster nodes do.</simpara>
<section xml:id="ref_host-and-guest-authentication-of-remote-nodes-remote-node-management">
<title>Host and guest authentication of pacemaker_remote nodes</title>
<simpara>The connection between cluster nodes and pacemaker_remote is secured
using Transport Layer Security (TLS) with pre-shared key (PSK)
encryption and authentication over TCP (using port 3121 by default).
This means both the cluster node and the node running
<literal>pacemaker_remote</literal> must
share the same private key. By default this key must be placed at
<literal>/etc/pacemaker/authkey</literal> on both
cluster nodes and remote nodes.</simpara>
<simpara>The <literal role="command">pcs cluster node add-guest</literal> command sets up the
<literal>authkey</literal> for guest nodes and the
<literal role="command">pcs cluster node add-remote</literal> command sets up the
<literal>authkey</literal> for remote nodes.</simpara>
</section>
<section xml:id="assembly_configuring-kvm-guest-nodes-remote-node-management">
<title>Configuring KVM guest nodes</title>
<simpara>A Pacemaker guest node is a virtual guest node running the <literal>pacemaker_remote</literal> service.
The virtual guest node is managed by the cluster.</simpara>
<section xml:id="ref_guest-node-resource-options-configuring-kvm-guest-nodes">
<title>Guest node resource options</title>
<simpara>When configuring a virtual machine to act as a guest node,
you create a <literal>VirtualDomain</literal> resource, which manages the virtual machine.
For descriptions of the options you can set for a <literal>VirtualDomain</literal>
resource, see
<xref linkend="tb-virtdomain-options-HAAR"/>.</simpara>
<simpara>In addition to the <literal>VirtualDomain</literal> resource options,
metadata options define the resource as a guest node and define the connection parameters.
You set these resource options
with the <literal role="command">pcs cluster node add-guest</literal> command.
<xref linkend="tb-remoteklm-options-HAAR"/>
describes these metadata options.</simpara>
<table xml:id="tb-remoteklm-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Metadata Options for Configuring KVM Resources as Remote Nodes</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>remote-node</literal></simpara></entry>
<entry align="left" valign="top"><simpara>&lt;none&gt;</simpara></entry>
<entry align="left" valign="top"><simpara>The name of the guest node this resource defines. This both
enables the resource as a guest node and defines the unique name used
to identify the guest node.
<emphasis>WARNING</emphasis>: This value cannot overlap with any resource or node IDs.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>remote-port</literal></simpara></entry>
<entry align="left" valign="top"><simpara>3121</simpara></entry>
<entry align="left" valign="top"><simpara>Configures a custom port to use for the guest connection to <literal>pacemaker_remote</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>remote-addr</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The address provided in the <literal>pcs host auth</literal> command</simpara></entry>
<entry align="left" valign="top"><simpara>The IP address or host name to connect to</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>remote-connect-timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
<entry align="left" valign="top"><simpara>Amount of time before a pending guest connection will time out</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="proc_integrating-vm-as-guest-node-configuring-kvm-guest-nodes">
<title>Integrating a virtual machine as a guest node</title>
<simpara>The following procedure is
a high-level summary overview of the
steps to perform to
have Pacemaker launch a virtual machine and to integrate
that machine as a guest node, using <literal>libvirt</literal>
and KVM virtual guests.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure the <literal>VirtualDomain</literal> resources.</simpara>
</listitem>
<listitem>
<simpara>Enter the following commands on every virtual machine to
install <literal>pacemaker_remote</literal> packages, start the
<literal>pcsd</literal> service and enable it to run
on startup, and allow
TCP port 3121 through the firewall.</simpara>
<literallayout class="monospaced"># <literal>yum install pacemaker-remote resource-agents pcs</literal>
# <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal>
# <literal>firewall-cmd --add-port 3121/tcp --permanent</literal>
# <literal>firewall-cmd --add-port 2224/tcp --permanent</literal>
# <literal>firewall-cmd --reload</literal></literallayout>
</listitem>
<listitem>
<simpara>Give each virtual machine a static network address and unique host name,
which should be known to all nodes.
For information on setting a static IP address for the guest virtual
machine, see the <emphasis><phrase role="citetitle">Virtualization Deployment and Administration
Guide</phrase></emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you have not already done so, authenticate <literal>pcs</literal>
to the node you will be integrating as a quest node.</simpara>
<literallayout class="monospaced"># <literal>pcs host auth <emphasis>nodename</emphasis></literal></literallayout>
</listitem>
<listitem>
<simpara>Use the following command to convert an existing
<literal>VirtualDomain</literal> resource into a guest node.
This command must be run on a cluster node and not on
the guest node which is being added.
In addition to converting the resource,
this command copies the <literal>/etc/pacemaker/authkey</literal>
to the guest node and
starts and enables the <literal>pacemaker_remote</literal> daemon on the
guest node.
The node name for the guest node, which you can define arbitrarily, can differ from
the host name for the node.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster node add-guest <emphasis>nodename</emphasis> <emphasis>resource_id</emphasis></literal> [<literal><emphasis>options</emphasis></literal>]</literallayout>
</listitem>
<listitem>
<simpara>After creating the <literal>VirtualDomain</literal> resource, you can treat the guest node just
as you would treat any other node in the cluster.
For example, you can create a resource and
place a resource constraint on the resource to run on the guest node as in the following
commands, which are run from a cluster node.
You can include guest nodes in groups,
which allows you to group a storage device, file system, and VM.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</literal>
# <literal>pcs constraint location webserver prefers</literal> <literal><emphasis>nodename</emphasis></literal></literallayout>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="assembly_configuring-remote-nodes-remote-node-management">
<title>Configuring Pacemaker remote nodes</title>
<simpara>A remote node is defined as a cluster resource with <literal>ocf:pacemaker:remote</literal>
as the resource agent. You create this resource
with the <literal role="command">pcs cluster node add-remote</literal> command.</simpara>
<section xml:id="ref_remote-node-resource-options-configuring-remote-nodes">
<title>Remote node resource options</title>
<simpara><xref linkend="tb-remotenode-options-HAAR"/>
describes
the resource options you can configure for a <literal>remote</literal> resource.</simpara>
<table xml:id="tb-remotenode-options-HAAR" frame="all" rowsep="1" colsep="1">
<title>Resource Options for Remote Nodes</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="38*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="38*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>reconnect_interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>0</simpara></entry>
<entry align="left" valign="top"><simpara>Time in seconds to wait before attempting to reconnect to a remote node
after an active connection to the remote node has been severed. This wait
is recurring. If reconnect fails after the wait period, a new reconnect
attempt will be made after observing the wait time. When this option is
in use, Pacemaker will keep attempting to reach out and connect to the
remote node indefinitely after each wait interval.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>server</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Address specified with <literal>pcs host auth</literal> command</simpara></entry>
<entry align="left" valign="top"><simpara>Server to connect to. This can be an IP address or host name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>port</literal></simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><simpara>TCP port to connect to.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="proc-integrating-remote-nodes-configuring-remote-nodes">
<title>Remote node configuration overview</title>
<simpara>This section provides a high-level summary overview of the
steps to perform to configure a Pacemaker Remote
node and to integrate that node into an existing Pacemaker
cluster environment.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On the node that you will be configuring as a remote node,
allow cluster-related services through the local firewall.</simpara>
<literallayout class="monospaced"># <literal>firewall-cmd --permanent --add-service=high-availability</literal>
success
# <literal>firewall-cmd --reload</literal>
success</literallayout>
<note>
<simpara>If you are using <literal>iptables</literal> directly, or some other firewall solution besides
<literal>firewalld</literal>, simply open the following ports: TCP ports 2224 and 3121.</simpara>
</note>
</listitem>
<listitem>
<simpara>Install the <literal role="command">pacemaker_remote</literal> daemon on the remote node.</simpara>
<literallayout class="monospaced"># <literal>yum install -y pacemaker-remote resource-agents pcs</literal></literallayout>
</listitem>
<listitem>
<simpara>Start and enable <literal>pcsd</literal> on the remote node.</simpara>
<literallayout class="monospaced"># <literal>systemctl start pcsd.service</literal>
# <literal>systemctl enable pcsd.service</literal></literallayout>
</listitem>
<listitem>
<simpara>If you have not already done so, authenticate <literal>pcs</literal>
to the node you will be adding as a remote node.</simpara>
<literallayout class="monospaced"># <literal>pcs host auth remote1</literal></literallayout>
</listitem>
<listitem>
<simpara>Add the remote node resource to the cluster with the following command.
This command also syncs all relevant configuration files to the new node, starts
the node, and configures it to start <literal>pacemaker_remote</literal> on boot.
This command must be run on a cluster node and not on
the remote node which is being added.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster node add-remote remote1</literal></literallayout>
</listitem>
<listitem>
<simpara>After adding the <literal>remote</literal> resource to the cluster, you can
treat the remote node just
as you would treat any other node in the cluster.
For example, you can create a resource and
place a resource constraint on the resource to run on the remote node as in the following
commands, which are run from a cluster node.</simpara>
<literallayout class="monospaced"># <literal>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</literal>
# <literal>pcs constraint location webserver prefers remote1</literal></literallayout>
<warning>
<simpara>Never involve a remote node connection resource in a resource group,
colocation constraint, or order constraint.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Configure fencing resources for the remote node.
Remote nodes are fenced the same way as cluster nodes.
Configure fencing resources for use with
remote nodes the same as you would with cluster nodes.
Note, however, that remote nodes can never initiate a fencing action. Only
cluster nodes are capable of actually executing a fencing operation against
another node.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="proc_changing-default-port-location-remote-node-management">
<title>Changing the default port location</title>
<simpara>If you need to change the default port
location for either Pacemaker
or <literal>pacemaker_remote</literal>,
you can set the <literal>PCMK_remote_port</literal> environment
variable that affects both of these daemons.
This environment variable
can be enabled by placing it in the <literal>/etc/sysconfig/pacemaker</literal> file
as follows.</simpara>
<literallayout class="monospaced"><emphasis role="marked">==</emphasis>==# Pacemaker Remote
...
#
# Specify a custom port for Pacemaker Remote connections
PCMK_remote_port=3121</literallayout>
<simpara>When changing the default port used by a particular guest node or remote
node, the <literal>PCMK_remote_port</literal> variable must be set in that node’s
<literal>/etc/sysconfig/pacemaker</literal> file, and the cluster resource creating the guest
node or remote node connection must also be configured with the same
port number (using the <literal>remote-port</literal> metadata option for guest nodes, or the
<literal>port</literal> option for remote nodes).</simpara>
</section>
<section xml:id="proc_upgrading-systems-with-pacemaker-remote-remote-node-management">
<title>Upgrading systems with pacemaker_remote nodes</title>
<simpara>If the <literal>pacemaker_remote</literal> service is stopped on an active
Pacemaker Remote node, the cluster will gracefully migrate resources off
the node before stopping the node.
This allows you to perform software upgrades and other routine
maintenance procedures without removing the node from the cluster.
Once <literal>pacemaker_remote</literal> is shut down, however,
the cluster will immediately try to reconnect.
If <literal>pacemaker_remote</literal> is not restarted within the resource’s monitor timeout,
the cluster will consider the monitor operation as failed.</simpara>
<simpara>If you wish to avoid monitor failures when
the <literal>pacemaker_remote</literal> service is stopped on an active
Pacemaker Remote node, you can use the following procedure
to take the node out of the cluster before performing any
system administration that might stop <literal>pacemaker_remote</literal></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Stop the node’s connection resource with
the <literal role="command">pcs resource disable <emphasis>resourcename</emphasis></literal>,
which will move all services off the node. For guest
nodes, this will also stop the VM, so the VM must be started outside the
cluster (for example, using <literal role="command">virsh</literal>) to perform any maintenance.</simpara>
</listitem>
<listitem>
<simpara>Perform the required maintenance.</simpara>
</listitem>
<listitem>
<simpara>When ready to return the node to the cluster, re-enable the resource with
the <literal role="command">pcs resource enable</literal>.</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters">
<title>Performing cluster maintenance</title>
<simpara>In order to perform maintenance on the nodes of your cluster, you may need to stop or move the resources
and services running on that cluster.
Or you may need to stop the cluster software while leaving the services
untouched.
Pacemaker provides a variety of methods for performing system maintenance.</simpara>
<itemizedlist>
<listitem>
<simpara>If you need to stop a node in a cluster while continuing to provide the services
running on that cluster on another node, you can put the cluster node in standby mode.
A node that is in standby mode is
no longer able to host resources. Any resource currently active on the node will be moved to another node,
or stopped if no other node is eligible to run the resource.
For information on standby mode, see
<link linkend="proc_stopping-individual-node-cluster-maintenance">Putting a node into standby mode</link>.</simpara>
</listitem>
<listitem>
<simpara>If you need to move an individual resource off the node on which it is currently running without
stopping that resource, you can use the
<literal role="command">pcs resource move</literal> command to move the resource to a different node.
For information on the
<literal role="command">pcs resource move</literal> command, see
<link linkend="assembly_manually-move-resources-cluster-maintenance">Manually moving cluster resources</link>.</simpara>
<simpara>When you execute the <literal role="command">pcs resource move</literal>
command, this adds a constraint to the resource to prevent it
from running on the node on which it is currently running.
When you are ready to move the resource back,
you can execute the
<literal role="command">pcs resource clear</literal> or the
<literal role="command">pcs constraint delete</literal> command to remove the
constraint. This does not necessarily move the resources
back to the original node, however, since where the resources can run at that
point depends on how you have configured your resources initially.
You can relocate a resource to
its preferred node with the <literal role="command">pcs resource relocate run</literal> command,
as described in
<link linkend="proc_moving-resource-to-preferred-node-manually-move-resources">Moving a resource to its preferred node</link>.</simpara>
</listitem>
<listitem>
<simpara>If you need to stop a running resource entirely and prevent the cluster from starting it again,
you can use the <literal role="command">pcs resource disable</literal> command.
For information on
the <literal role="command">pcs resource disable</literal> command, see
<link linkend="proc_disabling-resources-cluster-maintenance">Enabling, disabling, and banning cluster resources</link>.</simpara>
</listitem>
<listitem>
<simpara>If you want to prevent Pacemaker from taking any action
for a resource (for example, if you want to disable recovery actions while performing maintenance on the resource,
or if you need to reload the <literal>/etc/sysconfig/pacemaker</literal> settings), use the
<literal role="command">pcs resource unmanage</literal> command,
as described in
<link linkend="proc_unmanaging-resources-cluster-maintenance">Setting a resource to unmanaged mode</link>.
Pacemaker Remote connection resources should never be unmanaged.</simpara>
</listitem>
<listitem>
<simpara>If you need to put the cluster in a state where no services
will be started or stopped, you can set the <literal>maintenance-mode</literal>
cluster property. Putting the cluster into maintenance mode
automatically unmanages all resources.
For information on putting the cluster in maintenance mode, see
<link linkend="proc_setting-maintenance-mode-cluster-maintenance">Putting a cluster in maintenance mode</link>.</simpara>
</listitem>
<listitem>
<simpara>If you need to update the packages that make up the RHEL High Availability and
Resilient Storage Add-Ons, you can update the packages on one node at a time or on the
entire cluster as a whole, as summarized in
<link linkend="proc_updating-cluster-packages-cluster-maintenance">Updating a Red Hat Enterprise Linux high availability cluster</link>.</simpara>
</listitem>
<listitem>
<simpara>If you need to perform maintenance on a Pacemaker remote node, you can remove that node
from the cluster by disabling the remote node resource,
as described in
<link linkend="proc_upgrading-remote-nodes-cluster-maintenance">Upgrading remote nodes and guest nodes</link>.</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_stopping-individual-node-cluster-maintenance">
<title>Putting a node into standby mode</title>
<simpara>When a cluster node is in standby mode, the
node is no longer able to host resources.
Any resources currently active on the node will be
moved to another node.</simpara>
<simpara>The following command puts the
specified node into standby mode.
If you specify the
<literal role="option">--all</literal>, this command puts
all nodes into standby mode.</simpara>
<simpara>You can use this command when updating a resource’s packages.
You can also use this command when testing a configuration,
to simulate recovery without actually shutting down a
node.</simpara>
<literallayout class="monospaced">pcs node standby <emphasis>node</emphasis> | --all</literallayout>
<simpara>The following command removes the
specified node from standby mode. After
running this command,
the specified
node is then able to host resources.
If you specify the
<literal role="option">--all</literal>, this command removes
all nodes from standby mode.</simpara>
<literallayout class="monospaced">pcs node unstandby <emphasis>node</emphasis> | --all</literallayout>
<simpara>Note that when you execute the <literal role="command">pcs node standby</literal>
command, this prevents resources from
running on the indicated node. When you execute the
<literal role="command">pcs node unstandby</literal> command, this
allows resources to run on the indicated node.
This does not necessarily move the resources
back to the indicated node; where the resources can run at that
point depends on how you have configured your resources initially.</simpara>
</section>
<section xml:id="assembly_manually-move-resources-cluster-maintenance">
<title>Manually moving cluster resources</title>
<simpara>You can override the cluster
and force resources to move from their current
location.
There are two occasions when you would want to
do this:</simpara>
<itemizedlist>
<listitem>
<simpara>When a node is under maintenance, and you need to move
all resources running on that node to a different node</simpara>
</listitem>
<listitem>
<simpara>When individually specified resources needs to be moved</simpara>
</listitem>
</itemizedlist>
<simpara>To move all resources running on a node to a different node,
you put the node in standby mode.</simpara>
<simpara>You can move individually specified resources in either of the following ways.</simpara>
<itemizedlist>
<listitem>
<simpara>You can use the <literal role="command">pcs resource move</literal> command to move a resource off
a node on which it is currently running.</simpara>
</listitem>
<listitem>
<simpara>You can use the <literal role="command">pcs resource relocate run</literal> command to
move a resource to its preferred node, as determined
by current cluster status, constraints, location of resources and
other settings.</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_moving-resource-from-node-manually-move-resources">
<title>Moving a resource from its current node</title>
<simpara>To move a resource off the node on which it is currently
running, use the following command, specifying
the <emphasis>resource_id</emphasis> of the resource
as defined.
Specify the <literal>destination_node</literal>
if you want to indicate on which node to run the resource
that you are moving.</simpara>
<literallayout class="monospaced">pcs resource move <emphasis>resource_id</emphasis> [<emphasis>destination_node</emphasis>] [--master] [lifetime=<emphasis>lifetime</emphasis>]</literallayout>
<note>
<simpara>When you execute the <literal role="command">pcs resource move</literal>
command, this adds a constraint to the resource to prevent it
from running on the node on which it is currently running.
You can execute the
<literal role="command">pcs resource clear</literal> or the
<literal role="command">pcs constraint delete</literal> command to remove the
constraint. This does not necessarily move the resources
back to the original node; where the resources can run at that
point depends on how you have configured your resources initially.</simpara>
</note>
<simpara>If you specify the <literal>--master</literal> parameter of the
<literal role="command">pcs resource move</literal> command, the scope of the
constraint is limited to the master role and you must specify <emphasis>master_id</emphasis>
rather than <emphasis>resource_id</emphasis>.</simpara>
<simpara>You can optionally configure a <literal>lifetime</literal>
parameter for the <literal>pcs resource move</literal> command to indicate
a period of time the constraint should remain.
You specify
the units of a <literal>lifetime</literal> parameter
according to the format defined in ISO 8601, which
requires that you specify the unit
as a capital letter such
as Y (for years), M (for months),
W (for weeks), D (for days), H (for hours), M (for minutes), and S (for seconds).</simpara>
<simpara>To distinguish a unit of minutes(M) from a unit of months(M),
you must specify PT before indicating
the value in minutes. For example, a <literal>lifetime</literal> parameter
of 5M indicates an interval of five months, while a <literal>lifetime</literal> parameter
of PT5M indicates an interval of five minutes.</simpara>
<simpara>The <literal>lifetime</literal> parameter is checked at intervals defined
by the <literal>cluster-recheck-interval</literal> cluster property. By default
this value is 15 minutes. If your configuration requires that you check this parameter
more frequently, you can reset this value with the following command.</simpara>
<literallayout class="monospaced">pcs property set cluster-recheck-interval=<emphasis>value</emphasis></literallayout>
<simpara>You can optionally configure a <literal>--wait[=<emphasis>n</emphasis>]</literal>
parameter for the <literal>pcs resource move</literal> command to indicate
the number of seconds to wait for the resource to start on the destination node
before returning 0 if the resource is started or 1 if the resource has not yet
started. If you do not specify n, the default resource timeout will be used.</simpara>
<simpara>The following command moves the resource <literal>resource1</literal>
to node <literal>example-node2</literal> and prevents it from moving back to the
node on which it was originally running for one hour and thirty minutes.</simpara>
<literallayout class="monospaced">pcs resource move resource1 example-node2 lifetime=PT1H30M</literallayout>
<simpara>The following command moves the resource <literal>resource1</literal>
to node <literal>example-node2</literal> and prevents it from moving back to the
node on which it was originally running for thirty minutes.</simpara>
<literallayout class="monospaced">pcs resource move resource1 example-node2 lifetime=PT30M</literallayout>
</section>
<section xml:id="proc_moving-resource-to-preferred-node-manually-move-resources">
<title>Moving a resource to its preferred node</title>
<simpara>After a resource has moved, either due to a failover or to
an administrator manually moving the node, it will not necessarily
move back to its original node even after the circumstances that caused
the failover have been corrected. To relocate resources to their
preferred node, use the following command. A preferred node is
determined by the current cluster status, constraints,
resource location, and other settings and may change over time.</simpara>
<literallayout class="monospaced">pcs resource relocate run [<emphasis>resource1</emphasis>] [<emphasis>resource2</emphasis>] ...</literallayout>
<simpara>If you do not specify any resources, all resource are relocated to their preferred nodes.</simpara>
<simpara>This command calculates the preferred node for each resource while ignoring
resource stickiness. After calculating the preferred node, it creates location constraints which
will cause the resources to move to their preferred nodes. Once
the resources have been moved, the constraints are deleted automatically.
To remove all constraints created by the <literal role="command">pcs resource relocate run</literal> command,
you can enter the <literal role="command">pcs resource relocate clear</literal> command.
To display the current status of resources and their optimal node ignoring
resource stickiness, enter the <literal role="command">pcs resource relocate show</literal> command.</simpara>
</section>
</section>
<section xml:id="proc_disabling-resources-cluster-maintenance">
<title>Disabling, enabling, and banning cluster resources</title>
<simpara>In addition to the <literal role="command">pcs resource move</literal> and
<literal role="command">pcs resource relocate</literal> commands,
there are a variety of other commands you can use to control the behavior
of cluster resources.</simpara>
<bridgehead xml:id="disabling_a_cluster_resource" renderas="sect3" remap="_disabling_a_cluster_resource">Disabling a cluster resource</bridgehead>
<simpara>You can manually stop a running resource and prevent the cluster
from starting it again with the following command.
Depending on the rest of the configuration
(constraints, options, failures, and so on), the resource may remain
started. If you specify the <literal role="option">--wait</literal> option,
<emphasis role="strong"><phrase role="application">pcs</phrase></emphasis> will wait up to 'n' seconds
for the resource to stop and then return
0 if the resource is stopped or 1 if the resource has not stopped.
If 'n' is not specified it defaults to 60 minutes.</simpara>
<literallayout class="monospaced">pcs resource disable <emphasis>resource_id</emphasis> [--wait[=<emphasis>n</emphasis>]]</literallayout>
<simpara>As of Red Hat Enterprise Linux 8.2, you can specify that a resource be disabled only if disabling
the resource would not have an effect on other resources.
Ensuring that this would be the case can be impossible to do by hand when complex resource relations are set up.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal role="command">pcs resource disable --simulate</literal> command shows the effects of disabling a resource while not changing the cluster configuration.</simpara>
</listitem>
<listitem>
<simpara>The <literal role="command">pcs resource disable --safe</literal> command disables a resource only if no other resources would be affected in any way, such as being migrated from one node to another. The <literal role="command">pcs resource safe-disable</literal> command is an alias for the <literal>pcs resource disable --safe</literal> command.</simpara>
</listitem>
<listitem>
<simpara>The <literal role="command">pcs resource disable --safe --no-strict</literal> command disables a resource only if no other resources would be stopped or demoted</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="enabling_a_cluster_resource" renderas="sect3" remap="_enabling_a_cluster_resource">Enabling a cluster resource</bridgehead>
<simpara>Use the following command to allow the cluster
to start a resource. Depending on the rest of the
configuration, the resource may remain stopped.
If you specify the <literal role="option">--wait</literal> option,
<emphasis role="strong"><phrase role="application">pcs</phrase></emphasis> will wait up to
'n' seconds for the resource to start and then return
0 if the resource is started or 1 if the resource has not started.
If 'n' is not specified it defaults to 60 minutes.</simpara>
<literallayout class="monospaced">pcs resource enable <emphasis>resource_id</emphasis> [--wait[=<emphasis>n</emphasis>]]</literallayout>
<bridgehead xml:id="preventing_a_resource_from_running_on_a_particular_node" renderas="sect3" remap="_preventing_a_resource_from_running_on_a_particular_node">Preventing a resource from running on a particular node</bridgehead>
<simpara>Use the following command to prevent a resource from running on a specified
node, or on the current node if no node is specified.</simpara>
<literallayout class="monospaced">pcs resource ban <emphasis>resource_id</emphasis> [<emphasis>node</emphasis>] [--master] [lifetime=<emphasis>lifetime</emphasis>] [--wait[=<emphasis>n</emphasis>]]</literallayout>
<simpara>Note that when you execute the <literal role="command">pcs resource ban</literal>
command, this adds a -INFINITY location constraint to the resource to prevent it
from running on the indicated node.
You can execute the
<literal role="command">pcs resource clear</literal> or the
<literal role="command">pcs constraint delete</literal> command to remove the
constraint. This does not necessarily move the resources
back to the indicated node; where the resources can run at that
point depends on how you have configured your resources initially.</simpara>
<simpara>If you specify the <literal>--master</literal> parameter of the
<literal role="command">pcs resource ban</literal> command, the scope of the
constraint is limited to the master role and you must specify <emphasis>master_id</emphasis>
rather than <emphasis>resource_id</emphasis>.</simpara>
<simpara>You can optionally configure a <literal>lifetime</literal>
parameter for the <literal>pcs resource ban</literal> command to indicate
a period of time the constraint should remain.</simpara>
<simpara>You can optionally configure a <literal>--wait[=<emphasis>n</emphasis>]</literal>
parameter for the <literal>pcs resource ban</literal> command to indicate
the number of seconds to wait for the resource to start on the destination node
before returning 0 if the resource is started or 1 if the resource has not yet
started. If you do not specify n, the default resource timeout will be used.</simpara>
<bridgehead xml:id="forcing_a_resource_to_start_on_the_current_node" renderas="sect3" remap="_forcing_a_resource_to_start_on_the_current_node">Forcing a resource to start on the current node</bridgehead>
<simpara>Use the <literal role="command">debug-start</literal> parameter of
the <literal role="command">pcs resource</literal> command
to force a specified resource to start on the current node,
ignoring the cluster recommendations and printing the output from
starting the resource. This is mainly used for debugging resources;
starting resources on a cluster is (almost) always done by Pacemaker and
not directly with a <literal role="command">pcs</literal> command. If your resource
is not starting, it is usually
due to either a misconfiguration of the resource (which you debug in
the system log), constraints that prevent the resource from starting,
or the resource being disabled. You can use this command
to test resource configuration, but it should not normally be used to start
resources in a cluster.</simpara>
<simpara>The format of the <literal role="command">debug-start</literal> command is as follows.</simpara>
<literallayout class="monospaced">pcs resource debug-start <emphasis>resource_id</emphasis></literallayout>
</section>
<section xml:id="proc_unmanaging-resources-cluster-maintenance">
<title>Setting a resource to unmanaged mode</title>
<simpara>When a resource is in <literal>unmanaged</literal> mode, the resource is still in the
configuration but Pacemaker does not manage the resource.</simpara>
<simpara>The following command sets the indicated resources to <literal>unmanaged</literal>
mode.</simpara>
<literallayout class="monospaced">pcs resource unmanage <emphasis>resource1</emphasis>  [<emphasis>resource2</emphasis>] ...</literallayout>
<simpara>The following command sets resources to <literal>managed</literal> mode, which is the default
state.</simpara>
<literallayout class="monospaced">pcs resource manage <emphasis>resource1</emphasis>  [<emphasis>resource2</emphasis>] ...</literallayout>
<simpara>You can specify the name of a resource group with the <literal role="command">pcs resource manage</literal> or
<literal role="command">pcs resource unmanage</literal> command. The command will act on all of the resources
in the group, so that you can set all of the resources in a group
to <literal>managed</literal> or <literal>unmanaged</literal> mode with a single
command and then manage
the contained resources individually.</simpara>
</section>
<section xml:id="proc_setting-maintenance-mode-cluster-maintenance">
<simpara>To put a cluster in maintenance mode, use the following command to
set the <literal>maintenance-mode</literal> cluster property to <literal>true</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs property set maintenance-mode=true</literal></literallayout>
<simpara>To remove a cluster from maintenance mode, use the following command to
set the <literal>maintenance-mode</literal> cluster property to <literal>false</literal>.</simpara>
<literallayout class="monospaced"># <literal>pcs property set maintenance-mode=false</literal></literallayout>
<simpara>You can remove a cluster property from
the configuration with the following command.</simpara>
<literallayout class="monospaced">pcs property unset <emphasis>property</emphasis></literallayout>
<simpara>Alternately, you can remove a cluster property from a configuration by leaving the
value field of the <literal role="command">pcs property set</literal> command
blank. This restores that property to its default
value. For example, if you have previously set the
<literal>symmetric-cluster</literal> property to <literal>false</literal>,
the following command removes the value you have set
from the configuration and restores the value of
<literal>symmetric-cluster</literal> to <literal>true</literal>,
which is its default value.</simpara>
<literallayout class="monospaced"># <literal>pcs property set symmetric-cluster=</literal></literallayout>
</section>
<section xml:id="proc_updating-cluster-packages-cluster-maintenance">
<title>Updating a RHEL high availability cluster</title>
<simpara>Updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons,
either individually or as a whole, can be done in one of two general ways:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis>Rolling Updates</emphasis>: Remove one node at a time from service, update its software,
then integrate it back into the cluster.
This allows the cluster to continue
providing service and managing resources while each node is updated.</simpara>
</listitem>
<listitem>
<simpara><emphasis>Entire Cluster Update</emphasis>: Stop
the entire cluster, apply updates to all nodes, then start the cluster back up.</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>It is critical that when performing software update procedures for Red Hat Enterprise LInux
High Availability and Resilient Storage clusters, you ensure that any node that will
undergo updates is not an active member of the cluster before those updates are initiated.</simpara>
</warning>
<simpara>For a full description of each of these methods and the procedures to follow
for the updates, see
<link xlink:href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</link>.</simpara>
</section>
<section xml:id="proc_upgrading-remote-nodes-cluster-maintenance">
<title>Upgrading remote nodes and guest nodes</title>
<simpara>If the <literal>pacemaker_remote</literal> service is stopped on an active
remote node or guest node, the cluster will gracefully migrate resources off
the node before stopping the node.
This allows you to perform software upgrades and other routine
maintenance procedures without removing the node from the cluster.
Once <literal>pacemaker_remote</literal> is shut down, however,
the cluster will immediately try to reconnect.
If <literal>pacemaker_remote</literal> is not restarted within the resource’s monitor timeout,
the cluster will consider the monitor operation as failed.</simpara>
<simpara>If you wish to avoid monitor failures when
the <literal>pacemaker_remote</literal> service is stopped on an active
Pacemaker Remote node, you can use the following procedure
to take the node out of the cluster before performing any
system administration that might stop <literal>pacemaker_remote</literal></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Stop the node’s connection resource with
the <literal role="command">pcs resource disable <emphasis>resourcename</emphasis></literal>,
which will move all services off the node. For guest
nodes, this will also stop the VM, so the VM must be started outside the
cluster (for example, using <literal role="command">virsh</literal>) to perform any maintenance.</simpara>
</listitem>
<listitem>
<simpara>Perform the required maintenance.</simpara>
</listitem>
<listitem>
<simpara>When ready to return the node to the cluster, re-enable the resource with
the <literal role="command">pcs resource enable</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="proc_migrating-cluster-vms-cluster-maintenance">
<title>Migrating VMs in a RHEL cluster</title>
<simpara>Information on support policies for RHEL high availability clusters with virtualized cluster members
can be found in
<link xlink:href="https://access.redhat.com/articles/3131111/">Support Policies for RHEL High Availability Clusters - General Conditions with Virtualized Cluster Members</link>.
As noted, Red Hat does not support live migration of active cluster nodes across hypervisors or hosts.
If you need to perform a live migration, you will first need to stop the cluster services
on the VM to remove the node from the cluster, and then start the cluster back up after performing the migration.
The following steps outline the procedure for removing a VM from a cluster, migrating the VM, and restoring the VM to the cluster.</simpara>
<simpara>This procedure applies to VMs that are used as full cluster nodes,
not to VMs managed as cluster resources (including VMs used as guest
nodes) which can be live-migrated without special precautions.
For general information on the fuller procedure required for
updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons,
either individually or as a whole, see
<link xlink:href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</link>.</simpara>
<note>
<simpara>Before performing this procedure, consider the effect on cluster quorum of removing a cluster node.
For example, if you have a three-node cluster and you remove one node, your cluster can withstand
only one more node failure. If one node of a three-node cluster is already down,
removing a second node will lose quorum.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>If any preparations need to be made before stopping or moving the resources or software running on the VM to migrate, perform those steps.</simpara>
</listitem>
<listitem>
<simpara>Run the following command on the VM to stop the cluster software on the VM.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster stop</literal></literallayout>
</listitem>
<listitem>
<simpara>Perform the live migration of the VM.</simpara>
</listitem>
<listitem>
<simpara>Start cluster services on the VM.</simpara>
<literallayout class="monospaced"># <literal>pcs cluster start</literal></literallayout>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters">
<title>Configuring disaster recovery clusters</title>
<simpara>One method of providing disaster recovery for a high availability cluster
is to configure two clusters. You can then configure one cluster as your
primary site cluster, and the second cluster as your disaster recovery cluster.</simpara>
<simpara>In normal circumstances, the primary cluster is running resources in
production mode. The disaster recovery cluster has all the resources configured
as well and is either running them in demoted mode or not at all.
For example, there may be a database running in the primary cluster in promoted
mode and running in the disaster recovery cluster in demoted mode.
The database in this setup would be configured so that data is synchronized from
the primary to disaster recovery site. This is done through the
database configuration itself rather than through the <literal>pcs</literal> command
interface.</simpara>
<simpara>When the primary cluster goes down, users can use the <literal>pcs</literal> command
interface to  manually fail the resources over to the disaster recovery site. They can then
log in to the disaster site and promote and start the resources there.
Once the primary cluster has recovered, users can use the <literal>pcs</literal> command
interface to manually move resources back to the primary site.</simpara>
<simpara>As of Red Hat Enterprise Linux 8.2, you can use the <literal>pcs</literal> command
to display the status of both the primary and the disaster recovery site cluster
from a single node on either site.</simpara>
<section xml:id="ref_recovery-considerations-configuring-disaster-recovery">
<title>Considerations for disaster recovery clusters</title>
<simpara>When planning and configuring a disaster recovery site that you will manage and monitor with the
<literal>pcs</literal> command interface, note the following considerations.</simpara>
<itemizedlist>
<listitem>
<simpara>The disaster recovery site
must be a cluster.  This makes it possible to configure it with same
tools and similar procedures as the primary site.</simpara>
</listitem>
<listitem>
<simpara>The primary and disaster recovery clusters are
created by independent <literal>pcs cluster setup</literal> commands.</simpara>
</listitem>
<listitem>
<simpara>The clusters and their resources must be
configured so that
that the data is synchronized and failover is possible.</simpara>
</listitem>
<listitem>
<simpara>The cluster nodes in the recovery site can not have the same names as the nodes in the primary site.</simpara>
</listitem>
<listitem>
<simpara>The pcs user <literal>hacluster</literal> must be authenticated for each node in both clusters on the node from which you will be running <literal>pcs</literal> commands.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_disaster-recovery-display-configuring-disaster-recovery">
<title>Displaying status of recovery clusters (RHEL 8.2 and later)</title>
<simpara>To configure a primary and a disaster recovery cluster so that you can display the status of both clusters,
perform the following procedure.</simpara>
<note>
<simpara>Setting up a disaster recovery cluster does not automatically configure resources or
replicate data. Those items must be configured manually by the user.</simpara>
</note>
<simpara>In this example:</simpara>
<itemizedlist>
<listitem>
<simpara>The primary cluster will be named <literal>PrimarySite</literal> and will consist of the nodes
<literal>z1.example.com</literal>. and <literal>z2.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>The disaster recovery site cluster will be named <literal>DRsite</literal> and will consist of the nodes
<literal>z3.example.com</literal> and <literal>z4.example.com</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>This example sets up a basic cluster with no resources or fencing configured.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Authenticate all of the nodes that will be used for both clusters.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs host auth z1.example.com z2.example.com z3.example.com z4.example.com  -u hacluster -p password</literal>
z1.example.com: Authorized
z2.example.com: Authorized
z3.example.com: Authorized
z4.example.com: Authorized</literallayout>
</listitem>
<listitem>
<simpara>Create the cluster that will be used as the primary cluster and start cluster services for the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs cluster setup PrimarySite z1.example.com z2.example.com  --start</literal>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z1.example.com', 'z2.example.com'...</literallayout>
</listitem>
<listitem>
<simpara>Create the cluster that will be used as the disaster recovery cluster and start cluster services for the cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs cluster setup DRSite z3.example.com z4.example.com --start</literal>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z3.example.com', 'z4.example.com'...</literallayout>
</listitem>
<listitem>
<simpara>From a node in the primary cluster, set up the second cluster as the recovery site. The recovery site is defined
by a name of one of its nodes.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs dr set-recovery-site z3.example.com</literal>
Sending 'disaster-recovery config' to 'z3.example.com', 'z4.example.com'
z3.example.com: successful distribution of the file 'disaster-recovery config'
z4.example.com: successful distribution of the file 'disaster-recovery config'
Sending 'disaster-recovery config' to 'z1.example.com', 'z2.example.com'
z1.example.com: successful distribution of the file 'disaster-recovery config'
z2.example.com: successful distribution of the file 'disaster-recovery config'</literallayout>
</listitem>
<listitem>
<simpara>Check the disaster recovery configuration.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs dr config</literal>
Local site:
  Role: Primary
Remote site:
  Role: Recovery
  Nodes:
    z1.example.com
    z2.example.com</literallayout>
</listitem>
<listitem>
<simpara>Check the status of the primary cluster and the disaster recovery cluster from a node in the primary cluster.</simpara>
<literallayout class="monospaced">[root@z1 ~]# <literal>pcs dr status</literal>
--- Local cluster - Primary site ---
Cluster name: PrimarySite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z2.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:31 2019
  * Last change:  Mon Dec  9 04:06:10 2019 by hacluster via crmd on z2.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z1.example.com z2.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled


--- Remote cluster - Recovery site ---
Cluster name: DRSite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z4.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:34 2019
  * Last change:  Mon Dec  9 04:09:55 2019 by hacluster via crmd on z4.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z3.example.com z4.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled</literallayout>
</listitem>
</orderedlist>
<simpara>For additional display options for a disaster recovery configuration,
see the help screen for the <literal>pcs dr</literal> command.</simpara>
</section>
</chapter>
</book>
